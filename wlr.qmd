# Weighted logrank test {#sec-wlr}

```{r, setup, file="_common.R", include=FALSE}
```

```{r, warning=FALSE, message=FALSE}
library(gsDesign)
library(gsdmvn)
library(mvtnorm)
library(dplyr)
```

In this chapter, we start to discuss group sequential design for weighted logrank test.

## Assumptions

For a group sequential design, we assume there are total $K$ analyses in a trial.
The **calendar** time of the analyses are $t_k, k =1,\dots, K$.

We define the test statistic at analysis $k=1,2,\dots,K$ as

$$
Z_k =  \frac{\widehat{\Delta}_k}{\sigma(\widehat{\Delta}_k)}
$$

where $\widehat{\Delta}_k$ is a statistics to characterize group difference and
$\sigma(\widehat{\Delta}_k)$ is the standard deviation of $\widehat{\Delta}_k$.

### Asymptotic normality

In group sequential design, we consider the test statistics are asymptotically normal.

- Under the null hypothesis

$$
Z_k \sim \mathcal{N}(0,1)
$$

- Under a (local) alternative hypothesis

$$
Z_k \sim \mathcal{N}(\Delta_k I_k^{1/2}, 1),
$$

where $I_k$ is the Fisher information $I_k \approx 1/{\sigma^2(\widehat{\Delta}_k)}$

Then we can define the effect size as

$$
\theta = \Delta_k / \sigma_k = \Delta_k I_k^{1/2}$ with $\sigma_k = \sigma(\widehat{\Delta}_k)
$$.

So, it is equivalent to claim:

$$
Z_k \sim \mathcal{N}(\theta_k, 1)
$$

### Independent increments process

For group sequential design, we need to demonstrate the test statistics $Z = (Z_1, \dots, Z_K)$
is an independent increments process. The property for WLR has been proved by @scharfstein1997semiparametric
as a corollary of martingale representation.

- Under the null $Z ~ \sim MVN(0,\Sigma)$
- Under a local alternative $Z ~ \sim MVN(\theta, \Sigma)$

- Here, $\Sigma={\Sigma_{ij}}$ is a correlation matrix with

$$
\Sigma_{ij} = \min(I_i, I_j) / \max(I_i, I_j) \approx \min(\sigma_i, \sigma_j) / \max(\sigma_i, \sigma_j)
$$

## Type I error, $\alpha$-spending function, and group sequential design boundary

- Define test boundary for each interim analysis
  - $\alpha$-spending function @LanDeMets

- Bounds $-\infty \le a_k \le b_k \le \infty$ for $k=1,\dots,K$
  - non-binding futility bound $a_k = -\infty$ for all $k$

- Upper boundary crossing probabilities
  - $u_k = \text{Pr}(\{Z_k \ge b_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j < b_j\})$

- Lower boundary crossing probabilities
  - $l_k = \text{Pr}(\{Z_k < a_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j < b_j\})$

- Under the null hypothesis, the probability to reject the null hypothesis.
  - $\alpha = \sum_{k=1}^{K} u_k = \sum_{k=1}^{K} \text{Pr}(\{Z_k \ge b_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j \le b_j\} \mid H_0)$
  - With a spending function family $f(t,\gamma)$,
  - `gsDesign::sfLDOF()` (O'Brien-Fleming bound approximation) or other functions starts with `sf` in gsDesign

For simplicity, we directly use gsDesign boundary for this chapter.
It will inflate Type I Error for WLR tests.
We will discuss how to fix the issue in the next chapter.

```{r}
x <- gsDesign::gsSurv(
  k = 3, test.type = 4, alpha = 0.025,
  beta = 0.1, astar = 0, timing = c(1),
  sfu = gsDesign::sfLDOF, sfupar = c(0),
  sfl = gsDesign::sfLDOF, sflpar = c(0),
  lambdaC = c(0.1),
  hr = 0.6, hr0 = 1, eta = 0.01,
  gamma = c(10),
  R = c(12), S = NULL,
  T = 36, minfup = 24, ratio = 1
)
```

- Lower bound

```{r}
x$lower$bound
```

- Upper bound

```{r}
x$upper$bound
```

## Power

Given the lower and upper bound of a group sequential design, we can calculate the overall power of the design.

- Power: under a (local) alternative hypothesis, the probability to reject the null hypothesis.

$$
1 - \beta =\sum_{k=1}^{K} u_k = \sum_{k=1}^{K} \text{Pr}(\{Z_k \ge b_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j \le b_j\} \mid H_1)
$$

If there is no lower bound, the formula can be simplified as

$$
\beta = \text{Pr}(\cap_{j=1}^{K} \{Z_j \le b_j\} \mid H_1)
$$

We can calculate the sample size required for a group sequential design by solving
the power equation with a given lower and upper bound and power, type I error and power.

As a note, we can calculate futility probability similarly:

$$
\sum_{k=1}^{K} l_k = \sum_{k=1}^{K} \text{Pr}(\{Z_k < a_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j \le b_j\}\mid H_1)
$$

## Weighted logrank test

Similar to the fixed design, we can define the test statistics for weighted logrank test using the counting process formula as

$$
Z_k=\sqrt{\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\int_{0}^{t_k}w(t)\frac{\overline{Y}_{0}(t)\overline{Y}_{1}(t)}{\overline{Y}_{0}(t)+\overline{Y}_{0}(t)}\left\{ \frac{d\overline{N}_{1}(t)}{\overline{Y}_{1}(t)}-\frac{d\overline{N}_{0}(t)}{\overline{Y}_{0}(t)}\right\}
$$

Here $n_i$ are the number of subjects in group $i$.
$\overline{Y}_{i}(t)$ are the number of subjects in group $i$ at risk at time $t$.
$\overline{N}_{i}(t)$ are the number of events in group $i$ up to and including time $t$.

Note, the only difference is that the test statistics fixed analysis up to $t_k$ at $k$th interim analysis

::: {.callout-note}
For simplicity, we illustrate the sample size and power calculation based on the boundary from the logrank test.
The same boundary for different types of analysis for a fair comparison.
However, different WLR can have different information fraction for the same interim analysis time.
Further discussion of the spending function based on actual information fraction will be provided later.
:::

## Example scenario

We considered an example scenario that is similar to the fixed design.
The key difference is that we considered 2 interim analysis at 12 and 24 months before the final analysis at 36 months.

```{r}
enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500 / 12)

failRates <- tibble::tibble(
  Stratum = "All",
  duration = c(4, 100),
  failRate = log(2) / 15, # Median survival 15 month
  hr = c(1, 0.6),
  dropoutRate = 0.001
)
```

```{r}
# Randomization Ratio is 1:1
ratio <- 1

# Type I error (one-sided)
alpha <- 0.025

# Power (1 - beta)
beta <- 0.2
power <- 1 - beta

# Interim Analysis Time
analysisTimes <- c(12, 24, 36)
```

## Sample size under logrank test or $FH(0, 0)$

In gsdmvn, the sample size can be calculated using `gsdmvn::gs_design_wlr()` for the WLR test.
For comparison purposes, we also provided the sample size calculation using `gsdmvn::gs_design_ahr()` for the logrank test.
In each calculation, we compare the analytical results with the simulation results with 10,000 replications.

::: {.callout-note}
We described the theoretical details for sample size calculation in the technical details section.
:::

- AHR with logrank test

```{r}
gsdmvn::gs_design_ahr(
  enrollRates = enrollRates, failRates = failRates,
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = analysisTimes
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%
  tidyr::pivot_wider(names_from = Bound, values_from = Probability)
```

- Simulation results based on 10,000 replications.

```{r, echo = FALSE}
load("simulation/simu_gsd_wlr.Rdata")
res %>%
  subset(n == 386 & rho == 0 & gamma == 0) %>%
  select(-scenario, -rho, -gamma) %>%
  mutate_if(is.numeric, round, digits = 2)
```

- $FH(0,0)$

```{r}
gsdmvn::gs_design_wlr(
  enrollRates = enrollRates, failRates = failRates,
  weight = function(x, arm0, arm1) {
    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)
  },
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = c(12, 24, 36)
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%
  tidyr::pivot_wider(names_from = Bound, values_from = Probability)
```

- Simulation results based on 10,000 replications.

```{r, echo = FALSE}
load("simulation/simu_gsd_wlr.Rdata")
res %>%
  subset(n == 384 & rho == 0 & gamma == 0) %>%
  select(-scenario, -rho, -gamma) %>%
  mutate_if(is.numeric, round, digits = 2)
```

## Sample size under modestly WLR cut at 4 months

- Modestly WLR: $S^{-1}(t\land\tau_m)$ @magirr2019modestly

```{r}
gsdmvn::gs_design_wlr(
  enrollRates = enrollRates, failRates = failRates,
  weight = function(x, arm0, arm1) {
    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = -1, gamma = 0, tau = 4)
  },
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = analysisTimes
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2)
```

## Sample size under $FH(0, 1)$

```{r}
gsdmvn::gs_design_wlr(
  enrollRates = enrollRates, failRates = failRates,
  weight = function(x, arm0, arm1) {
    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)
  },
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = analysisTimes
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%
  tidyr::pivot_wider(names_from = Bound, values_from = Probability)
```

- Simulation results based on 10,000 replications.

```{r, echo = FALSE}
load("simulation/simu_gsd_wlr.Rdata")
res %>%
  subset(n == 317 & rho == 0 & gamma == 1) %>%
  select(-scenario, -rho, -gamma) %>%
  mutate_if(is.numeric, round, digits = 2)
```

## Sample size under $FH(0, 0.5)$

```{r}
gsdmvn::gs_design_wlr(
  enrollRates = enrollRates, failRates = failRates,
  weight = function(x, arm0, arm1) {
    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)
  },
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = analysisTimes
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%
  tidyr::pivot_wider(names_from = Bound, values_from = Probability)
```

- Simulation results based on 10,000 replications.

```{r, echo = FALSE}
load("simulation/simu_gsd_wlr.Rdata")
res %>%
  subset(n == 314 & rho == 0 & gamma == 0.5) %>%
  select(-scenario, -rho, -gamma) %>%
  mutate_if(is.numeric, round, digits = 2)
```

## Sample size under $FH(0.5, 0.5)$

```{r}
gsdmvn::gs_design_wlr(
  enrollRates = enrollRates, failRates = failRates,
  weight = function(x, arm0, arm1) {
    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)
  },
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = analysisTimes
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2) %>%
  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%
  tidyr::pivot_wider(names_from = Bound, values_from = Probability)
```

- Simulation results based on 10,000 replications.

```{r, echo = FALSE}
load("simulation/simu_gsd_wlr.Rdata")
res %>%
  subset(n == 317 & rho == 0.5 & gamma == 0.5) %>%
  select(-scenario, -rho, -gamma) %>%
  mutate_if(is.numeric, round, digits = 2)
```

## Average hazard ratio comparison

The average hazard ratio depends on the weight used in the WLR.
We illustrate the difference in the figure below.

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("images/g_ahr.png")
```

Note that the average hazard ratio is weighted by corresponding weighted logrank weights.

## Standardized effect size comparison

Standardized effect size of $FH(0, 0.5)$ is larger than other weight function at month 36.
The reason $FH(0.5, 0.5)$ provides slightly smaller sample size compared with $FH(0, 1)$ and $FH(0, 0.5)$
is mainly due to smaller weight when variance becomes larger than $FH(0, 1)$ with later events.

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("images/g_theta.png")
```

## Information fraction comparison

The logrank test information fraction curve is close to a linear function of time (the boundary calculation approach we used).
Other WLR test information fraction curves are convex functions.
Information fraction under the null hypothesis is smaller than information fraction under alternative at the same time.

```{r, echo = FALSE, out.width = "60%"}
knitr::include_graphics("images/g_info.png")
```

## Technical details

This section describes the details of calculating the sample size
and events required for WLR under group sequential design.
It can be skipped for the first read of this training material.

We illustrate the idea using $FH(0, 1)$.

```{r}
weight <- function(x, arm0, arm1) {
  gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)
}
```

```{r}
# Define study design object in each arm
gs_arm <- gsdmvn:::gs_create_arm(
  enrollRates, failRates,
  ratio = 1, # Randomization ratio
  total_time = 36 # Total study duration
)
arm0 <- gs_arm[["arm0"]]
arm1 <- gs_arm[["arm1"]]
```

The calculation of power is essentially the multiple integration of multivariate normal distribution, and we implement it into a function `gs_power()` where `mvtnorm::pmvnorm()` is used to take care of the multiple integration.

```{r}
#' Power and futility of group sequential design
#'
#' @param z Numerical vector of Z statistics
#' @param corr Correlation matrix of Z statistics
#' @param futility_bound Numerical vector of futility bound.
#'                       -Inf indicates non-binding futility bound.
#' @param efficacy_bound Numerical vector of efficacy bound.
#'                       Inf indicates no early stop to declare superiority.
gs_power <- function(z, corr, futility_bound, efficacy_bound) {
  p <- c()
  p[1] <- 1 - pnorm(efficacy_bound[1], mean = z[1], sd = 1)

  for (k in 2:length(z)) {
    lower_k <- c(futility_bound[1:(k - 1)], efficacy_bound[k])
    upper_k <- c(efficacy_bound[1:(k - 1)], Inf)
    p[k] <- mvtnorm::pmvnorm(
      lower = lower_k, upper = upper_k,
      mean = z[1:k], corr = corr[1:k, 1:k]
    )
  }

  p
}
```

To utilize `gs_power()`, four important blocks are required.

1. The expectation mean of Z statistics. To derive it, one needs $\Delta_k$, $\sigma_k^{2}$ and sample size ratio at each interim analysis.
1. The correlation matrix of Z statistics (denoted as $\Sigma$).
1. The numerical vector of futility bound for multiple integration.
1. The numerical vector of efficacy bound for multiple integration.

First, we calculate

$$
  \Delta_k
  =
  \int_{0}^{t_k}w(s)\frac{p_{0}\pi_{0}(s)p_{1}\pi_{1}(s)}{\pi(s)}\{\lambda_{1}(s)-\lambda_{0}(s)\}ds,
$$

where $p_0 = n_0/(n_0 + n_1), p_1 = n_1/(n_0 + n_1)$ are the randomization ratio of the control and treatment arm, respectively.
$\pi_0(t) = E(N_0(t)), \pi_1(t) = E(N_1(t))$ are the expected number of failures of the control and treatment arm, respectively.
$\pi(t) = p_0 \pi_0(t) + p_1 \pi_1(t)$ is the probability of events.
$\lambda_0(t), \lambda_1(t)$ are hazard functions.
The detailed calculation of $\Delta_k$ is implemented in `gsdmvn:::gs_delta_wlr()`:

```{r}
delta <- abs(sapply(analysisTimes, function(x) {
  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = x, weight = weight)
}))
delta
```

Second, we calculate

$$
\sigma_k^{2}=\int_{0}^{t_k}w(s)^{2}\frac{p_{0}\pi_{0}(s)p_{1}\pi_{1}(s)}{\pi(s)^{2}}dv(s),
$$

where $v(t)= p_0 E(Y_0(t)) + p_1 E(Y_1(t))$ is the probability of people at risk.
The detailed calculation of $\sigma_k^{2}$ is implemented in `gsdmvn:::gs_sigma2_wlr()`:

```{r}
sigma2 <- abs(sapply(analysisTimes, function(x) {
  gsdmvn:::gs_sigma2_wlr(arm0, arm1, tmax = x, weight = weight)
}))
sigma2
```

Third, the sample size ratio at each interim analysis can be calculated by

```{r}
# Group sequential sample size ratio over time
gs_n_ratio <- (npsurvSS::paccr(analysisTimes, arm0) +
  npsurvSS::paccr(analysisTimes, arm1)) / 2
gs_n_ratio
```

After getting $\Delta_k$, $\sigma_k^{2}$, and the sample size ratio, one can calculate the expectation mean of Z statistics as

```{r}
n <- 500 # Assume a sample size to get power
delta / sqrt(sigma2) * sqrt(gs_n_ratio * n)
```

Then, we calculate the correlation matrix $\Sigma$ as

```{r}
corr <- outer(sqrt(sigma2), sqrt(sigma2), function(x, y) pmin(x, y) / pmax(x, y))
corr
```

Finally, for numerical vector of futility and bound, they are simply

```{r}
x$lower$bound
x$upper$bound
```

### Power and sample size calculation

- **Power**

With all blocks prepared, one can calculate the power

$$
  1 -\beta
  =
  \sum_{k=1}^{K} u_k = \sum_{k=1}^{K} \text{Pr}(\{Z_k \ge b_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j \le b_j\})
$$

by

```{r}
cumsum(gs_power(
  delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),
  corr,
  x$lower$bound,
  x$upper$bound
))
```

- **Type I error**

One can also calculate the type I error by

```{r}
cumsum(gs_power(
  rep(0, length(delta)),
  corr,
  rep(-Inf, length(delta)),
  x$upper$bound
))
```

- **Futility probability**

Similarly, we can calculate the futility probability

$$
\sum_{k=1}^{K} l_k = \sum_{k=1}^{K} \text{Pr}(\{Z_k < a_k\} \cap_{j=1}^{k-1} \{a_j \le Z_j \le b_j\})
$$

by

```{r}
#' @rdname power_futility
gs_futility <- function(z, corr, futility_bound, efficacy_bound) {
  p <- c()
  p[1] <- pnorm(futility_bound[1], mean = z[1], sd = 1)

  for (k in 2:length(z)) {
    lower_k <- c(futility_bound[1:(k - 1)], -Inf)
    upper_k <- c(efficacy_bound[1:(k - 1)], futility_bound[k])
    p[k] <- mvtnorm::pmvnorm(
      lower = lower_k, upper = upper_k,
      mean = z[1:k], corr = corr[1:k, 1:k]
    )
  }

  p
}
```

```{r}
cumsum(gs_futility(
  delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),
  corr, x$lower$bound, x$upper$bound
))
```

- **Sample size**

In addition to power and futility probability, one can also calculate the sample size by

```{r}
efficacy_bound <- x$upper$bound
futility_bound <- x$lower$bound

# Sample size to event
n <- uniroot(
  function(n, power) {
    power - sum(gs_power(
      delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),
      corr, futility_bound, efficacy_bound
    ))
  },
  interval = c(1, 1e5), power = 1 - beta
)$root
```

```{r}
n_subject <- n * ratio / sum(ratio)
n_subject * gs_n_ratio
```

- **Number of events**

With the sample size available, one can calculate the number of events by

```{r}
n0 <- n1 <- n_subject / 2
gsdmvn:::prob_event.arm(arm0, tmax = analysisTimes) * n0 +
  gsdmvn:::prob_event.arm(arm1, tmax = analysisTimes) * n1
```

- **Average hazard ratio**

The function below implements the connection between $\Delta$ and average hazard ratio using
Taylor series expansion.

```{r}
log_ahr <- sapply(analysisTimes, function(t_k) {
  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t_k, weight = weight) /
    gsdmvn:::gs_delta_wlr(arm0, arm1,
      tmax = t_k, weight = weight,
      approx = "generalized schoenfeld",
      normalization = TRUE
    )
})
exp(log_ahr)
```

- The `info` column is $\sigma^2_k$ times $N$

```{r}
n_subject * sigma2
```

- The `info0` column is $\sigma^2_k$ times $N$ under the null where we both use `arm0` for calculation in active and control group.

### Cross comparison with `gs_design_wlr()`

```{r}
gsdmvn::gs_design_wlr(
  enrollRates = enrollRates, failRates = failRates,
  weight = function(x, arm0, arm1) {
    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)
  },
  ratio = ratio, alpha = alpha, beta = beta,
  upar = x$upper$bound,
  lpar = x$lower$bound,
  analysisTimes = analysisTimes
)$bounds %>%
  mutate_if(is.numeric, round, digits = 2)
```

### Illustration of `gs_info_wlr()`

The necessary information has also been summarized in a data frame using `gsdmvn::gs_info_wlr()`.

```{r}
gs_info <- gsdmvn::gs_info_wlr(
  enrollRates, failRates, ratio,
  analysisTimes = analysisTimes,
  weight = weight
)

gs_info %>% mutate_if(is.numeric, round, digits = 2)
```

- `N` is based on the information in `enrollRates`

```{r}
N <- sum(enrollRates$rate * enrollRates$duration)
N
```

- `Events` is the probability of events times $N$

```{r}
n0 <- n1 <- N / 2
gsdmvn:::prob_event.arm(arm0, tmax = analysisTimes) * n0 +
  gsdmvn:::prob_event.arm(arm1, tmax = analysisTimes) * n1
```

- `AHR` is the average hazard ratio

```{r}
log_ahr <- sapply(analysisTimes, function(t_k) {
  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t_k, weight = weight) /
    gsdmvn:::gs_delta_wlr(
      arm0, arm1,
      tmax = t_k,
      weight = weight,
      approx = "generalized schoenfeld",
      normalization = TRUE
    )
})
exp(log_ahr)
```

- `delta`, `sigma2`, and `theta` are defined as above

```{r}
delta / sigma2
```

- The `info` column is $\sigma^2_k$ times $N$

```{r}
N * sigma2
```

- The `info0` column is $\sigma^2_k$ times $N$ under the null.
