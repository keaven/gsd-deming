[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"Welcome 2021 Deming Conference course group sequential design.consider group sequential design time--event endpoints proportional non-proportional hazards assumptions randomized clinical trials. primary focus logrank testing due regulatory acceptance, weighted logrank test, combination tests RMST also considered. Timing analyses boundary setting efficacy futility critical topics discussed length. simple, piecewise model can used approximate arbitrary scenarios proposed. addition 2-arm comparisons single endpoint, also discuss graphical methods strong control Type error hypotheses multiple endpoints /multiple populations. Asymptotic theory briefly noted background, focus applications, including software quickly compare designs scenarios. Throughout course, develop designs incorporating key new concept.","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"training-overview","chapter":"Preface","heading":"0.1 Training overview","text":"course, present concepts, theory, software Shiny interface.\nMainly focus designs might consider time--event endpoints.\naddition classical approaches assuming proportional hazards assumption, provide methods designing non-proportional hazards assumptions.\nstudies still use logrank test, also touch alternatives along potential advantages disadvantages.DisclaimerAll opinions expressed presenters Merck & Co., Inc., Kenilworth, NJ, USA.","code":""},{"path":"preface.html","id":"chapters-and-training-sections","chapter":"Preface","heading":"Chapters and training sections","text":"Background theory (30 minutes)\nExtension non-proportional hazards\nGroup sequential design asymptotic distribution\nSpending function bounds\nBackground theory (30 minutes)Extension non-proportional hazardsGroup sequential design asymptotic distributionSpending function boundsProportional hazards applications Shiny app (40 minutes)\nLachin Foulkes method sample size derivation\nDesign setup exponential distribution\nDesign setup cure model\nUpdating bounds time analysis\nEvent-based calendar-based spending bounds\nExercise\nProportional hazards applications Shiny app (40 minutes)Lachin Foulkes method sample size derivationDesign setup exponential distributionDesign setup cure modelUpdating bounds time analysisEvent-based calendar-based spending boundsExerciseBreak (15 minutes)Break (15 minutes)Non-proportional hazards model logrank test (60 minutes)\nPiecewise model\nAverage hazard ratio\nStatistical information time\nIntroduction gsdmvn, gsDesign2 simtrial\nNon-proportional hazards model logrank test (60 minutes)Piecewise modelAverage hazard ratioStatistical information timeIntroduction gsdmvn, gsDesign2 simtrialBreak (10 minutes)Break (10 minutes)Weighted logrank, RMST combination tests (55 minutes)\nIntroduction methods\nWeighted logrank\nMaxCombo\nExercise\nWeighted logrank, RMST combination tests (55 minutes)Introduction methodsWeighted logrankMaxComboExercise","code":""},{"path":"preface.html","id":"software-and-supporting-materials","chapter":"Preface","heading":"0.2 Software and supporting materials","text":"Useful directories course repository https://github.com/keaven/gsd-deming:\ndata/: contains design files examples; also simulation results\nvignettes/: reports produced Shiny app summarize designs\nsimulation/: R code simulation data last part course\ndata/: contains design files examples; also simulation resultsvignettes/: reports produced Shiny app summarize designssimulation/: R code simulation data last part course","code":""},{"path":"preface.html","id":"installing-r-packages","chapter":"Preface","heading":"0.3 Installing R packages","text":"choose install R packages locally:gsDesign package (v3.2.1) available CRANFor non-proportional hazards, following 3 packages useful installYou need reasonably recent versions R packages.","code":"\ninstall.packages(\"gsDesign\")\nremotes::install_github(\"Merck/gsdmvn\")\nremotes::install_github(\"Merck/gsDesign2\")\nremotes::install_github(\"Merck/simtrial\")"},{"path":"preface.html","id":"authors-and-contributors","chapter":"Preface","heading":"0.4 Authors and contributors","text":"document maintained community.\nreading document, can contributor well.\nquality document relies .Authors: contributed majority content least one chapter.\nKeaven Anderson <keaven_anderson@merck.com>, Yilong Zhang, Nan Xiao, Yujie ZhaoAuthors: contributed majority content least one chapter.Keaven Anderson <keaven_anderson@merck.com>, Yilong Zhang, Nan Xiao, Yujie ZhaoContributors: contributed least one commit source code.\ngrateful improvements brought contributors (chronological order): Name (@octocat).Contributors: contributed least one commit source code.grateful improvements brought contributors (chronological order): Name (@octocat).","code":""},{"path":"background.html","id":"background","chapter":"1 Background","heading":"1 Background","text":"","code":""},{"path":"background.html","id":"nph","chapter":"1 Background","heading":"1.1 Non-proportional hazards","text":"Non-proportional hazards (NPH) common see delayed effect.\nOne example given KEYNOTE040, targets cure recurrent head neck squamous cell carcinoma.\ntwo arms, one pembrolizumab, standard care (SOC).\nrandomization ratio 1:1 total sample size 495.\nprimary endpoint overall survival (OS) 90% power HR=0.7, 1-sided \\(\\alpha=0.025\\)\ndetailed design can referred protocol).two interim analysis planned, one 144 deaths (information fraction 0.42) 216 deaths (information fraction 0.64).\nefficacy boundary Hwang-Shih-DeCani (HSD) spending \\(\\gamma=-4\\), futility boundary non-binding \\(\\beta\\)-spending HSD \\(\\gamma = -16\\).\nfinal analysis finds 388 deaths, compared 340 planned death.\n1-sided nominal p-value OS final analysis 0.0161.\nFIGURE 1.1: KEYNOTE040\nfigure , can see delay treatment effect.delayed effect just oncology, also lies TA.\nexample, cholesterol lowering mortality (Miettinen et al 1997), also delayed effect.\nFIGURE 1.2: Scandinavian Simvistatin Survival Study\naddition delayed effect, NPH also exists cross survival curves.\nexample, KEYNOTE061 (recurrent advanced gastric gastro-oesophageal junction cancer), cross pembrolizumab arm paclitaxel arm (see figure ).\n1:1 randomization 360 planned events (395 actual events) end trial CPS \\(\\ge\\) 1 population.\nfirst primary endpoints OS PD-L1 CPS \\(\\ge\\) 1.\npower 91% HR=0.67 1-sided \\(\\alpha=0.0215\\).\nsecondary primary endpoints PFS PD-L1 CPS \\(\\ge\\) 1.\ndetailed design can found protocolWhen proceeded final analysis, 326 deaths 290 planned.\nOne interim analysis conducted 240 events information fraction 0.83.\ngroup sequential design, efficacy boundary Hwang-Shih-DeCani (HSD) spending \\(\\gamma=-4\\) futility boundary.\n1-sided nominal p-value OS 0.0421 (threshold: p=0.0135).\nPost hoc FH(\\(\\rho=1,\\gamma=1\\)) p=0.0009.\nFIGURE 1.3: KEYNOTE061\ntwo examples motivate us think following questions:impact (potentially) delayed treatment effect trial design analysis.Test statistics used (logrank, weighted logrank test max combo test)Sample size vs duration follow-upTiming analysesFutility boundsUpdating boundsMultiplicity adjustments","code":""},{"path":"background.html","id":"gsd","chapter":"1 Background","heading":"1.1.1 Group sequential design","text":"group sequential design, groups observations collected repeatedly analyzed, controlling error rates.\ngroup sequential design ongoing, stopping rule specifies trial might halted points.\nstopping rule usually consists two components, test statistics threshold.\nthreshold usually decided spending functions, introduced Section 1.2.\ntest statistics usually Z-process B-process, used monitor treatment effect group sequential design.\nZ-process (B-process) cross boundary, trail stopped.","code":""},{"path":"background.html","id":"z-process-and-b-process","chapter":"1 Background","heading":"1.1.2 Z-process and B-process","text":"Section, introduce Z-process B-process.\nSuppose group sequential design, totally \\(K\\) looks.\n\\(k\\)-th look, \\(n_k\\) available observations.\nnotation simplicity, denote final number available observation \\(N\\), .e., \\(N \\triangleq n_K\\).Definition 1.1  Z-process defined standardized treatment effect, .e.,\n\\[\\begin{equation}\n  Z_{k}\n  =\n  \\frac{\\widehat\\theta_k}{\\sqrt{\\hbox{Var}(\\hat\\theta_k)}},\n  \\tag{1.1}\n\\end{equation}\\]\n\\(k\\) index looks, assume totally \\(K\\) number looks, .e., \\(k = 1, 2, \\ldots, K\\).\nnumerator \\(\\widehat\\theta_k\\) treatment effect estimated \\(k\\)-th look.\\(\\widehat\\theta_k\\), form depends outcome clinical trials.\nclinical trails continuous outcome \\(X_1, X_2, \\ldots \\\\mathbb R\\), treatment effect estimated \\(k\\)-th look \n\\[\\begin{equation}\n  \\widehat{\\theta}_k\n  =\n  \\frac{\\sum_{=1}^{n_k} X_{}}{n_k}\\equiv \\bar X_{k},\n  \\tag{1.2}\n\\end{equation}\\]\n\\(n_k\\) number available observations \\(k\\)-th look.\nclinical trials binary outcome \\(X_1, X_2, \\ldots \\\\mathbb \\{0, 1\\}\\), \\(\\widehat\\theta_k\\) can estimated equation (1.2).\nclinical trials survival outcome, \\(\\widehat\\theta_k\\) typically represent Cox model coefficient representing logarithm hazard ratio experimental vs. control treatment.\n\\(n_k\\) represent planned number events \\(k\\)-th look.discussing Z-process, let us take look B-process.Definition 1.2  B-process defined \n\\[\\begin{equation}\n  B_{k} = \\sqrt{t_k} Z_k,\n  \\tag{1.3}\n\\end{equation}\\]\n\\(Z_k\\) defined Definition 1.1 \\(t_k\\) information fraction \\(k\\)-th look, .e.,\n\\[\n  t_k = \\mathcal I_k / \\mathcal I_K,\n\\]\n\\(\\mathcal I_k\\) information \\(k\\)-th look, defined \n\\[\n  \\mathcal I_k \\triangleq \\frac{1}{\\text{Var}(\\widehat\\theta_k)}.\n\\]information sequence plays important role group sequential design.\nexample, clinical trial continuous outcome \\(X_1, X_2, \\ldots \\overset{..d.}{\\sim} N(\\mu, \\sigma^2)\\), information \\(k\\)-th look \\(\\mathcal I_k = n_k\\) \\(n_k\\) number available observations \\(k\\)-th look.\nlogic applies binary outcome.\nAnother example clinical trial survival outcome. case, information \\(k\\)-th look \\(\\mathcal I_k = n_k\\) \\(n_k\\) number events \\(k\\)-th look.","code":""},{"path":"background.html","id":"sec:EcfCf","chapter":"1 Background","heading":"1.1.3 Canonical form","text":"group sequential design, given total \\(K\\) look, sequences Z-process B-process, .e., \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) \\(\\{B_k\\}_{k = 1, \\ldots, K}\\).\nCF refers joint distribution \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) \\(\\{B_k\\}_{k = 1, \\ldots, K}\\), including distribution, expectation mean, variance covariance.\nPlease note distribution asymptotic, asymptotic conditions usually requires sample size relative large.\nProperly speaking, \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) \\(\\{B_k\\}_{k = 1, \\ldots, K}\\) depends sample size, can re-denoted \\(\\{ Z_{k,N} \\}_{k = 1, \\ldots, K}\\) \\(\\{ B_{k, N} \\}_{k = 1, \\ldots, K}\\).\nsample size relatively large, can get rid sample size, .e.,\n\\[\n  Z_k = \\lim\\limits_{N \\\\infty} Z_{k,N}.\n\\]\nasymptotic assumed throughout book.important assumption CF \\(E(\\widehat\\theta_k) = \\theta\\), \\(\\theta\\) constant \\(\\widehat\\theta\\) defined equation (1.2).Theorem 1.1  Suppose group sequential design, totally \\(K\\) looks.\n\\(k\\)-th look, \\(n_k\\) available observations, .e., \\(\\{X_i\\}_{= 1, \\ldots, n_k}\\).\nAssume \\(\\text{Var}(X_i) = 1\\) \\(= 1, 2, \\ldots, N\\).\ndenote \\(\\mathcal I_k \\triangleq \\frac{1}{\\text{Var}(\\widehat\\theta(t_k))}\\) \\(E(\\widehat\\theta_k) = \\theta\\).\njoint distribution Z-process \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) \n\\[\n\\left\\{\n\\begin{array}{l}\n  (Z_1, \\ldots, Z_K) \\text{ multivariate normal} \\\\\n  E(Z_k) =  \\sqrt{\\mathcal{}_k}\\theta \\\\\n  \\hbox{Var}(Z_k)  =  1 \\\\\n  \\text{Cov}(Z_i, Z_j)  =  \\sqrt{t_i/t_j} \\;\\; \\forall 1 \\leq \\leq j \\leq K\n\\end{array}\n\\right..\n\\]\n\\(t_i = n_i / N\\) \\(N \\triangleq n_K\\) final number observations last look.\nstatistics distribution, declare canonical joint distribution information levels \\(\\{\\mathcal I_1, \\ldots, \\mathcal I_K \\}\\) parameter \\(\\theta\\).theorem, found CF treats \\(E(\\widehat\\theta)\\) constant, .e., \\(\\theta\\).\nFollowing similar, can also summarize distribution B-process.Theorem 1.2  Following setting Theorem 1.1\ndistribution B-process \n\\[\n\\left\\{\n\\begin{array}{l}\n  (B_1, \\ldots, B_K) \\text{ multivariate normal} \\\\\n  E(B_k) = \\sqrt{t_{k}\\mathcal{}_k}\\theta = t_k \\sqrt{\\mathcal{}_K} \\theta = \\mathcal{}_k \\theta / \\sqrt{\\mathcal{}_K} \\\\\n  \\hbox{Var}(B_k) = t_k \\\\\n  \\text{Cov}(B_i, B_j) = t_i \\;\\; \\forall 1 \\leq \\leq j \\leq K\n\\end{array}\n\\right.,\n\\]\n\\(t_i = n_i/N\\) \\(N \\triangleq n_K\\) final number observations last look.noted correlation B-process covariance Z-process, .e.,\n\\[\\begin{equation}\n  Corr(B_i, B_j) = Cov(Z_i, Z_j) = \\sqrt{t_i/t_j} \\;\\; \\forall 1 \\leq \\leq j \\leq K.\n\\end{equation}\\]\nlearn covariance/correlation better, let us take continuous outcome example.\nSuppose \\(K\\) looks clinical trial, \\(k\\)-th look, \\(n_k\\) available observations outcome \\(\\{ X_i \\}_{= 1, \\ldots, n_k}\\).\nexample, \n\\[\\begin{eqnarray}\n  B_k\n  & = &\n  \\sqrt{t_k} Z_k\n  =\n  \\sqrt{\\mathcal I_k / \\mathcal I_K} \\frac{\\widehat\\theta_k}{\\sqrt{Var(\\widehat\\theta_k)}}\n  =\n  \\mathcal I_k\\sqrt{ 1 / \\mathcal I_K}  \\widehat\\theta_k \\\\\n  & = &\n  \\mathcal I_k\\sqrt{ 1 / \\mathcal I_K}  \\frac{\\sum_{=1}^{n_k} X_i}{n_k}\n  =\n  \\frac{\\sum_{=1}^{n_k} X_i}{\\sqrt{I_K}}\n  =\n  \\frac{\\sum_{=1}^{n_k} X_i}{\\sqrt{N}}.\n\\end{eqnarray}\\]\nGiven B-process, \\(1 \\leq \\leq j \\leq K\\), \n\\[\n  B_j - B_i \\sim N(\\mathcal I_j \\theta(t_j) - \\mathcal I_i \\theta(t_i), t_j - t_i)\n\\]\nindependent \\(B_1, B_2, \\ldots, B_i\\).\n\n\\[\\begin{eqnarray}\n  Corr(B_i, B_j)\n  & = &\n  Cov(B_i, B_j) \\bigg / \\sqrt{Var(B_i) Var(B_j)} \\\\\n  & = &\n  t_i \\bigg / \\sqrt{t_i t_j} = \\sqrt{t_i / t_j}   \\\\\n  & = &\n  Cov(Z_i, Z_j)\n\\end{eqnarray}\\]","code":""},{"path":"background.html","id":"sec:EcfEcf","chapter":"1 Background","heading":"1.1.4 Extended canonical form","text":"main difference extended canonical form (ECF) canonical form (CF) , ECF treats \\(E(\\widehat\\theta_k)\\) time-varying parameter, .e., \\(\\theta(t_k)\\).\nCF treats constant \\(\\theta\\).\nsummarize joint distribution Z-process B-process ECF followsTheorem 1.1  Suppose group sequential design, totally \\(K\\) looks.\n\\(k\\)-th look, \\(n_k\\) available observations, .e., \\(\\{X_i\\}_{= 1, \\ldots, n_k}\\).\nAssume \\(\\text{Var}(X_i) = 1\\) \\(= 1, 2, \\ldots, N\\).\ndenote \\(\\mathcal I_k \\triangleq \\frac{1}{\\text{Var}(\\widehat\\theta(t_k))}\\) \\(E(\\widehat\\theta_k) = \\theta(t_k)\\).\njoint distribution Z-process \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) \n\\[\n\\left\\{\n\\begin{array}{l}\n  (Z_1, \\ldots, Z_K) \\text{ multivariate normal} \\\\\n  E(Z_k) =  \\sqrt{\\mathcal{}_k} \\theta(t_k) \\\\\n  \\hbox{Var}(Z_k)  =  1 \\\\\n  \\text{Cov}(Z_i, Z_j)  =  \\sqrt{t_i/t_j} \\;\\; \\forall 1 \\leq \\leq j \\leq K\n\\end{array}\n\\right..\n\\]\njoint distribution B-process \\(\\{B_k\\}_{k = 1, \\ldots, K}\\) \n\\[\n\\left\\{\n\\begin{array}{l}\n  (B_1, \\ldots, B_K) \\text{ multivariate normal} \\\\\n  E(B_k) = \\sqrt{t_{k}\\mathcal{}_k}\\theta(t_k) = t_k \\sqrt{\\mathcal{}_K} \\theta(t_k) = \\mathcal{}_k\\theta(t_k)/\\sqrt{\\mathcal{}_K}\\\\\n  \\hbox{Var}(B_k) = t_k \\\\\n  \\text{Cov}(B_i, B_j) = t_i \\;\\; \\forall 1 \\leq \\leq j \\leq K\n\\end{array}\n\\right.,\n\\]\n\\(t_i = n_i / N\\) \\(N \\triangleq n_K\\) final number observations last look.\nstatistics distribution, declare canonical joint distribution information levels \\(\\{\\mathcal I_1, \\ldots, \\mathcal I_K \\}\\) parameter \\(\\theta\\).","code":""},{"path":"background.html","id":"sf","chapter":"1 Background","heading":"1.2 Spending function bounds","text":"section, introduce spending function bounds group sequential design.\nstart definitions categories.\nintroduce three frequently used spending functions, (1) Haybittle boundary, (2) Pocock boundary (3) O’Brien-Fleming boundary.Suppose \\(K\\) analyses conducted information fraction \\(t_1, t_2, \\ldots, t_k, \\ldots, t_K\\).\ncall \\(\\{a_k, b_k \\}_{k = 1,\\ldots, K}\\) decision boundaries, reject \\(H_0\\) \\(Z(t_k) < a_k\\) \\(Z(t_k) > b_k\\).\ncall \\(\\{b_k\\}_{k=1,\\ldots,K}\\) efficiency boundary, decided control type error, \\(\\{a_k\\}_{k=1,\\ldots,K}\\) futility boundary, used control boundary crossing probabilities.calculation boundaries, typically two methods.\nfirst one called error spending approach, specifies boundary crossing probabilities analysis.\ncommonly done error spending function approach (Lan DeMets 1983).\nsecond one called boundary family approach, specifies big boundary values relative adjust relative values constant multiple control overall error rates.\ncommonly applied boundary family include:Haybittle boundary (Haybittle 1971);Wang-Tsiatis boundary (Wang Tsiatis 1987):\nPocock boundary (Pocock 1977);\nO’Brien Fleming boundary (O’Brien Fleming 1979).\nPocock boundary (Pocock 1977);O’Brien Fleming boundary (O’Brien Fleming 1979).remaining section, introduce spending functions one one.","code":""},{"path":"background.html","id":"haybittle-boundary","chapter":"1 Background","heading":"1.2.1 Haybittle boundary","text":"\nFIGURE 1.4: Haybittle boundary\nHaybittle boundary three modified versions.first modified version uses Bonferroni adjustment: first \\(K-1\\) analyses, significant p-value set 0.001; final analysis, significant p-value set \\(0.05 - 0.01 \\times (K-1)\\).\nmodification helps avoid type error inflation.\nBesides, require equally spaced terms information can used regardless joint distribution Z-score, .e., \\(Z(t_1),...,Z(t_k)\\).second modified version sets Z-score 4 first half study set Z-score boundary 3 thereafter.third modified version requires crossing boundary two successive looks.summary, Haybittle boundary uses piecewise constant boundary, make easy implement.\nkind conservative selection constant boundary (e.g., 3 1.96) needs justifications.","code":""},{"path":"background.html","id":"wang-tsiatis-boundary","chapter":"1 Background","heading":"1.2.2 Wang-Tsiatis boundary","text":"2-sided testing, Wang Tsiatis (1987) defined boundary function \\(k\\)-th look \n\\[\n  \\Gamma(\\alpha, K, \\Delta) k^{\\Delta - 0.5},\n\\]\n\\(\\Gamma(\\alpha, K, \\Delta)\\) constant chosen level significance equal \\(\\alpha\\).two selection \\(\\Delta\\), Wang-Tsiatis boundary gives two special cases.\n\\(\\Delta = 0.5\\), Pocock bounds.\n\\(\\Delta = 0\\), O’Brien-Fleming bounds.","code":""},{"path":"background.html","id":"pocock-boundary","chapter":"1 Background","heading":"1.2.3 Pocock boundary","text":"2-sided testing, Pocock procedure rejects \\(k\\)-th \\(K\\) looks \n\\[\n  |Z(k/K)| > c_P(K),\n\\]\n\\(c_P(K)\\) fixed given number total looks \\(K\\) chosen \\(\\text{Pr}(\\cup_{k=1}^{K} |Z(k/K)| > c_P(K)) = \\alpha\\).\nFIGURE 1.5: Pocock boundary\nexample Pocock boundary.reject \\(H_0\\) \\(|Z(k/4)| > 2.361\\) \\(k = 1,2,3,4\\) (final analysis).summary, Pocock boundary special case Wang-Tsiatis boundary (\\(\\Delta = 0.5\\)) constant Z-score boundaries, .e., \\(|Z(k/K)| > c_P(K)\\).\nconstant boundary \\(c_P(K)\\) decided type error rate total number looks.\nmain weakness high price end trial requirement equally spaced looks.","code":""},{"path":"background.html","id":"obrien-fleming-boundary","chapter":"1 Background","heading":"1.2.4 O’Brien-Fleming boundary","text":"O’Brien-Fleming boundary conservative beginning, gives realtively large boundary first.\ngives nominal value close overall value design \\(\\approx 1.96\\) final stage.\nvisualization can found follows.\nFIGURE 1.6: OBrien-Fleming boundary\nexample O’Brien-Fleming boundary.Since tabled value 2.024 flat B-value boundary.\nB-value boundary can easily transformed decreasing Z-score boundary \\(Z(t) = B(t)/\\sqrt{t}\\):\\(2.024/\\sqrt{1/4} = 4.05\\)\\(2.024/\\sqrt{2/4} = 2.86\\)\\(2.024/\\sqrt{3/4} = 2.34\\)\\(2.024/\\sqrt{4/4} = 2.02\\)summary, O’Brien-Fleming boundary designs decreasing Z-score boundary, large boundaries early stage ~1.96 boundary approaches final stage.Although Pocock boundary OBF boundary belonging Wang-Tsiatis boundary’s family.\ntwo differences .first difference , Pocock boundary flat Z-score boundaries; O’Brien-Fleming decreasing Z-score boundary.\nAccordingly, O’Brien-Fleming boundary makes much difficult stop early Pocock boundary.\nHowever, O’Brien-Fleming boundary extracts much smaller price end.\nFIGURE 1.7: Pocock (circles) O’Brien-Fleming (squares) z-score boundaries four looks\nsecond difference , different performance cumulative type error rate (probability rejection \\(t_j\\), .e., \\(\\text{Pr}(\\cup_{=1}^j |Z(t_i)| > c_i)\\)).Pocock cumulative type error rate increases sharply first, much less toward end.O’Brien-Fleming behaves just opposite way.\nFIGURE 1.8: Cumulative type error rate used Pocock (circles) O’Brien-Fleming (squares) procedures four looks.\nconclusion, visualize three spending function following figure.\nFIGURE 1.9: Three boundaries\n","code":""},{"path":"background.html","id":"statistical-information-and-time","chapter":"1 Background","heading":"1.3 Statistical information and time","text":"Statistical information time key component group sequential design.\nsection, introduce statistical information three different outcomes, (1) continuous outcome, (2) binary outcome (3) survival outcome.start continuous outcome.\nSuppose \\(T\\)-month trial, interim analysis conducted \\(\\tau\\) month \\(n\\) \\(N\\) planned observations evaluated arm.\n\\(n\\) available observation denoted \\(\\{X_i\\}_{= 1, \\ldots, n}\\) control arm \\(\\{Y_i\\}_{= 1, \\ldots, n}\\) treatment arm.\nThus, treatment effect can estimated \n\\[\n  \\widehat\\delta(\\tau) = \\frac{\\sum_{=1}^n (Y_i - X_i)}{n}.\n\\]\n\\(Y_i - X_i\\), commonly assume \\(Y_i - X_i \\overset{..d.}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) \\(= 1, \\ldots, n\\), \\(\\mu = 0\\) null hypothesis \\(\\mu \\neq 0\\) alternative hypothesis.information \\(\\tau\\)-th month \n\\[\n  (\\tau) = 1/\\text{Var}(\\widehat\\delta(\\tau)) = n/\\sigma^2.\n\\]\nAccordingly, define information fraction ratio interim information final information, .e.,\n\\[\n  t = (\\tau)/(T)= \\frac{n/\\sigma^2}{N/\\sigma^2} = \\frac{n}{N}.\n\\]binary outcome, follows similar logic continuous outcome.\ncommonly assume \\(Y_i - X_i \\overset{..d.}{\\sim} \\text{Bernoulli}(p)\\) \\(= 1, \\ldots, n\\), \\(p = 0.5\\) null hypothesis \\(p \\neq 0.5\\) alternative hypothesis.\ninformation \\(\\tau\\)-th month \n\\[\n  (\\tau) = 1/\\text{Var}(\\widehat\\delta(\\tau)) = \\frac{n}{p(1-p)}.\n\\]\nAccordingly, information fraction \n\\[\n  t = (\\tau)/(T)= \\frac{\\frac{n}{p(1-p)}}{\\frac{N}{p(1-p)}} = \\frac{n}{N}.\n\\]survival outcome, little bit complicated.\nSuppose interim analysis conducted \\(\\tau\\) month \\(d\\) \\(D\\) planned events evaluated arm.\ntreatment effect survival outcome usually measured log rank test, .e.,\n\\[\n  \\widehat\\delta(\\tau)\n  =\n  \\frac{\\sum_{=1}^d (O_i - E_i)}{\\sum_{= 1}^d V_i}\n  \\triangleq\n  \\frac{S(\\tau)}{\\sum_{= 1}^d V_i}.\n\\]\nAccordingly, information fraction \n\\[\n\\begin{array}{ccl}\n  t\n  & = & \\text{Var}(S(\\tau)) / \\text{Var}(S(1)) \\\\\n  & = & \\text{Var}(\\sum_{=1}^{d} (O_i - E_i)) \\bigg / \\text{Var}(\\sum_{=1}^{D} (O_i - E_i)) \\\\\n  & = & \\sum_{=1}^{d} \\text{Var}(O_i - E_i) \\bigg / \\sum_{=1}^{D} \\text{Var}(O_i - E_i) \\\\\n  & = & \\sum_{=1}^{d} E_i(1-E_i) \\bigg / \\sum_{=1}^{D} E_i(1-E_i) \\\\\n  & \\approx & \\sum_{=1}^{d} 0.5(1-0.5) \\bigg / \\sum_{=1}^{D} 0.5(1-0.5) \\\\\n  & = & d/D.\n\\end{array}\n\\]Comparing survival outcome continuous/binary outcome, found information (fraction) survival outcome decided number events, decided number available observations continuous/binary outcome.multiple interim analysis \\(\\tau_1\\)-th, \\(\\tau_2\\)-th, \\(\\ldots\\), \\(\\tau_K\\)-th month, get sequence information \\((\\tau_1), (\\tau_2), \\ldots, (\\tau_K)\\) sequence information fraction \\(t_1, t_2, \\ldots, t_K\\).\nPlease note , information sequence sometimes simplified \\(I_1, I_2, \\ldots, I_K\\) (Jennison Turnbull 2000).","code":""},{"path":"proportional-hazards.html","id":"proportional-hazards","chapter":"2 Proportional hazards","heading":"2 Proportional hazards","text":"consider designs assumption proportional hazards chapter applying gsDesign package.\neasily developed using gsDesign Shiny app https://rinpharma.shinyapps.io/gsdesign/.\nsaved designs can re-loaded app bookdown website https://github.com/keaven/gsd-deming;\ncopy repository local PC, able easily access designs.brief review methodology, recreate designs evaluate final inference multiple published trials:begin KEYNOTE 189 trial (Gandhi et al 2018) show straightforward, common application metastatic (high-risk) cancer.proceed AFCAPS/TEXCAPS trial (Downs et al 1997) (Downs et al 1998), cardiovascular endpoint trial much lower risk metastatic oncology trial.Next look EXAMINE trial (White et al 2013), trial diabetic patients evaluating non-inferiority hypothesis cardiovascular outcomes.also create design hypothetical trial oncology long-term plateau endpoints likely. approach proportional hazards cure model.\nalso demonstrate use calendar-based spending example.","code":""},{"path":"proportional-hazards.html","id":"lachin-foulkes","chapter":"2 Proportional hazards","heading":"2.1 Lachin and Foulkes method","text":"gsDesign R package focuses sample size power derivation primarily method Lachin Foulkes (Lachin Foulkes 1986a).\nallows simple specification piecewise constant enrollment, failure rates dropout rates can approximate general distributions .\nsample size implementation focuses primarily increasing enrollment rate achieve power within targeted trial duration, also option fix maximum enrollment rate extend follow-power achieved Kim Tsiatis (1990); method always work without adjustment assumptions.\nmethods focus 2-arm trials constant treatment effect time (proportional hazards).\nassumed (stratified) logrank test Wald test Cox model coefficient used statistical testing.\nnumber events drives power, regardless study duration, noted Schoenfeld (1981); approximation event requirements provided nSurv() function fixed design gsSurv() group sequential design can seen code generated app gsDesign package documentation.One place deviate Lachin Foulkes method approximate hazard ratio study bounds.\nalways considered approximation informative reviewing design rather requirement cross bound.\ncase, approximation Schoenfeld (1981) quite simple implemented function gsDesign::gsHR().","code":""},{"path":"proportional-hazards.html","id":"mediansurvival","chapter":"2 Proportional hazards","heading":"2.2 Metastatic oncology example","text":"KEYNOTE 189 trial (Gandhi et al (2018)) evaluated progression free survival (PFS) overall survival (OS) patients previously untreated metastatic non-small cell lung cancer (NSCLC).\nPatients treated chemotherapy, randomized 2:1 add-pembrolizumab placebo.\nimportant finding : median follow-10.5 months, estimated rate overall survival 12 months 69.2% (95% confidence interval [CI], 64.1 73.8) pembrolizumab-combination group versus 49.4% (95% CI, 42.1 56.2) placebo-combination group (hazard ratio death, 0.49; 95% CI, 0.38 0.64; P<0.001) (Gandhi et al (2018)).design allocated 0.025 family-wise error rate (FWER, also referred 1-sided Type error) PFS (\\(\\alpha=0.095\\)) OS (\\(\\alpha=0.015\\)).\ngraphical method \\(\\alpha\\)-control group sequential design Maurer Bretz (2013) used control Type error.\nreview design OS , attempt exactly reproduce details.\nKey aspects design documented protocol accompanying Gandhi et al (2018) :\\(\\alpha=0.0155\\).Control group survival follows exponential distribution median 13 months.Exponential dropout rate 0.1% per month.90% power detect hazard ratio 0.7.2:1 randomization, experimental:control.Enrollment 1 year; specified protocol, assumed.Trial duration approximately 35 months.Observed deaths 240, 332 416 3 planned analyses; yields information fractions 0.5769231 0.7980769 interim analyses.one-sided bound using Lan DeMets (1983) spending function approximating O’Brien-Fleming bound.adjusted exponential dropout rate 0.133% per month hazard ratio 0.70025 following targeted events targeted sample size close integers.\nsort adjustment can limit confusion computation interim bounds demonstrated.design derived :describe information table follows:IA percent percent statistical information (planned events)\ninterim analysis relative final analysis.\nmatches timing input.note sample size (N) events rounded table,\ndesign stores continuous values numbers exactly match\ninput timing analyses. Thus, calculating rounded numbers\ntable may exactly match IA % table.calendar timing analysis expected timing relative \nopening enrollment.Z-values standard normal test statistics positive values\nrepresenting treatment benefit experimental group.p-values nominal p-values corresponding Z-values.~HR bound asymptotic approximation hazard ratio required\ncross bound. criterion must met Type error control.P(Cross) HR=1 cumulative probability crossing bound\n(efficacy futility) given analysis. note \ndefault non-binding Type error option P(Cross) HR=1\nless specified Type error. Type error\ncomputed ignoring futility bound since advisory .\nactual cumulative Type error spending \n0.001439, 0.00674, 0.0155.P(Cross) HR =0.7 cumulative probability crossing\nbound alternate hypothesis.design updated interim analysis 235 deaths follows.\ncode can written app , provided wish dive details.\nnote function gsDesign() used update gsSurv() used original design.\ngsSurv() function useful approximate calendar timing event accrual, something longer needed time interim analysis.\nNote gsBoundSummary() call specifically state treatment differences table hazard ratio scale well interpret event count key calculating information fraction analysis.p-value reported interim 1 <0.001 (Gandhi et al 2018), establishing statistical significance relative bound 0.00128 summary table well publication.","code":"\nlibrary(gsDesign)\nlibrary(dplyr)\nlibrary(gt)\n\nKEYNOTE189design <- gsSurv(\n  # Number of analyses\n  k = 3,\n  # One-sided design\n  test.type = 1,\n  # 1-sided Type I error\n  alpha = 0.0155,\n  # Type II error (1 - targeted power)\n  beta = 0.1,\n  # Timing (information fraction) at interim analyses\n  timing = c(0.5769231, 0.7980769),\n  # Efficacy bound spending function (no spending parameter needed)\n  sfu = sfLDOF,\n  # Control group exponential failure rate to get median OS of 13 months\n  lambdaC = log(2) / 13,\n  # Alternate hypothesis hazard ratio\n  hr = 0.70025,\n  # Dropout rate (exponential rate)\n  eta = 0.00133,\n  # Enrollment rates during ramp-up period\n  gamma = c(2.5, 5, 7.5, 10),\n  # Relative enrollment rate time period durations\n  R = c(2, 2, 2, 6),\n  # Calendar time of final analysis\n  T = 35,\n  # Minimum follow-up time after enrollment complete\n  minfup = 23,\n  # Randomization ratio (experimental/placebo)\n  ratio = 2\n)\nKEYNOTE189design %>%\n  gsBoundSummary(ratio = 2, digits = 5, ddigits = 2, tdigits = 1, timename = \"Month\") %>%\n  gt() %>%\n  tab_header(title = \"KEYNOTE189 OS design\")\nx <- KEYNOTE189design\nxu <- gsDesign(\n  # First 3 parameters are all updated\n  k = x$k, # This must be at least 2\n  n.I = c(235, ceiling(x$n.I[2:3])), # Enter integers for actual events\n  # This parameter sets spending (information) fraction as\n  # planned final event count from the original trial design\n  maxn.IPlan = x$n.I[x$k],\n  # REMAINING PARAMETERS COPIED FROM ORIGINAL DESIGN\n  test.type = x$test.type, alpha = x$alpha, beta = x$beta,\n  astar = x$astar, sfu = x$upper$sf, sfupar = x$upper$param,\n  sfl = x$lower$sf, sflpar = x$lower$param,\n  delta = x$delta, delta1 = x$delta1, delta0 = x$delta0\n)\n\n# Now we summarize bounds in a table\nxu %>%\n  gsBoundSummary(\n    deltaname = \"HR\", # Name of treatment difference measure\n    logdelta = TRUE, # For survival, delta is on a log scale; this transforms\n    Nname = \"Events\", # \"N\" for analyses is events for survival analysis\n    digits = 5, # decimals for numbers in body of table\n    ddigits = 2, # decimals for natural parameter; HR in this case\n    tdigits = 1, # Time digits (not needed here)\n    # We select key parameters for printing\n    exclude = c(\n      \"B-value\", \"CP\", \"CP H1\", \"PP\",\n      paste0(\"P(Cross) if HR=\", round(c(x$hr0, x$hr), digits = 2))\n    )\n  ) %>%\n  gt() %>%\n  tab_header(\n    title = \"Updated bounds for interim analysis\",\n    subtitle = \"KEYNOTE 189 trial\"\n  )"},{"path":"proportional-hazards.html","id":"cardiovascular-outcomes-reduction","chapter":"2 Proportional hazards","heading":"2.3 Cardiovascular outcomes reduction","text":"AFCAPS/TEXCAPS trial evaluated use lovastatin reduce cardiovascular outcomes.\ndesign described Downs et al (1997) results reported Downs et al (1998).\napproximate design evaluation statistical significance .\nSoftware assumptions model completely clear, exact reproduction design, close.\nKey assumptions replicate :5 years minimum follow-patients enrolled.Interim analyses 0.375 0.75 final planned event count accrued.2-sided bound using Hwang et al (1990) spending function parameter \\(\\gamma = -4\\) approximate O’Brien-Fleming bound.arbitrarily set following parameters match design:\nPower 90% hazard ratio 0.6921846; slightly different 0.7 hazard ratio suggested Downs et al (1997) due use Lachin Foulkes (1986a) method used gsSurv() routine .\nEnrollment duration 1/2 year constant enrollment.\nexponential failure rate 0.01131 per year nearly identical annual dropout rate 0.01125.\nexponential dropout rate 0.004 per year nearly identical annual dropout rate 0.00399.\nPower 90% hazard ratio 0.6921846; slightly different 0.7 hazard ratio suggested Downs et al (1997) due use Lachin Foulkes (1986a) method used gsSurv() routine .Enrollment duration 1/2 year constant enrollment.exponential failure rate 0.01131 per year nearly identical annual dropout rate 0.01125.exponential dropout rate 0.004 per year nearly identical annual dropout rate 0.00399.design saved can reloaded gsDesign Shiny app book website data directory file AFCAPSDesign.rds.\nprovide code generated design.\nbegins loading gsDesign package setting parameters design associated comments help explain; default comments updated .\nparameters needed given sometimes needed code generated app.\nComments indicate case.\nCode comments largely copied app, may help reader interpret see .Now plug parameters gsSurv() generate design.\nSee comments interpretation parameters.reader can see summaries following commands run :details available list items returned; see help file try names(x) see available.\nexamples, look number events nominal p-value bounds design.\nNote events planned continuous numbers rather integers.\nParameters chosen design close less whole numbers.\nrequired, can limit misinterpretations.nominal 1-sided p-value bounds also seen gsBoundSummary(x) command .Doubling 1-sided bounds, can compare bounds Downs et al (1997): group sequential boundary (2-sided, \\(\\alpha = 0.05\\)) scheduled analyses 2.947, 2.411, 2.011, correspond p-values 0.0032, 0.0159, 0.0443, respectively. see results nearly identical.Downs et al (1998) *average follow-5.2 years, lovastatin reduced incidence first acute major coronary events (183 vs 116 first events; relative risk [RR], 0.63; 95% confidence interval [CI], 0.50-0.79; P<.001).` second interim analysis. see immediately 299 events less second planned analysis 332 events.\nbounds easily adapted app. provide code generated design update provided app. assumed first interim performed planned 240 events.\nFirst, enter updated event counts.Based can update design.\nNote now using gsDesign() rather gsSurv() calendar time longer relevant.inspect resulting bounds, run gsBoundSummary() .\nSeveral parameters useful get desired results noted comments.\nnote either Z p-value bound needed.\nanalysis 2, nominal 1-sided bound 0.0183 clearly exceeded p < 0.001 reported manuscript, confirming statistical significance trial.\nSince know actual events interim 1 reported, final bound trial may slightly different.\ncan see much changes trying different event counts updated design specification app.","code":"\n# Number of analyses\nk <- 3\n# See gsDesign() help for description of boundary types\ntest.type <- 2 # This indicates a 2-sided design\n# 1-sided Type I error\nalpha <- 0.025\n# Type II error (1 - targeted power)\nbeta <- 0.1\n# If test.type = 5 or 6, this sets maximum spending for futility\n# under the null hypothesis. Otherwise, this is ignored.\n# (astar did not need to be specified in this case)\nastar <- 0\n# Timing (information fraction) at interim analyses\ntiming <- c(0.375, 0.75)\n# Efficacy bound spending function\nsfu <- sfHSD\n# Upper bound spending function parameters, if any\nsfupar <- -4\n# NOTE THAT THE NEXT 2 PARAMETERS ARE NOT NEEDED AS THE\n# LOWER BOUND WAS SPECIFIED IN sfu and sfupar SINCE test.type=2 was specified.\n# Lower bound spending function, if used (test.type > 2)\nsfl <- sfLDOF\n# Lower bound spending function parameters, if any\nsflpar <- c(0)\n# Assumed hazard ratio under alternate hypothesis\nhr <- 0.6921846\n# Null hypothesis hazard ratio (this is a default; for superiority trials)\nhr0 <- 1\n# Dropout rate (exponential rate)\neta <- 0.004\n# Enrollment rate (will be inflated by gsSurv to achieve power)\ngamma <- 10\n# Relative enrollment rates by time period\nR <- 0.5\n# Interval durations for piecewise failure rates\n# not needed since there is only 1 rate;\n# last interval always extends indefinitely (to infininity),\n# so it does not need to be specified\nS <- NULL\n# Calendar time of final analysis\nT <- 5.5\n# Minimum follow-up time after enrollment complete\nminfup <- 5\n# Relative enrollment (experimental/placebo)\nratio <- 1\n# This gets the single control group failure rate\n# (complicated by the fact that the app has to allow piecewise rates)\nobs <- matrix(c(100, 0.01131), ncol = 2)\nobs <- obs[(!is.na(obs[, 1])) & (!is.na(obs[, 2])), 2]\nlambdaC <- obs\nx <- gsSurv(\n  k = k, test.type = test.type, alpha = alpha, beta = beta,\n  astar = astar, timing = timing, sfu = sfu, sfupar = sfupar, sfl = sfl,\n  sflpar = sflpar, lambdaC = lambdaC, hr = hr, hr0 = hr0, eta = eta,\n  gamma = gamma, R = R, S = S, T = T, minfup = minfup, ratio = ratio\n)\nsummary(x) # Textual summary of design\ngsBoundSummary(x) # Tabular summary of design\nx$n.I # Events at planned analyses#> [1] 119.9993 239.9986 319.9981\npnorm(-x$upper$bound)#> [1] 0.001623978 0.007976344 0.022198916\nn.I <- c(120, 299, 320)\nku <- length(n.I)\n# The following are not needed here\n# They just specify the spending time as information fraction based on\n# observed events (integers) divided by final planned number of events\n# from the design (continuous)\nusTime <- n.I / x$n.I[x$k]\nlsTime <- usTime\nxu <- gsDesign(\n  # Updated parameters\n  k = ku, # Number of analyses\n  n.I = n.I, # Number of events\n  # Spending time; you don't really need this if you are just using\n  # information (event) fraction\n  usTime = usTime,\n  lsTime = lsTime,\n  # Remaining parameters from original design\n  test.type = x$test.type, alpha = x$alpha, beta = x$beta,\n  astar = x$astar, sfu = x$upper$sf, sfupar = x$upper$param,\n  sfl = x$lower$sf, sflpar = x$lower$param,\n  # maxn.IPlan is key for update to set max planned information\n  maxn.IPlan = x$n.I[x$k],\n  delta = x$delta, delta1 = x$delta1, delta0 = x$delta0\n)\nlibrary(dplyr)\nlibrary(gt)\n\ngsBoundSummary(\n  xu, # Updated design\n  deltaname = \"HR\", # Name of treatment difference measure\n  logdelta = TRUE, # For survival, delta is on a log scale; this transforms\n  Nname = \"Events\", # \"N\" for analyses is events for survival analysis\n  digits = 4, # decimals for numbers in body of table\n  ddigits = 2, # decimals for natural parameter; HR in this case\n  tdigits = 1, # Time digits (not needed here)\n  # We select key parameters for printing\n  exclude = c(\n    \"B-value\", \"CP\", \"CP H1\", \"PP\",\n    paste0(\"P(Cross) if HR=\", round(c(hr0, hr), digits = 2))\n  )\n) %>%\n  gt() %>%\n  tab_header(title = \"Updated AFCAPS bounds at IA 2\")"},{"path":"proportional-hazards.html","id":"cardiovascular-outcomes-non-inferiority","chapter":"2 Proportional hazards","heading":"2.4 Cardiovascular outcomes non-inferiority","text":"EXAMINE trial (White et al (2013)) established non-inferiority major cardiovascular outcomes (MACE) treatment diabetes using DPP4 inhibitor alogliptin compared placebo.\ndesign described White et al (2011).\napproximate design primary analysis evaluation .\nSoftware assumptions model completely clear, exact reproduction Downs et al (1997), close.\nKey assumptions replicate :primary analysis evaluates treatment effect using Cox proportional hazards model primary\nendpoint MACE stratified geographic region screening renal function.\n1-sided repeated confidence interval HR analysis.\nFocus analysis rule HR \\(\\ge 1.3\\), also test superiority.\nAnalyses planned 550, 600, 650 MACE events.\nO’Brien-Fleming-like spending function Lan DeMets (1983).\n2.5% Type error.\nApproximately 91% power.\n3.5% annual MACE event rate.\nUniform enrollment 2 years.\n4.75 years trial duration.\n1% annual loss--follow-rate.\nSoftware: EAST 5 (Cytel).\n1-sided repeated confidence interval HR analysis.Focus analysis rule HR \\(\\ge 1.3\\), also test superiority.Analyses planned 550, 600, 650 MACE events.O’Brien-Fleming-like spending function Lan DeMets (1983).2.5% Type error.Approximately 91% power.3.5% annual MACE event rate.Uniform enrollment 2 years.4.75 years trial duration.1% annual loss--follow-rate.Software: EAST 5 (Cytel).Following plot expected accrual subjects events time\ndesign assumptions alternate hypothesis.\nmay choose ignore code need plot like .Findings trial: primary end point occurred similar rates alogliptin placebo groups (11.3% 11.8% patients, respectively, median exposure 18 months; hazard ratio, 0.96; upper boundary one-sided repeated CI, 1.16; P<0.001 noninferiority (White et al 2013). first interim testing non-inferiority hypothesis rule excess risk HR=1.3. analysis performed 621 events rather planned 550. overrun atypical setting database cutoff prior final data cleanup (blinded) endpoint review.\ncase, updated design.\nenter events interim analysis trial.\nSince gsDesign() requires least 2 analyses, also provide final analysis planned event count even though needed time interim analysis updated time final analysis, needed.Now incorporate update original design.\nprevious examples, arguments copied original design use gsDesign() update rather gsSurv() used original design.Next document updated bounds.\nfirst code line needed case hr0 != 1; .e. non-inferiority trials , case vaccines, super-superiority trials.","code":"\nEXAMINEdesign <- gsSurv(\n  k = 3, # Number of analyses\n  test.type = 1, # 1-sided testing\n  timing = c(550, 600) / 650, # Event fractions at interim analyses\n  beta = 1 - 0.90658, # Type II error (1 - power)\n  alpha = .025, # 1-sided Type I error\n  lambdaC = .035, # Exponential failure rate\n  eta = .0108, # Exponential dropout rate\n  sfu = sfLDOF, # O'Brien-Fleming-like spending\n  hr0 = 1.3, # Non-inferiority margin\n  hr = 1, # Alternate hypothesis is equal risk\n  T = 4.75, # Calendar time of final analysis\n  minfup = 2.75 # Minimum follow-up time after enrollment complete\n)\n\ngsBoundSummary(EXAMINEdesign, digits = 4, ddigits = 2, tdigits = 2, timename = \"Year\") %>%\n  gt() %>%\n  tab_header(title = \"EXAMINE non-inferiority design bounds\")\naccrual_fn <- function(Time, x) {\n  xx <- nEventsIA(tIA = Time, x = x, simple = FALSE)\n  data.frame(\n    Time = Time,\n    Accrual = c(xx$eNC + xx$eNE, xx$eDC + xx$eDE),\n    Count = c(\"Sample size\", \"Events\")\n  )\n}\naccrual <- data.frame(Time = 0, Accrual = c(0, 0), Count = c(\"Sample size\", \"Events\"))\n\nxtime <- (0:50) / 50 * max(EXAMINEdesign$T)\nfor (i in seq_along(xtime[xtime > 0])) {\n  accrual <- rbind(accrual, accrual_fn(Time = xtime[i + 1], x = EXAMINEdesign))\n}\n\nggplot(accrual, aes(x = Time, y = Accrual, col = Count)) +\n  geom_line() +\n  ylab(\"Expected Accrual\") +\n  ggtitle(\"Expected accrual of enrollment and events for EXAMINE trial\")\nn.I <- c(621, 650)\nku <- length(n.I)\n# This is just specifying event fraction vs final planned\n# is used for computing spending.\n# If calendar spending were specified, calendar fraction\n# would be used here.\nusTime <- n.I / EXAMINEdesign$n.I[x$k]\nlsTime <- usTime\nx <- EXAMINEdesign\nxu <- gsDesign(\n  k = ku,\n  test.type = test.type,\n  alpha = x$alpha,\n  beta = x$beta,\n  astar = astar,\n  timing = timing,\n  sfu = sfu,\n  sfupar = sfupar,\n  sfl = sfl,\n  sflpar = sflpar,\n  n.I = n.I,\n  maxn.IPlan = x$n.I[x$k],\n  delta = x$delta,\n  delta1 = x$delta1,\n  delta0 = x$delta0,\n  usTime = usTime,\n  lsTime = lsTime\n)\n# The first line is required to make gsBoundSummary to work correctly\n# for non-inferiority and super-superiority trials\nxu$hr0 <- EXAMINEdesign$hr0\ngsBoundSummary(\n  xu,\n  deltaname = \"HR\",\n  logdelta = TRUE,\n  Nname = \"Events\",\n  digits = 4,\n  ddigits = 2,\n  tdigits = 1,\n  exclude = c(\n    \"B-value\", \"CP\", \"CP H1\", \"PP\"\n  )\n) %>%\n  gt() %>%\n  tab_header(\n    title = \"EXAMINE trial design\",\n    subtitle = \"Update at time of analysis\"\n  )"},{"path":"proportional-hazards.html","id":"exercise","chapter":"2 Proportional hazards","heading":"2.4.1 Exercise","text":"See following link Moderna COVID-19 design replication: https://medium.com/@yipeng_39244/reverse-engineering--statistical-analyses---moderna-protocol-2c9fd7544326Can reproduce using Shiny interface?","code":""},{"path":"proportional-hazards.html","id":"poissonmixture","chapter":"2 Proportional hazards","heading":"2.5 Cure model","text":"consider Poisson mixture model De Castro et al (2010) also referred transformation cure model Zhang Shao (2018) promotion time cure model Zeng et al (2006).\nmodel can useful approximate event accrual trial substantial portion patients expected good long-term result.\nExample applications used estimate long-term survival provided Hellmann et al (2017) well De Castro et al (2010).\nnature uncertainty event accumulation, choose implement calendar-based spending Lan DeMets (1989).\nchoice need carefully evaluated.\nPart suggested rationale trial needs completed certain timeframe event distribution range described course planned survival distribution adequate, extreme long-term proportional hazards assumption may less safe shorter window.Much simpler models 2- 3-piece piecewise exponential model equally effective design nature, just one example approach situation nature.\nlongest example; simpler version demonstrated app.\nReaders immediately interested approach can simply skip rest chapter.","code":""},{"path":"proportional-hazards.html","id":"cure-model-parameters","chapter":"2 Proportional hazards","heading":"2.5.1 Cure model parameters","text":"basic Poisson mixture cure model consider takes form\\[S(t)= \\exp(-\\theta (1 - \\exp(-\\lambda t)).\\]note :\\(1-\\exp(-\\lambda t)\\) cumulative distribution function exponential distribution can replaced arbitrary continuous cumulative distribution function.\\(t\\) approaches \\(\\infty\\), \\(S(t)\\) approaches \\(\\exp(-\\theta)\\) referred cure rate.Generally, can useful historical data proposed group suggests plateau survival time. parameter \\(\\lambda\\) can used determine quickly events occur, basically setting bend survival curve. instance, assume historical data suggests plateau 0.7 event free survival indication indication adjuvant neoadjuvant treatment cancer.\nassume 18 months event free survival already 0.75.\nUsing Poisson mixture cure model , can back-calculate parameters :\\[\\theta = -\\log(0.7)\\]\n\\[\\lambda = -\\log(1 + \\log(0.75) / \\theta) / 18.\\]proportional hazards model hazard ratio \\(HR\\) can assume control survival \\(S_C(t)=S(t)\\) experimental survival \\(S_E(t)=S_C(t)^{HR}\\).\nAssuming \\(HR=0.72\\) plot assumed survival 48 months treatment group.Next translate inputs parameters Poison mixture model control group.Now plot assumed survival curves model parameters targeted time period specified .\nuse fairly refined grid ensure plot smooth.perform asymptotic calculations approximate survival rates matrix piecewise constant hazard rates match survival rates just computed discrete points .\nSince cumulative hazard \\(H(t)= - \\log(S(t))\\) \\(H(t)=\\int_0^t h(s)ds\\) \\(h()\\) hazard rate, approximate \n\\[\\tilde h(t)=(H(t_m)-H(t_{m-1}))/(t_m - t_{m-1}), m=1,\\ldots,M.\\]","code":"\n# Assumed long-term survival rate\ncureRate <- 0.7\n# Survival rate at specific time\nsurvRate <- 0.75\nsurvRateTime <- 18\n# Maximum time for plot and study duration\nmaxTime <- 60\n# Assumed hazard ratio to power trial\nhr <- 0.72\n# Compute theta for Poisson mixture cure rate model\ntheta <- -log(cureRate)\n# Compute rate parameter lambda for Poisson mixture cure rate model\nlambda <- -log(1 + log(survRate) / theta) / survRateTime\nMonth <- 0:maxTime\n# Control survival\nS <- exp(-theta * (1 - exp(-lambda * Month)))\n# Experimental survival\nS_E <- S^hr\n# Put in a data frame and plot\ncure_model <- rbind(\n  data.frame(Treatment = \"Control\", Month = Month, Survival = S),\n  data.frame(Treatment = \"Experimental\", Month = Month, Survival = S_E)\n)\nggplot(cure_model, aes(x = Month, y = Survival, col = Treatment)) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, maxTime, 6)) +\n  scale_y_continuous(breaks = seq(.6, 1, .1), limits = c(.6, 1))\ncure_model <- cure_model %>%\n  mutate(\n    H = -log(Survival),\n    duration = Month - lag(Month, default = 0),\n    h = (H - lag(H, default = 0) / duration)\n  )"},{"path":"proportional-hazards.html","id":"expected-event-accumulation-over-time","chapter":"2 Proportional hazards","heading":"2.5.2 Expected event accumulation over time","text":"Event accumulation time can sensitive many trial design assumptions.\nGenerally, trying mimic slowing event accumulation time.\nassume following relative enrollment rates end enrollment duration.plot event accumulation scenario calendar time post study initiation.\noverlay percent final events 1, 2, 3, 4, 5 6 years start study.Next can compute expected accrual events time relative complete event accrual end study.\nactual number events sample size increase proportionately long relative enrollment rates durations remain unchanged.note use calendar time spending:","code":"\n# Relative enrollment rates\nenrollRates <- 1:4\n# Total duration for each enrollment rate\n# Here they total 18 months planned to complete enrollment\nenrollDurations <- c(2, 2, 2, 12)\n# Calendar times\nti <- seq(0, maxTime, 1) # maxTime is study duration from above\n# Placeholder for expected event counts\nn <- ti\n\n# Begin with putting parameters into a design with no interim analysis\n# This will enable calculation of expected event accumulation\nxx <- nSurv(\n  alpha = 0.025, # 1-sided Type I error\n  beta = NULL, # Here we will NOT compute sample size\n  # Control group exponential failure rate to get median OS of 13 months\n  lambdaC = cure_model$h[-1],\n  # Time period durations for cure model; length 1 less than lambdaC\n  S = cure_model$duration[-c(1, nrow(cure_model))],\n  # Alternate hypothesis hazard ratio\n  hr = hr,\n  # Dropout rate per month (exponential rate)\n  eta = 0.001,\n  # Enrollment rates during ramp-up period\n  gamma = enrollRates,\n  # Relative enrollment rate time period durations\n  R = enrollDurations,\n  # Calendar time of final analysis\n  T = maxTime,\n  # Minimum follow-up time after enrollment complete\n  minfup = maxTime - sum(enrollDurations)\n)\nfor (i in seq_along(ti[-1])) {\n  n[i + 1] <- nEventsIA(tIA = ti[i + 1], x = xx)\n}\n# Now do a line plot of % of final events by month\nev <- tibble(Month = ti, ev = n / max(n) * 100)\np <- ggplot(ev, aes(x = Month, y = ev)) +\n  geom_line() +\n  ylab(\"% of Final Events\") +\n  xlab(\"Study Time\") +\n  scale_x_continuous(breaks = seq(0, maxTime, 12)) +\n  scale_y_continuous(breaks = seq(0, 100, 20))\n# Add text overlay at targeted analysis times\nsubti <- c(12, 18, 24, 36, 48, 60)\nsubpct <- n[subti + 1] / n[maxTime + 1] * 100\ntxt <- tibble(Month = subti, ev = subpct, txt = paste(as.character(round(subpct, 1)), \"%\", sep = \"\"))\np + geom_label(data = txt, aes(x = Month, y = ev, label = txt))\n# Spending fraction at interim analyses\nusTime <- subti / maxTime\n# Information fraction is required for gsSurv input\n# This was computed above\ntiming <- n[subti + 1] / n[maxTime + 1]"},{"path":"proportional-hazards.html","id":"the-design","chapter":"2 Proportional hazards","heading":"2.5.3 The design","text":"Now design trial efficacy futility bounds determined calendar spending.see table 7 events expected final year design assumptions.\nHowever, quite possible event rate assumptions incorrect duration follow-wanted reasonably evaluate possibility plateau survival.\nnote 18 months enrollment duration expected, give complete follow-42 months planned end trial.","code":"\ncure_model_design <-\n  gsSurv(\n    k = length(timing), # Specify number of analyses\n    alpha = 0.025, # 1-sided Type I error\n    beta = .1, # 1 - .1 = .9 or 90% power\n    # Control group exponential failure rate to get median OS of 13 months\n    lambdaC = cure_model$h[-1],\n    # Time period durations for cure model; length 1 less than lambdaC\n    S = cure_model$duration[-c(1, nrow(cure_model))],\n    # Alternate hypothesis hazard ratio\n    hr = hr,\n    # Dropout rate per month (exponential rate)\n    eta = 0.001,\n    # Enrollment rates during ramp-up period\n    gamma = enrollRates,\n    # Relative enrollment rate time period durations\n    R = enrollDurations,\n    # Calendar time of final analysis\n    T = maxTime,\n    # Minimum follow-up time after enrollment complete\n    minfup = maxTime - sum(enrollDurations),\n    timing = timing, # Information fraction from above\n    usTime = usTime, # Calendar time fraction of analyses from above\n    lsTime = usTime,\n    # Efficacy spending\n    # Lan-DeMets spending function to approximate O'Brien-Fleming design\n    sfu = sfLDOF,\n    # Futility spending\n    sfl = sfHSD, # Hwang-Shih-DeCani (HSD) spending\n    sflpar = -8, # Spending function parameter can be adjusted to get appropriate bound\n    # Specify non-binding futility bounds\n    test.type = 4\n  )\ncure_model_design %>%\n  gsBoundSummary() %>%\n  gt() %>%\n  tab_header(title = \"Calendar-based spending\", subtitle = \"Design with cure model\")"},{"path":"proportional-hazards.html","id":"updating-calendar-based-spending-bounds-at-analysis","chapter":"2 Proportional hazards","heading":"2.5.4 Updating calendar-based spending bounds at analysis","text":"provide example assumptions incorrectly predicted event accrual show potential advantages disadvantages calendar-based timing.\ndeviations can due enrollment different planned, patient event rates different planned, treatment effect different planned.\ndesire design adapts bounds well variety scenarios, allows carrying trial targeted duration, provides reasonable power.\nexample assume events accumulate rapidly design approximation.\nAnalysis times also assumed deviate plan can due operational issues.update using actual event counts calendar times analysis, otherwise copying parameters planned design cure_model_design.","code":"\nactual_events <- c(110, 230, 333, 444, 475, 500)\nactual_analysis_times <- c(11.5, 17, 22.5, 37, 49, 62)\nku <- length(actual_events)\nusTime <- actual_analysis_times / max(cure_model_design$T)\nlsTime <- usTime\ncure_model_update <- gsDesign(\n  k = ku,\n  test.type = cure_model_design$test.type,\n  alpha = cure_model_design$alpha,\n  beta = cure_model_design$beta,\n  astar = cure_model_design$astar,\n  sfu = cure_model_design$upper$sf,\n  sfupar = cure_model_design$upper$param,\n  sfl = cure_model_design$lower$sf,\n  sflpar = cure_model_design$lower$param,\n  n.I = actual_events,\n  maxn.IPlan = cure_model_design$n.I[cure_model_design$k],\n  delta = cure_model_design$delta,\n  delta1 = cure_model_design$delta1,\n  delta0 = cure_model_design$delta0,\n  usTime = usTime,\n  lsTime = lsTime\n)\ngsBoundSummary(\n  cure_model_update,\n  deltaname = \"HR\",\n  logdelta = TRUE,\n  Nname = \"Events\",\n  digits = 5,\n  ddigits = 2,\n  tdigits = 1,\n  exclude = c(\n    \"B-value\", \"CP\", \"CP H1\", \"PP\",\n    paste0(\"P(Cross) if HR=\", round(c(cure_model_design$hr0, cure_model_design$hr), digits = 2))\n  )\n) %>%\n  gt() %>%\n  tab_header(\n    title = \"Calendar-based spending design\",\n    subtitle = \"Update at time of analysis\"\n  )"},{"path":"ahr.html","id":"ahr","chapter":"3 Average hazard ratio","heading":"3 Average hazard ratio","text":"consider designs non-proportional hazards chapter.\nimportance regulatory applications, focus logrank tests.\nOthers tests considered Chapter 5.","code":""},{"path":"ahr.html","id":"piecewise-model","chapter":"3 Average hazard ratio","heading":"3.1 Piecewise model","text":"model time-varying hazard ratio, consider model failure rates experimental control groups piecewise constant.\nSince model can use arbitrarily small piecewise intervals, really restriction.\nconcept, extension piecewise proportional hazards model Lachin Foulkes (1986a).\nSuppose piecewise constant changes change points \\(0 = t_0 < t_1 < \\cdots < t_M \\le \\infty\\), individual interval \\((t_{m-1}, t_m]\\) \\(m = 1, \\ldots, M\\), hazard ratio constant \\(HR_m\\) (experimental:control) , .e.,\n\\[\n  \\text{hazard ratio}\n  =\n  \\left\\{\n  \\begin{array}{ll}\n    HR_1  & \\text{} t \\(0, t_1] \\\\\n    HR_2  & \\text{} t \\(t_1, t_2] \\\\\n    \\vdots \\\\\n    HR_M  & \\text{} t \\(t_{M-1}, t_M] \\\\\n  \\end{array}\n  \\right..\n\\]\n\\(m = 1, \\ldots, M\\),\n\\[\n  HR_m = \\lambda_{1,m} / \\lambda_{0,m},\n\\]\nsubscript \\(\\) indexes treatment group \\(= 0\\) control arm \\(=1\\) treatment arm.\nLachin Foulkes (1986a), also assume exponential dropout rate \\(\\eta_{,m}\\) treatment arm \\(=0,1, m=1,\\ldots,M\\).\nnecessary, software implementation gsDesign2 gsdmvn packages simplified \\(\\eta_{0,m}=\\eta_{1,m}=\\eta_m, m=1,\\ldots,M\\).denote\n\\[\\begin{equation}\n   \\tag{3.1}\n   \\lambda_{,m} = e^{\\gamma_{, m}}.\n\\end{equation}\\]using delta method, get asymptotic distribution \\(\\widehat\\lambda_{,m}\\) \n\\[\\begin{equation}\n  \\tag{3.2}\n  \\log(\\widehat\\lambda_{,m})\n  \\overset{\\cdot}{\\sim}\n  \\text{Normal}\n  \\left(\n    \\log(\\lambda_{,m}), \\; 1/d_{,m}\n  \\right),\n  \\;\\;\n  \\forall \\\\{0, 1\\}\n\\end{equation}\\]estimation \\(\\{\\lambda_{,m}\\}_{=0,1 \\text{ } m = 1, \\ldots, M}\\), complicated get estimation asymptotic distribution \\(HR_m\\), defined \\(HR_m = \\lambda_{1,m}/\\lambda_{0,m}\\).\nchapter, interested logarithm \\(HR_m\\) denote \\(\\beta_m\\).\nRecall \n\\[\\begin{equation}\n  \\tag{3.3}\n  \\beta_m\n  \\triangleq\n  \\log(HR_m)\n  =\n  \\log\\left( \\frac{\\lambda_{1,m}}{\\lambda_{0,m}} \\right)\n  =\n  \\log(\\lambda_{1,m}) - \\log(\\lambda_{0,m}).\n\\end{equation}\\]\n\\(\\lambda_{1,m}\\) \\(\\lambda_{0,m}\\) , equation (??), know can estimated \n\\[\n  \\widehat\\lambda_{,m} = \\frac{d_{,m}}{T_{, m}} \\;\\; \\\\{0, 1\\}\n\\]\n\\(d_{0,m}, d_{1,m}\\) number events \\((t_{m-1}, t_m]\\) group \\(0,1\\), respectively.plugging asymptotic distribution \\(\\{\\lambda_{0, m}, \\lambda_{1,m}\\}\\) equation (3.2) (3.3), can derive asymptotic distribution \\(\\beta_m\\):\n\\[\\begin{equation}\n  \\tag{3.4}\n  \\widehat\\beta_m\n  \\overset{\\cdot}{\\sim}\n  \\text{Normal}\n  \\left(\n    \\beta_m,\n    \\frac{1}{D_{0m}} + \\frac{1}{D_{1m}}\n  \\right)\n  \\;\\; \\forall m = 1,\\ldots, M .\n\\end{equation}\\]","code":""},{"path":"ahr.html","id":"secAhr","chapter":"3 Average hazard ratio","heading":"3.2 Average hazard ratio","text":"section, define average hazard ratio (AHR) use derive asymptotic normal distribution logrank test.\nactually weighted geometric mean hazard ratio piecewise intervals defined .\nDefining \\(\\beta_m=\\log(\\lambda_{1,m}/\\lambda_{0,m})= \\log(\\lambda_{1,m})-\\log(\\lambda_{0,m}), m=1,\\ldots,M\\), define logarithm AHR weighted sum individual log hazard ratios:\n\\[\n  \\beta\n  =\n  \\sum_{m=1}^M w_m \\log(HR_m)=\\sum_{m=1}^M w_m \\beta_m=\\sum_{m=1}^Mw_m(\\log(\\lambda_{1,m})-\\log(\\lambda_{0,m})).\n\\]\n\\(w_m\\) propose inverse variance weighting based expected number events expected treatment group \\((t_{m-1},t_m],\\) \\(m=1,\\ldots,M\\).\ndenote \\(d_{,m}, T_{,m},\\) \\(=0,1,\\) \\(m=1,\\ldots,M\\) observed events total time risk treatment group \\(\\) period \\((t_{m-1},t_m]\\).\nThus, \\(m=1,\\ldots,M\\) \\[w_m=\\frac{(1/E(d_{0,m})+1/E(d_{1,m}))^{-1}}{\\sum_{j=1}^M (1/E(d_{0,j})+1/E(d_{1,j}))^{-1}}.\\]\ncorresponding estimate \\(\\log(\\lambda_{,m}), =0,1, m=1,\\ldots,M\\) \n\\[\\hat{\\gamma}_{,m}=\\log\\hat\\lambda_{,m} = \\log(d_{,m}/T_{,m})\\]\nasymptotically normal variance\n\\[\\text{Var}(\\hat\\gamma_{,m})=1/E(d_{,m})\\]\nvariance estimate\n\\[\\widehat{\\text{Var}}(\\hat\\gamma_{,m})=1/d_{,m}.\\]\npropose estimate \\(\\beta\\) using estimated weights piecewise model.\n\\[\\begin{equation}\n  \\tag{3.5}\n  \\tilde\\beta\n  =\n  \\sum_{m=1}^M\n  \\hat{w}_m\n  \\left(\\hat\\gamma_{1,m} - \\hat\\gamma_{0,m}\\right)\n\\end{equation}\\]selection weight \\(\\hat{w}_m,\\) \\(m = 1, \\ldots, M\\), use inverse variance weighting\n\\[\n  \\hat{w}_m\n  =\n  \\left.\n  \\left(\n    \\frac{1}{1/d_{0,m}+1/d_{1,m}}\n  \\right)^{-1}\n  \\right/\n  \\sum_{=1}^M\n  \\left(\n    \\frac{1}{1/d_{0,}+1/d_{1,}}\n  \\right)^{-1}.\n\\]\nplugging weights equation (3.5), \\(\\beta\\) can estimated \n\\[\n  \\tilde\\beta\n  =\n  \\frac{\n    \\sum_{m=1}^M\n    \\left( \\frac{1}{d_{0,m}}+\\frac{1}{d_{1,m}} \\right)^{-1}\n    \\left( \\log(d_{1,m} / T_{1,m}) - \\log(d_{0,m}/T_{0,m}) \\right)\n  }{\n    \\sum_{m=1}^M\n    \\left( \\frac{1}{d_{0,m}}+\\frac{1}{d_{1,m}} \\right)^{-1}\n  }.\n\\]\ncorresponding variance estimate :\n\\[\n  \\widehat{\\hbox{Var}}(\\tilde\\beta)\n  =\n  \\left(\\sum_{m=1}^M(1/d_{0,m} + 1/d_{1,m})^{-1}\\right)^{-1}.\n\\]plugging asymptotic distribution \\(\\hat\\beta_m\\) equation (3.4), one gets asymptotic distribution \\(\\tilde\\beta\\) \\[\n  \\tilde\\beta\n  \\overset{\\cdot}{\\sim}\n  \\hbox{Normal}(\\beta, \\; \\mathcal{}^{-1}),\n\\]\n\\[\\mathcal{} = \\sum_{m = 1}^M \\left( \\frac{1}{E(d_{0,m})} + \\frac{1}{E(d_{1,m})} \\right)^{-1}.\\]shown asymptotic distribution logrank test piecewise model follows results Schoenfeld (1981).\ndetails computing \\(E(d_{,m}),\\), \\(=0,1\\), \\(m=1,\\ldots,M\\) piecewise model demonstrated vignette gsDesign2 package.\ncomputations form basis asymptotic approximations power sample size implemented gsdmvn package using functions gsdmvn::gs_power_ahr() gsdmvn::gs_design_ahr().","code":""},{"path":"pkgs.html","id":"pkgs","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4 Introduction to gsdmvn, gsDesign2, and simtrial","text":"simtrial, gsDesign2 gsdmvn development hosted GitHub/Merck.\nGithub repo link source codesimtrial: https://github.com/Merck/simtrialgsDesign2: https://github.com/Merck/gsDesign2gsdmvn: https://github.com/Merck/gsdmvn","code":""},{"path":"pkgs.html","id":"simtrial","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.1 simtrial","text":"R package simtrial targets time--event trial simulation piecewise model.\nuses logrank weighted logrank analysis.\ncan simulate fixed group sequential design, also potential simulate adaptive design.\nvalidation near completion (thanks AP colleagues Amin Shirazi).simtrial, several functions generate simulated datasets:simfix(): Simulation fixed sample size design time--event endpointsimfix2simPWSurv(): Conversion enrollment failure rates simfix() simPWSurv() formatsimPWSurv(): Simulate stratified time--event outcome randomized trialThere also functions cut data analysis:cutData(): Cut dataset analysis specified datecutDataAtCount(): Cut dataset analysis specified event countgetCutDateForCount(): Get date event count reachedMost importantly, functions analysis:tenFH(): Fleming-Harrington weighted logrank teststenFHcorr(): Fleming-Harrington weighted logrank tests plus correlationstensurv(): Process survival data counting process formatpMaxCombo(): MaxCombo p-valuepwexpfit(): Piecewise exponential survival estimationwMB(): Magirr Burman modestly weighted logrank testsIn simtrial, reverse engineered datasets:Ex1delayedEffect: Time--event data example 1 non-proportional hazards working groupEx2delayedEffect: Time--event data example 2 non-proportional hazards working groupEx3curewithph: Time--event data example 3 non-proportional hazards working groupEx4belly: Time--event data example 4 non-proportional hazards working groupEx5widening: Time--event data example 5 non-proportional hazards working groupEx6crossing: Time--event data example 6 non-proportional hazards working groupMBdelayed: Simulated survival dataset delayed treatment effect","code":""},{"path":"pkgs.html","id":"gsdesign2","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.2 gsDesign2","text":"R package gsDesign2 main functions listed follows.AHR(): Average hazard ratio non-proportional hazards (test version)eAccrual(): Piecewise constant expected accrualeEvents_df(): Expected events observed piecewise exponential modelppwe(): Estimate piecewise exponential cumulative distribution functions2pwe(): Approximate survival distribution piecewise exponential distributiontEvents(): Predict time targeted event count achieved","code":""},{"path":"pkgs.html","id":"gsdmvn","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.3 gsdmvn","text":"R package gsdmvn extends Jennison Turnbull (2000) computational model non-constant treatment effects.\nbound supports functions includegs_b(): direct input bounds;gs_spending_bound(): spending function bounds.Besides, covers three models analysis.Average hazard ratio model (see detailed description Section 3.2)\ngs_power_ahr(): Power computation\ngs_design_ahr(): Design computations\ngs_power_ahr(): Power computationgs_design_ahr(): Design computationsWeighted logrank model (see detailed description Section 7)\ngs_power_wlr(): Power computation\ngs_design_wlr(): Design computations\ngs_power_wlr(): Power computationgs_design_wlr(): Design computationsMaxCombo model (see detailed description Section 9)\ngs_power_combo(): Power computation\ngs_design_combo(): Design computations\ngs_power_combo(): Power computationgs_design_combo(): Design computations","code":""},{"path":"pkgs.html","id":"simulation","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.4 Simulation","text":"section, conduct three simulations.first simulation compare simulated power asymptotic power.\nSpecifically speaking, compare results gsdmvn::gs_power_ahr() simtrial::simfix()\ndetails simulation can found Section 4.4.1The second simulation compare simulated power asymptotic power.\nSpecifically speaking, compare results gsDesign2::AHR() simtrial::simPWSurv().\ndetails simulation can found Section 4.4.2.third simulation compare estimation \\(\\beta\\) MLE Section ?? weighted summation Section ??.\ndetails simulation can found Section 4.4.3.","code":""},{"path":"pkgs.html","id":"secAhrSimSimfix","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.4.1 Simulation 1: Compare the IA power by gs_power_ahr() and simulation sim_fix()","text":"section compares simulated power asymptotic power.\nsimulated power calculated simtrial::simfix(), asymptotic power calculated gsdmvn::gs_power_ahr().\nconduct comparison, first save output simtrial::simfix() running following code chunk.\n, simply load simulation results compare asymptotic power.","code":"\nlibrary(gsDesign)\nlibrary(simtrial)\nlibrary(dplyr)\nlibrary(gsdmvn)\n\n## Set the enrollment rates\nmy_enrollRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(2, 2, 2, 6),\n  rate = c(6, 12, 18, 24)\n)\n## Set the failure rates\nmy_failRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = 1,\n  failRate = log(2) / 9,\n  hr = 0.65,\n  dropoutRate = 0.001\n)\n## Set the number of simulations\nmy_nsim <- 1e+5\n\n## Create a group sequential design for survival outcome\nmy_gsSurv <- gsSurv(\n  k = 2,\n  test.type = 1,\n  alpha = 0.025,\n  beta = 0.2,\n  astar = 0,\n  timing = 0.7,\n  sfu = sfLDOF,\n  sfupar = c(0),\n  sfl = sfLDOF,\n  sflpar = c(0),\n  lambdaC = log(2) / 9,\n  hr = 0.65,\n  hr0 = 1,\n  eta = 0.001,\n  gamma = c(6, 12, 18, 24),\n  R = c(2, 2, 2, 6),\n  S = NULL,\n  T = NULL,\n  minfup = NULL,\n  ratio = 1\n)\n\n# Update my_gsSurv with gsDesign() to get integer event counts\nmy_gsDesign <- gsDesign(\n  k = my_gsSurv$k,\n  test.type = 1,\n  alpha = my_gsSurv$alpha,\n  beta = my_gsSurv$beta,\n  sfu = my_gsSurv$upper$sf,\n  sfupar = my_gsSurv$upper$param,\n  n.I = ceiling(my_gsSurv$n.I),\n  maxn.IPlan = ceiling(my_gsSurv$n.I[my_gsSurv$k]),\n  delta = my_gsSurv$delta,\n  delta1 = my_gsSurv$delta1,\n  delta0 = my_gsSurv$delta0\n)\n\nset.seed(123)\nmy_simfix <- simfix( # Number of simulations to perform.\n  nsim = my_nsim,\n  # Total sample size per simulation.\n  sampleSize = ceiling(max(my_gsSurv$eNC) + max(my_gsSurv$eNE)),\n  # A tibble with\n  # (1) strata specified in `Stratum`\n  # (2) probability (incidence) of each stratum in `p`.\n  strata = tibble::tibble(Stratum = \"All\", p = 1),\n  # Targeted event count for analysis.\n  # Here we only target on the 1st IA only.\n  targetEvents = my_gsDesign$n.I[1],\n  # Vector of treatments to be included in each block\n  block = c(rep(\"Control\", 2), rep(\"Experimental\", 2)),\n  # Piecewise constant enrollment rates by time period.\n  enrollRates = my_enrollRates,\n  # Piecewise constant control group failure rates,\n  # hazard ratio for experimental vs control,\n  # and dropout rates by stratum and time period.\n  failRates = my_failRates,\n  # `timingType` has up to 5 elements indicating different options for data cutoff.\n  # `timingType = 1`: uses the planned study duration\n  # `timingType = 2`: the time the targeted event count is achieved\n  # `timingType = 3`: the planned minimum follow-up after enrollment is complete\n  # `timingType = 4`: the maximum of planned study duration and targeted event count cuts (1 and 2)\n  # `timingType = 5`: the maximum of targeted event count and minimum follow-up cuts (2 and 3)\n  timingType = 2\n)\n# Save the simulation data\nsave(my_gsDesign, file = \"./data/simulation_gs_power_ahr_my_gsDesign.Rdata\")\nsave(my_simfix, file = \"./data/simulation_gs_power_ahr_my_simfix.Rdata\")\nlibrary(gsDesign)\nlibrary(simtrial)\nlibrary(dplyr)\nlibrary(gsdmvn)\n\nload(\"./data/simulation_gs_power_ahr_my_simfix.Rdata\")\nload(\"./data/simulation_gs_power_ahr_my_gsDesign.Rdata\")\n\n## Set the enrollment rates\nmy_enrollRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(2, 2, 2, 6),\n  rate = c(6, 12, 18, 24)\n)\n## Set the failure rates\nmy_failRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = 1,\n  failRate = log(2) / 9,\n  hr = 0.65,\n  dropoutRate = 0.001\n)\nmy_nsim <- 1e+5\n\n# Calculate the simulated power at the 1st IA\nmy_sim_IA_power <- as.numeric(my_simfix %>% summarize(mean(Z <= -my_gsDesign$upper$bound[1])))\n\n# Calculate the power by gs_ahr_power() at the 1st IA\nout <- gs_power_ahr(\n  enrollRates = my_enrollRates,\n  failRates = my_failRates,\n  ratio = 1,\n  events = my_gsDesign$n.I, # set number of events the same as my_gsDesign above from gsDesign()\n  analysisTimes = NULL,\n  binding = FALSE,\n  upper = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL, theta = 0),\n  lower = gs_spending_bound,\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2, param = NULL, timing = NULL, theta = 0),\n  test_upper = TRUE,\n  test_lower = FALSE\n)\nmy_ahr_IA_power <- out$Probability[1]\n\ncat(\"The power at the 1st IA by gs_power_ahr() is:\", my_ahr_IA_power, \"\\n\")#> The power at the 1st IA by gs_power_ahr() is: 0.4605649\ncat(\"The power at the 1st IA by\", my_nsim, \"simulation is:\", my_sim_IA_power, \"\\n\")#> The power at the 1st IA by 1e+05 simulation is: 0.46655"},{"path":"pkgs.html","id":"secAhrSimSimPWSurv","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.4.2 Simulation 2: Compare AHR by gsDesign2::AHR() and simtrial::simPWSurv()","text":"section compares simulated AHR asymptotic AHR\nsimulated power calculated simtrial::simPWSurv(), asymptotic power calculated gsDesign2::AHR().\nconduct comparison, first save output simtrial::simPWSurv() running following code chunk.\n, simply load simulation results compare asymptotic AHR","code":"\nlibrary(survival)\nlibrary(dplyr)\nlibrary(simtrial)\n\n# Set the sample size\nmy_N <- 500\n# Set the analysis time, i.e., there are 4 looks\nmy_analysisTimes <- c(12, 20, 28, 36)\n# Set the enrollment rates\n# This format of enrollment rates is design for simtrial::simfix()\n# If it is later used to simtrial::simPWSurv(),\n# function simtrial::simfix2simPWSurv() can used for transformation\nmy_enrollRates <- tibble(\n  Stratum = \"All\",\n  duration = 12,\n  rate = my_N / 12\n)\n# Set the failure rates\n# This format of failure rates is design for simtrial::simfix()\n# If it is later used to simtrial::simPWSurv(),\n# function simtrial::simfix2simPWSurv() can used for transformation\nmy_failRates <- tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15,\n  hr = c(1, 0.6),\n  dropoutRate = 0.001\n)\n\n# Set number of simulations\nmy_nsim <- 10\n# Set up matrix for simulation results\nresults <- matrix(0, nrow = my_nsim * 4, ncol = 6)\ncolnames(results) <- c(\"Sim\", \"Analysis\", \"Events\", \"beta\", \"var\", \"logrank\")\n\n# Set the index for results row\nii <- 1\nset.seed(123)\nfor (sim in 1:my_nsim) {\n  # Simulate a trial\n  ds <- simPWSurv( # Generate my_N observations\n    n = my_N,\n    # Use the same enrollRates as that in AHR\n    enrollRates = my_enrollRates,\n    # Conversion of failRates from simfix() to simPWSurv() format\n    failRates = simfix2simPWSurv(my_failRates)$failRates,\n    # Conversion of dropoutRates from simfix() to simPWSurv() format\n    dropoutRates = simfix2simPWSurv(my_failRates)$dropoutRates\n  )\n  # For each generated my_N observations\n  # Go through pre-defined 4 looks\n  for (j in seq_along(my_analysisTimes)) {\n    # Cut data at specified analysis times\n    # Use cutDataAtCount to cut at event count\n    # des is a dataset ready for survival analysis\n    dsc <- ds %>% cutData(my_analysisTimes[j])\n\n    # 1st column of results records the index of the simulation\n    results[ii, 1] <- sim\n    # 2nd column of results records the index of the look\n    results[ii, 2] <- j\n    # 3rd column of results records the number of events\n    results[ii, 3] <- sum(dsc$event)\n\n    # Apply Cox model\n    cox <- coxph(Surv(tte, event) ~ Treatment, data = dsc)\n    # 4th column of results records the number log HR\n    results[ii, 4] <- as.numeric(cox$coefficients)\n    # 5th column of results records the variance\n    results[ii, 5] <- as.numeric(cox$var)\n\n    # Logrank test\n    Z <- dsc %>%\n      tensurv(txval = \"Experimental\") %>%\n      tenFH(rg = tibble::tibble(rho = 0, gamma = 0))\n    # 5th column of results records the logrank\n    results[ii, 6] <- as.numeric(Z$Z)\n\n    # Increate the row index\n    ii <- ii + 1\n  }\n}\nsave(results, file = \"./data/simulation_AHR_simPRSurv.Rdata\")\n# Calculate the simulated AHR\nload(\"./data/simulation_AHR_simPRSurv.Rdata\")\nAHR_simulated <- tibble::as_tibble(results) %>%\n  group_by(Analysis) %>%\n  summarize(\n    AHR = exp(mean(beta)),\n    Events = mean(Events),\n    info = 1 / mean(var(beta)),\n    info0 = Events / 4\n  ) %>%\n  select(AHR, Events, info, info0)\ncolnames(AHR_simulated) <- c(\"AHR_sim\", \"Events_sim\", \"info_sim\", \"info0_sim\")\n\n# Calculate the AHR asymptotically\n# gsDesign2::AHR() uses asymptotic distribution\n# We will compare its outputs with the simulated outputs\n# The simulated outputs is calculated by simtrial::simPWSurv()\n\n# Set the sample size the same as simPWSurv()\nmy_N <- 500\n# Set the analysis time the same as simPWSurv()\nmy_analysisTimes <- c(12, 20, 28, 36)\n# Set the enrollment rates the same as simPWSurv()\nmy_enrollRates <- tibble(\n  Stratum = \"All\",\n  duration = 12,\n  rate = my_N / 12\n)\n# Set the failure rates the same as simPWSurv()\nmy_failRates <- tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15,\n  hr = c(1, 0.6),\n  dropoutRate = 0.001\n)\n\nAHR_asymptotics <- gsDesign2::AHR(\n  enrollRates = my_enrollRates,\n  failRates = my_failRates,\n  totalDuration = my_analysisTimes,\n  ratio = 1\n)\ncolnames(AHR_asymptotics) <- c(\"Time\", \"AHR_asy\", \"Events_asy\", \"info_asy\", \"info0_asy\")\n\n# Compare the results\ncbind(AHR_asymptotics, AHR_simulated) %>%\n  gt::gt() %>%\n  gt::fmt_number(columns = c(2, 4, 5, 6, 8, 9), decimals = 4) %>%\n  gt::fmt_number(columns = c(3, 7), decimals = 2)"},{"path":"pkgs.html","id":"secAhrSimMleWs","chapter":"4 Introduction to gsdmvn, gsDesign2, and simtrial","heading":"4.4.3 Simulation 3: Compare \\(\\beta\\) by MLE and weighted summation","text":"section compares AHR estimated MLE (see theoretical details Section ??) weighted summation (see theoretical details Section ??).","code":"\nlibrary(dplyr)\nlibrary(gt)\nmy_nsim <- 10\nmy_nNewtonRaphson <- 10\ncompare_results <- matrix(0, nrow = my_nsim, ncol = 2)\nset.seed(123)\n\nfor (sim in 1:my_nsim) {\n  # Generate the number of change points, i.e., total number of timeline piece - 1\n  sim_M <- sample(3:10, size = 1)\n  sim_d0_start <- sample(4:8, size = 1)\n  sim_d1_start <- sim_d0_start - 3\n  sim_T0_start <- sample(10:20, size = 1)\n  sim_T1_start <- sim_T0_start + 1\n  # Generate simulated data\n  obs_data <- data.frame( # m = 1:5,\n    m = sim_M,\n    # d0 = 4:8,\n    d0 = seq(from = sim_d0_start, length.out = sim_M),\n    # d1 = 1:5,\n    d1 = seq(from = sim_d1_start, length.out = sim_M),\n    # T0 = 10:14,\n    T0 = seq(from = sim_T0_start, length.out = sim_M),\n    # T1 = 11:15\n    T1 = seq(from = sim_T1_start, length.out = sim_M)\n  ) %>%\n    mutate(\n      lambda0 = d0 / T0,\n      lambda1 = d1 / T1,\n      HR = lambda1 / lambda0,\n      # beta_m\n      gamma = log(HR),\n      # Var(beta_m)\n      vargamma = 1 / d0 + 1 / d1\n    )\n\n  # Estimate beta by weighted summation\n  # beta = variable `logAHR`\n  estimate_beta_WS <- obs_data %>%\n    summarise(\n      wsum = sum(1 / vargamma),\n      # estimation of beta_WS\n      beta_WS = sum(gamma / vargamma) / wsum,\n      # variance of the estimation of beta_WS\n      var = sum(vargamma^(-1))^(-1),\n      # standard derivation of the estimation of beta_WS\n      se = sqrt(var),\n      # AHR: average of lambda_{1,m}/lambda_{0,m}\n      AHR = exp(beta_WS)\n    )\n  compare_results[sim, 1] <- estimate_beta_WS$beta_WS\n\n  # Estimate beta by MLE\n  beta_MLE <- estimate_beta_WS$beta_WS\n  # beta_MLE_seq <- beta_MLE    # ensure convergence\n\n  for (i in 1:my_nNewtonRaphson) {\n    ## Calculate the first order derivative at the value of beta_k\n    temp_beta_d1 <- obs_data %>%\n      summarise(beta_d1 = -sum((d0 + d1) * T1 * exp(beta_MLE) / (T0 + T1 * exp(beta_MLE)))\n      +\n        sum(d1))\n    beta_d1_curr <- temp_beta_d1$beta_d1\n\n    ## Calculate the second order derivative at the value of beta_k\n    temp_beta_d2 <- obs_data %>%\n      summarise(beta_d2 = sum((T1 * exp(beta_MLE))^2 * (d0 + d1) / (T0 + T1 * exp(beta_MLE))^2)\n      -\n        sum(((d0 + d1) * T1 * exp(beta_MLE)) / (T0 + T1 * exp(beta_MLE))))\n    beta_d2_curr <- temp_beta_d2$beta_d2\n\n    ## Update beta by Newton-Raphson method, i.e.,\n    ## beta_k+1 = beta_k - l'(beta_k)/l''(beta_k)\n    beta_MLE <- beta_MLE - beta_d1_curr / beta_d2_curr\n    # beta_MLE_seq <- c(beta_MLE_seq, beta_MLE)\n  }\n\n  compare_results[sim, 2] <- beta_MLE\n}\n\ncolnames(compare_results) <- c(\"Weighted Summation\", \"MLE\")\nsave(compare_results, file = \"./data/simulation_MLE_vs_WS.Rdata\")\nload(\"./data/simulation_MLE_vs_WS.Rdata\")\nmy_nsim <- 1e+5\nhead(compare_results)#>      Weighted Summation        MLE\n#> [1,]         -0.4139139 -0.4158463\n#> [2,]         -0.6729929 -0.6772665\n#> [3,]         -0.4203335 -0.4207679\n#> [4,]         -0.5409583 -0.5530679\n#> [5,]         -0.4247562 -0.4252013\n#> [6,]         -0.7252355 -0.7411065\ncat(\n  \"The MSE between MLE and weighted summation is \",\n  sum((compare_results[, 1] - compare_results[, 2])^2) / my_nsim, \"\\n\"\n)#> The MSE between MLE and weighted summation is  4.464504e-05"},{"path":"overview-other-tests.html","id":"overview-other-tests","chapter":"5 Overview","heading":"5 Overview","text":"consider alternative tests group sequential design.Weighted logrank tests Harrington Fleming (1982) Magirr Burman (2019a) tests.Combination tests MaxCombo test Lin et al (2020), Roychoudhury et al (2021).illustrate concepts based experimental R packages.gsdmvn: analytically based group sequential design.part, coverReview fixed design based weighted logrank test MaxCombo testGroup sequential design weighted logrank testGroup sequential design MaxCombo test","code":""},{"path":"fixed-design.html","id":"fixed-design","chapter":"6 Fixed design","heading":"6 Fixed design","text":"briefly review fixed design based weighted logrank test MaxCombo test.fixed design, npsurvSS\ntool training.fixed design part largely follows concept described Yung Liu (2019).","code":""},{"path":"fixed-design.html","id":"summary-of-assumptions","chapter":"6 Fixed design","heading":"6.1 Summary of assumptions","text":"simplicity, made key assumptions.Balanced design (1:1 randomization ratio).1-sided test.Local alternative: variance null alternative approximately equal.Accrual distribution:\nPiecewise uniform using npsurvSS::create_arm()\nPoisson process piecewise uniform enrollment using gsDesign::nSurv() Lachin Foulkes (1986b) gsdmvn::gs_design_ahr()\nPiecewise uniform using npsurvSS::create_arm()Poisson process piecewise uniform enrollment using gsDesign::nSurv() Lachin Foulkes (1986b) gsdmvn::gs_design_ahr()Survival distribution: piecewise exponentialLoss follow-: exponentialNo stratificationNo cure fractionSome assumptions generalized literature R package gsdmvn.\nassumptions used unless clarification made specific section.","code":""},{"path":"fixed-design.html","id":"notation","chapter":"6 Fixed design","heading":"6.2 Notation","text":"also define commonly used notation .\\(\\alpha\\): Type error\\(\\beta\\): Type II error power (1 - \\(\\beta\\))\\(z_\\alpha\\): upper \\(\\alpha\\) percentile standard normal distribution\\(z_\\beta\\): upper \\(\\beta\\) percentile standard normal distributionFor illustration purpose, considered 1-sided test type error \\(\\alpha=0.025\\) \\(1-\\beta=80\\%\\) power.\nR, easy calculate \\(z_\\alpha\\) \\(z_\\beta\\) .","code":"\nz_alpha <- abs(qnorm(0.025))\nz_alpha#> [1] 1.959964\nz_beta <- abs(qnorm(0.2))\nz_beta#> [1] 0.8416212"},{"path":"fixed-design.html","id":"sample-size-calculation","chapter":"6 Fixed design","heading":"6.3 Sample size calculation","text":"\\(\\theta\\): effect size.key step calculate sample size define effect size.\nexample, effect size two-sample t-test \\((\\mu_1 - \\mu_2)/\\sigma\\),\n\\(\\mu_1\\) \\(\\mu_2\\) group mean \\(\\sigma\\) pooled standard deviation.\\(n\\): total sample size.\\(n\\): total sample size.\\(Z\\): test statistics asymptotic normal.\nnull hypothesis: \\(Z \\sim \\mathcal{N}(0, \\sigma_0^2)\\)\nalternative hypothesis: \\(Z \\sim \\mathcal{N}(\\sqrt{n}\\theta, \\sigma_1^2)\\)\n\\(Z\\): test statistics asymptotic normal.null hypothesis: \\(Z \\sim \\mathcal{N}(0, \\sigma_0^2)\\)alternative hypothesis: \\(Z \\sim \\mathcal{N}(\\sqrt{n}\\theta, \\sigma_1^2)\\)assuming local alternative, \\[\\sigma_0^2 \\approx \\sigma_1^2 = \\sigma^2\\]\nsimplified case, sample size can calculated \\[ n = \\frac{4 (z_{\\alpha}+z_{\\beta})^{2}}{\\theta^2} \\]","code":""},{"path":"fixed-design.html","id":"two-sample-t-test","chapter":"6 Fixed design","heading":"6.4 Two-sample t-test","text":"Let’s revisit two sample t-test make connection math formula R code.consider scenarios treatment effect 0.5 pooled standard deviation 2.\nLet’s calculate sample size using formula .assumption used gsDesign::nNormal().stats::power.t.test() uses t-distribution test statistics recommended practice.\nprovides slightly larger sample size study design scenario.","code":"\n# Effect size\ntheta <- 0.5 / 2\n# Sample size formula\n4 * (z_alpha + z_beta)^2 / theta^2#> [1] 502.3283\ngsDesign::nNormal(delta1 = 0.5, sd = 2, alpha = 0.025, beta = 0.2)#> [1] 502.3283\nstats::power.t.test(delta = 0.5, sd = 2, sig.level = 0.05, power = 0.8)$n * 2#> [1] 504.2562"},{"path":"fixed-design.html","id":"logrank-test","chapter":"6 Fixed design","heading":"6.5 Logrank test","text":"Let’s also explore number events required logrank test proportional hazards assumption.may hear sample size calculation “event-driven”.\nnice feature proportional hazards assumption effect size number events depends hazard ratio.\\[\\theta = \\log{\\text{HR}} / 2\\]\\(\\text{HR}\\) hazard ratio.\\(d\\) number events.exercise, readers can derive effect size formula (9.3) \nSection 9.5 NCSU ST520 course notes.effect size, can use formula calculate number events .\\[ d = \\frac{4 (z_{\\alpha}+z_{\\beta})^{2}}{(\\log{\\text{HR}/2})^2} \\]total number events defined,\nsample size (\\(n\\)) can determined based accrual distribution,\nsurvival distribution loss follow-distribution study duration.sample size calculation implemented ingsDesign::nSurv()npsurvSS::size_two_arm()simplicity, skip detail sample size calculation,\ninterested readers can refer Lachin Foulkes (1986b).Let’s make connection math formula R code considering scenario \nhazard ratio 0.6.compare results using npsurvSS.\nkey argument npsurvSS::create_arm() surv_scale defines hazard rates arm.can use npsurvSS::size_two_arm() calculate number events sample size.number events gsDesign::nSurv() slightly smaller\ngsDesign::nSurv() follows Lachin Foulkes (1986b)\nrelax local alternative assumption recommended practice.","code":"\n# Effect size\ntheta <- log(0.6) / 2\n# Number of Events\n(z_alpha + z_beta)^2 / theta^2#> [1] 120.3157\n# Define study design assumption for each arm\narm0 <- npsurvSS::create_arm(\n  size = 1, accr_time = 6, surv_scale = 1,\n  loss_scale = 0.1, follow_time = 12\n)\n\narm1 <- npsurvSS::create_arm(\n  size = 1, accr_time = 6, surv_scale = 0.6,\n  loss_scale = 0.1, follow_time = 12\n)\n# Sample size for logrank test\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(\n    test = \"weighted logrank\", weight = \"1\",\n    mean.approx = \"event driven\"\n  )\n)#>        n0        n1         n        d0        d1         d \n#>  68.12167  68.12167 136.24335  61.92878  58.38693 120.31570\ngsDesign::nSurv(\n  lambdaC = 1,\n  hr = 0.6,\n  eta = 0.1,\n  alpha = 0.025,\n  beta = 0.2,\n  T = 18,\n  minfup = 12\n)#> Fixed design, two-arm trial with time-to-event\n#> outcome (Lachin and Foulkes, 1986).\n#> Solving for:  Accrual rate \n#> Hazard ratio                  H1/H0=0.6/1\n#> Study duration:                   T=18\n#> Accrual duration:                   6\n#> Min. end-of-study follow-up: minfup=12\n#> Expected events (total, H1):        119.7983\n#> Expected sample size (total):       135.6574\n#> Accrual rates:\n#>     Stratum 1\n#> 0-6   22.6096\n#> Control event rates (H1):\n#>       Stratum 1\n#> 0-Inf         1\n#> Censoring rates:\n#>       Stratum 1\n#> 0-Inf       0.1\n#> Power:                 100*(1-beta)=80%\n#> Type I error (1-sided):   100*alpha=2.5%\n#> Equal randomization:          ratio=1"},{"path":"fixed-design.html","id":"non-proportional-hazards","chapter":"6 Fixed design","heading":"6.6 Non-proportional hazards","text":"proportional hazard assumption, commonly used event driven approach sample size calculation.\n(.e., calculate number events derive sample size.)Event (\\(d\\)) Sample size (\\(n\\)) d->nUnder non-proportional hazards, event driven approach applicable.\nneed derive sample size first.Sample size (\\(n\\)) Event (\\(d\\)) n->d\\(\\tau_a\\): accrual time\\(\\tau_f\\): follow-timeThe two figures copied Yung Liu (2019).Let’s consider delayed effect scenario illustrate NPH sample size calculation.Duration enrollment: 12 monthsEnrollment rate: 500/12 per monthFailure rate control group: log(2) / 15\nMedian survival time: 15 months.\nMedian survival time: 15 months.Hazard ratio:\nFirst 4 months: 1\n4 months: 0.6\nFirst 4 months: 1After 4 months: 0.6Dropout Rate: 0.001The figure illustrated survival probability time two treatment groups.","code":"\nenrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500 / 12)\nenrollRates#> # A tibble: 1 × 3\n#>   Stratum duration  rate\n#>   <chr>      <dbl> <dbl>\n#> 1 All           12  41.7\nfailRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15, # Median survival 15 months\n  hr = c(1, .6), # Delay effect after 4 months\n  dropoutRate = 0.001\n)\nfailRates#> # A tibble: 2 × 5\n#>   Stratum duration failRate    hr dropoutRate\n#>   <chr>      <dbl>    <dbl> <dbl>       <dbl>\n#> 1 All            4   0.0462   1         0.001\n#> 2 All          100   0.0462   0.6       0.001"},{"path":"fixed-design.html","id":"weighted-logrank-test","chapter":"6 Fixed design","heading":"6.7 Weighted logrank test","text":"weighted logrank test, first illustrated using Fleming-Harrington weight.\\[FH^{\\rho, \\gamma}(t) = S(t)^\\rho(1 - S(t))^\\gamma\\]demonstrate weight function, covert input enrollRates failRates required npsurvSS.Note: npsurvSS, p1_q0 \\(\\rho=q=0\\) \\(\\gamma=p=1\\)FH(0,1): Place weights later time pointsFH(1,1): Place weights middle time pointsFH(1,0): Place weights earlier time pointsFH(0,0) logrank test","code":"\n# Define study design object in each arm\ngs_arm <- gsdmvn:::gs_create_arm(\n  enrollRates,\n  failRates,\n  ratio = 1, # Randomization ratio\n  total_time = 36 # Total study duration\n)\narm0 <- gs_arm[[\"arm0\"]]\narm1 <- gs_arm[[\"arm1\"]]\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p1_q0\")\n)#>        n0        n1         n        d0        d1         d \n#> 138.39353 138.39353 276.78707 102.15617  81.23792 183.39408\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p1_q1\")\n)#>        n0        n1         n        d0        d1         d \n#> 130.75651 130.75651 261.51302  96.51884  76.75493 173.27377\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p0_q1\")\n)#>       n0       n1        n       d0       d1        d \n#> 237.5982 237.5982 475.1964 175.3848 139.4717 314.8565\n# Sample size for logrank test\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p0_q0\")\n)#>        n0        n1         n        d0        d1         d \n#> 164.97626 164.97626 329.95252 121.77839  96.84215 218.62055"},{"path":"fixed-design.html","id":"effect-size","chapter":"6 Fixed design","heading":"6.7.1 Effect size","text":"Yung Liu (2019), section 2.3.3, shown , test statistics \\(Z\\rightarrow_d\\mathcal{N}(\\sqrt{n}\\theta, 1)\\) approximately,\n\\(\\theta = \\Delta/\\sigma\\) effect size. test statistics weighted logrank test :\\[ Z=\\sqrt{\\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\\int_{0}^{\\tau}w(t)\\frac{\\overline{Y}_{0}(t)\\overline{Y}_{1}(t)}{\\overline{Y}_{0}(t)+\\overline{Y}_{0}(t)}\\left\\{ \\frac{d\\overline{N}_{1}(t)}{\\overline{Y}_{1}(t)}-\\frac{d\\overline{N}_{0}(t)}{\\overline{Y}_{0}(t)}\\right\\} \\]Weight \\(w(t)\\): implemented gsdmvn.Integration total follow-time \\(\\tau\\).\\[\\Delta=\\int_{0}^{\\tau}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)}\\{\\lambda_{1}(s)-\\lambda_{0}(s)\\}ds\\],\\[\\sigma^{2}=\\int_{0}^{\\tau}w(s)^{2}\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^{2}}dv(s)\\]definitions component.\\(n = n_0 + n_1\\): Total sample size\\(p_0 = n_0/n\\), \\(p_1 = n_1/n\\): Randomization probability inferred randomization ratio\\(\\pi_0(t) = E\\{N_0(t)\\}\\), \\(\\pi_1(t) = E\\{N_1(t)\\}\\)\\(\\pi(t) = p_0\\pi_0(t)+p_1\\pi_1(t)\\): Probability events\\(v(t) = p_0E\\{Y_0(t)\\}+p_1E\\{Y_1(t)\\}\\): Probability people risk","code":"\nweight <- function(x, arm0, arm1) {\n  gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n}\ndelta <- abs(gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = arm0$total_time, weight = weight))\ndelta#> [1] 0.02623776\nsigma2 <- gsdmvn:::gs_sigma2_wlr(arm0, arm1, tmax = arm0$total_time, weight = weight)\nsigma2#> [1] 0.0242674"},{"path":"fixed-design.html","id":"sample-size-and-number-of-events","chapter":"6 Fixed design","heading":"6.7.2 Sample size and number of events","text":"can calculate sample size deriving effect size.\\[ n = \\frac{\\sigma^{2}(z_{\\alpha}+z_{\\beta})^{2}}{\\Delta^2} = \\frac{(z_{\\alpha}+z_{\\beta})^{2}}{\\theta^2} \\]number events can also calculated . details can found technical details.","code":"\nz_alpha <- qnorm(1 - 0.025)\nz_beta <- qnorm(1 - 0.2)\nn <- sigma2 * (z_alpha + z_beta)^2 / delta^2\nn#> [1] 276.6798\n# Sample size for FH(0,1)\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p1_q0\")\n)#>        n0        n1         n        d0        d1         d \n#> 138.39353 138.39353 276.78707 102.15617  81.23792 183.39408\nn0 <- n1 <- n / 2\ngsdmvn:::prob_event.arm(arm0, tmax = arm0$total_time) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = arm0$total_time) * n1#> [1] 183.323"},{"path":"fixed-design.html","id":"technical-details","chapter":"6 Fixed design","heading":"6.8 Technical details","text":"","code":""},{"path":"fixed-design.html","id":"accrual-and-follow-up-time","chapter":"6 Fixed design","heading":"6.8.1 Accrual and follow-up time","text":"\\(R\\): time study entry\\(F_R(\\cdot)\\) CDF \\(R\\)\\(\\tau_a\\): accrual time\\(\\tau_f\\): follow-time\\(\\tau = \\tau_a + \\tau_f\\): total study duration","code":"\nx <- seq(0, arm0$total_time, 0.1)\n# Piecewise Uniform Distribution\nplot(x, npsurvSS::daccr(x, arm = arm0),\n  type = \"s\",\n  xlab = \"Calendar time (month)\", ylab = \"Accrual Probability\"\n)\narm0$accr_time#> [1] 12\narm0$follow_time#> [1] 24\narm0$total_time#> [1] 36"},{"path":"fixed-design.html","id":"event-time","chapter":"6 Fixed design","heading":"6.8.2 Event time","text":"\\(T\\): time event study entry.\\(S_T(\\cdot)\\): Survival function\\(F_T(\\cdot)\\): CDF \\(T\\)\\(f_T(\\cdot)\\): Density function","code":"\n# Survival function of time to event from study entry\n# Piecewise Exponential distribution\nplot(x, 1 - npsurvSS::psurv(x, arm0),\n  type = \"l\",\n  xlab = \"Calendar time (month)\", ylab = \"Survival Probability\", ylim = c(0, 1)\n)\nlines(x, 1 - npsurvSS::psurv(x, arm1), lty = 2)\nlegend(\"topright\", lty = c(1, 2), legend = c(\"control\", \"experiment\"))"},{"path":"fixed-design.html","id":"censoring-time","chapter":"6 Fixed design","heading":"6.8.3 Censoring time","text":"\\(L\\): time loss follow-study entry\\(S_L(\\cdot)\\): Survival function \\(L\\)\\(C = \\min(L, \\tau - R)\\): time censoring study entry.","code":"\n# PDF of the time to loss of follow-up from study entry\n# Exponential Distribution\nplot(x, 1 - pexp(x),\n  type = \"l\",\n  xlab = \"Calendar time (month)\", ylab = \"Loss to Follow-up Probability\"\n)"},{"path":"fixed-design.html","id":"observed-time","chapter":"6 Fixed design","heading":"6.8.4 Observed time","text":"\\(U = \\min(T,C)\\): observed time.\\(\\delta = (T<C)\\): event indicator","code":""},{"path":"fixed-design.html","id":"expected-events","chapter":"6 Fixed design","heading":"6.8.5 Expected events","text":"\\(N(t) = (U \\le t, \\delta = 1)\\): counting process\\(E\\{N(t)\\}\\): expected probability events\\[E\\{N(t)\\} = \\int_{0}^{t}f_{T}(s)S_{L}(s)F_{R}(\\tau_{}\\land(\\tau-s))ds\\]\\(Y(t) = (U\\ge t)\\): -risk process\\(E\\{Y(t)\\}\\): expected probability risk\\[E\\{Y(t)\\} = S_{T}(s)S_{L}(s)F_{R}(\\tau_{}\\land(\\tau-s))\\]","code":"\n# Probability of Events\nx_int <- 0:arm0$total_time\nplot(x_int, gsdmvn:::prob_event.arm(arm0, tmax = x_int),\n  type = \"l\",\n  xlab = \"Calendar time (month)\", ylab = \"Event Probability\",\n  ylim = c(0, 1)\n)\n# Probability of People at risk\nplot(x_int, gsdmvn:::prob_risk(arm0, x_int, arm0$total_time),\n  type = \"l\",\n  xlab = \"Calendar time (month)\", ylab = \"Probability at Risk\", ylim = c(0, 1)\n)"},{"path":"fixed-design.html","id":"weight-function-in-weighted-logrank-test","chapter":"6 Fixed design","heading":"6.8.6 Weight function in weighted logrank test","text":"Define different weight functions gsdmvnWeight function: \\(w(t)\\)Constant weight (logrank test): \\(1\\)Fleming-Harrington weight: \\(w(t) = FH^{\\rho, \\gamma}(t) = S(t)^\\rho(1 - S(t))^\\gamma\\)Modestly WLR: \\(S^{-1}(t\\land\\tau_m)\\) Magirr Burman (2019b)Modestly WLR: \\(S^{-1}(t\\land\\tau_m)\\) Magirr Burman (2019b)Choose \\(\\tau_m\\) around change point delay effect scenarioChoose \\(\\tau_m\\) around change point delay effect scenarioOnly -weight early event. Constant weight change point.-weight early event. Constant weight change point.","code":"\ngsdmvn::wlr_weight_1(x = c(12, 24, 36), arm0, arm1)#> [1] 1\nweight_legend <- apply(expand.grid(c(\"rho=0\", \"rho=1\"), c(\"gamma=0\", \"gamma=1\")), 1, paste0, collapse = \"; \")\nplot(1:36, gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 0, gamma = 0),\n  xlab = \"Calendar time (month)\", col = 1,\n  ylab = \"Weight\", type = \"l\", ylim = c(-0.5, 1.2)\n)\nlines(1:36, gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 1, gamma = 0), col = 2)\nlines(1:36, gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 0, gamma = 1), col = 3)\nlines(1:36, gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 1, gamma = 1), col = 4)\nlegend(\"bottomright\", legend = weight_legend, lty = 1, col = 1:4)\nplot(1:36, gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = -1, gamma = 0, tau = 4),\n  xlab = \"Calendar time (month)\", ylab = \"Weight\", type = \"l\"\n)"},{"path":"fixed-design.html","id":"average-hazard-ratio","chapter":"6 Fixed design","heading":"6.8.7 Average hazard ratio","text":"\\(\\Delta\\) weighted average hazard function difference \\(\\lambda_{1}(\\cdot)-\\lambda_{0}(\\cdot)\\).\\[\\Delta=\\int_{0}^{\\tau}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)}\\{\\lambda_{1}(s)-\\lambda_{0}(s)\\}ds\\]Taylor expansion builds connection \\(\\Delta\\) weighted average log hazard ratio (AHR).\ngeneralization Schoenfeld (1981) asymptotic expansion.\\[\\Delta\\approx\\int_{0}^{\\tau}w(s)\\log\\left(\\frac{\\lambda_1(s)}{\\lambda_0(s)}\\right)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^2}v'(s)ds\\]Log AHR can estimated normalizing weights \\(\\Delta\\)\\[ \\log{AHR} = \\frac{\\Delta}{\\int_{0}^{\\tau}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^2}v'(s)ds} \\]Note AHR depends choice weight function \\(w(\\cdot)\\).Note AHR depends choice weight function \\(w(\\cdot)\\).logrank test piecewise exponential distribution 1:1 randomization ratio,\ncan simplified toUnder logrank test piecewise exponential distribution 1:1 randomization ratio,\ncan simplified \\[\\log(AHR) = \\frac{\\sum d_i \\log{HR_i}}{\\sum d_i}\\]Also computed gsDesign2::AHR().","code":"\nt <- 1:36\nlog_ahr <- sapply(t, function(t) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t, weight = weight) /\n    gsdmvn:::gs_delta_wlr(arm0, arm1,\n      tmax = t, weight = weight,\n      approx = \"generalized schoenfeld\", normalization = TRUE\n    )\n})\nplot(t, exp(log_ahr),\n  ylim = c(0.6, 1),\n  xlab = \"Calendar time (month)\",\n  ylab = \"Average Hazard Ratio\",\n  type = \"l\"\n)\ngsDesign2::AHR(enrollRates, failRates, totalDuration = arm0$total_time)#> # A tibble: 1 × 5\n#>    Time   AHR Events  info info0\n#>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n#> 1    36 0.683   331.  81.4  82.8\nlog_ahr <- gsdmvn:::gs_delta_wlr(arm0, arm1,\n  tmax = arm0$total_time,\n  weight = gsdmvn:::wlr_weight_1\n) /\n  gsdmvn:::gs_delta_wlr(arm0, arm1,\n    tmax = arm0$total_time,\n    weight = gsdmvn:::wlr_weight_1,\n    approx = \"generalized schoenfeld\",\n    normalization = TRUE\n  )\nexp(log_ahr)#> [1] 0.6831735"},{"path":"fixed-design.html","id":"maxcombo-test","chapter":"6 Fixed design","heading":"6.9 MaxCombo test","text":"MaxCombo test statistics maximum multiple WLR test statistics using different weight functions.\nfocus Fleming Harrington weight function \\(FH(\\rho, \\gamma)\\) defined gsdmvn::wlr_weight_fh().calculate sample size MaxCombo test,\nneed know variance-covariance multiple WLR test statistics.Fleming Harrington weight function, covariance different WLR test statistics can calculated.\nBased Karrison (2016) Wang et al (2019),\ncovariance two test statistics \\(Z_1 = Z(\\rho_1, \\gamma_1), Z_2 = Z(\\rho_2, \\gamma_2)\\) \\[\\text{Cov}(Z_1, Z_2) = \\text{Var}(Z(\\frac{\\rho_1 + \\rho_2}{2}, \\frac{\\gamma_1 + \\gamma_2}{2}))\\]illustrate idea based \\(FH(0, 0.5)\\) \\(FH(0.5, 0.5)\\)illustrate idea based \\(FH(0, 0.5)\\) \\(FH(0.5, 0.5)\\)weight functionsAll weight functions\\(\\Delta\\)CovarianceCorrelation","code":"\nweight_combo <- list(\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0, gamma = 0.5\n    )\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0.25, gamma = 0.5\n    )\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0.25, gamma = 0.5\n    )\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0.5, gamma = 0.5\n    )\n  }\n)\ndelta_combo <- sapply(weight_combo[c(1, 4)], function(x) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = arm0$total_time, weight = x)\n})\ndelta_combo#> [1] -0.03980774 -0.02933963\nsigma2_combo <- sapply(weight_combo, function(x) {\n  gsdmvn:::gs_sigma2_wlr(arm0, arm1, tmax = arm0$total_time, weight = x)\n})\nsigma2_combo <- matrix(sigma2_combo, nrow = 2)\nsigma2_combo#>            [,1]       [,2]\n#> [1,] 0.05441018 0.04007109\n#> [2,] 0.04007109 0.03014093\ncorr_combo <- cov2cor(sigma2_combo)\ncorr_combo#>          [,1]     [,2]\n#> [1,] 1.000000 0.989493\n#> [2,] 0.989493 1.000000"},{"path":"fixed-design.html","id":"type-i-error-and-power","chapter":"6 Fixed design","heading":"6.9.1 Type I error and power","text":"formula calculate sample size based effect size directly apply,\ntest statistic MaxCombo asymptotically normal.type Error :\\[\\alpha = \\text{Pr}(\\max(Z_1, Z_2) > z_{\\alpha} \\, | \\, H_0) = 1 - \\text{Pr}(Z_1 < z_{\\alpha}, Z_2 < z_{\\alpha} \\, | \\, H_0)\\]power \\[\\beta = \\text{Pr}(\\max(Z_1, Z_2) > z_{\\alpha} \\, | \\, H_1) = 1 - \\text{Pr}(Z_1 < z_{\\alpha}, Z_2 < z_{\\alpha} \\, | \\, H_1)\\]","code":"\nlibrary(mvtnorm)\n\nz_alpha <- qmvnorm(p = 1 - 0.025, corr = corr_combo)\nz_alpha$quantile#> [1] 2.014555\n#' @param n Sample size\n#' @param power Target power. Use power=0 to calculate the actual power.\npower_combo <- function(n, power = 0) {\n  theta <- abs(delta_combo) / sqrt(diag(sigma2_combo))\n  power_combo <- 1 - pmvnorm(\n    upper = z_alpha$quantile,\n    mean = sqrt(n) * theta, corr = corr_combo\n  )\n  as.numeric(power_combo) - power\n}\npower_combo(n = 150)#> [1] 0.5493368"},{"path":"fixed-design.html","id":"sample-size","chapter":"6 Fixed design","heading":"6.9.2 Sample size","text":"calculate sample size, can solve power function giving Type error power.","code":"\nn_combo <- uniroot(power_combo, interval = c(0, 500), power = 0.8)$root\nn_combo#> [1] 271.0453\nc(z = z_alpha$quantile, n = n_combo, power = power_combo(n_combo))#>          z          n      power \n#>   2.014555 271.045320   0.800000"},{"path":"fixed-design.html","id":"number-of-events","chapter":"6 Fixed design","heading":"6.9.3 Number of events","text":"number events required can calculated accordingly weighted logrank test.","code":"\nn0 <- n1 <- n_combo / 2\ngsdmvn:::prob_event.arm(arm0, tmax = arm0$total_time) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = arm0$total_time) * n1#> [1] 179.5897"},{"path":"wlr.html","id":"wlr","chapter":"7 Weighted logrank test","heading":"7 Weighted logrank test","text":"chapter, start discuss group sequential design weighted logrank test.","code":""},{"path":"wlr.html","id":"assumptions","chapter":"7 Weighted logrank test","heading":"7.1 Assumptions","text":"group sequential design, assume total \\(K\\) analyses trial.\ncalendar time analyses \\(t_k, k =1,\\dots, K\\).define test statistic analysis \\(k=1,2,\\dots,K\\) \\[Z_k =  \\frac{\\widehat{\\Delta}_k}{\\sigma(\\widehat{\\Delta}_k)}\\]\n\\(\\widehat{\\Delta}_k\\) statistics characterize group difference \n\\(\\sigma(\\widehat{\\Delta}_k)\\) standard deviation \\(\\widehat{\\Delta}_k\\).","code":""},{"path":"wlr.html","id":"asymptotic-normality","chapter":"7 Weighted logrank test","heading":"7.1.1 Asymptotic normality","text":"group sequential design, consider test statistics asymptotically normal.null hypothesis\\[Z_k \\sim \\mathcal{N}(0,1)\\](local) alternative hypothesis\\[Z_k \\sim \\mathcal{N}(\\Delta_k I_k^{1/2}, 1)\\],\\(I_k\\) Fisher information \\(I_k \\approx 1/{\\sigma^2(\\widehat{\\Delta}_k)}\\)can define effect size \\(\\theta = \\Delta_k / \\sigma_k = \\Delta_k I_k^{1/2}\\) \\(\\sigma_k = \\sigma(\\widehat{\\Delta}_k)\\)., equivalent claim:\\[Z_k \\sim \\mathcal{N}(\\theta_k, 1)\\]","code":""},{"path":"wlr.html","id":"independent-increments-process","chapter":"7 Weighted logrank test","heading":"7.1.2 Independent increments process","text":"group sequential design, need demonstrate test statistics \\(Z = (Z_1, \\dots, Z_K)\\)\nindependent increments process. property WLR proved Scharfstein et al (1997)\ncorollary martingale representation.null \\(Z ~ \\sim MVN(0,\\Sigma)\\)null \\(Z ~ \\sim MVN(0,\\Sigma)\\)local alternative \\(Z ~ \\sim MVN(\\theta, \\Sigma)\\)local alternative \\(Z ~ \\sim MVN(\\theta, \\Sigma)\\)\\(\\Sigma={\\Sigma_{ij}}\\) correlation matrix withHere \\(\\Sigma={\\Sigma_{ij}}\\) correlation matrix \\[\\Sigma_{ij} = \\min(I_i, I_j) / \\max(I_i, I_j) \\approx \\min(\\sigma_i, \\sigma_j) / \\max(\\sigma_i, \\sigma_j)\\]","code":""},{"path":"wlr.html","id":"type-i-error-alpha-spending-function-and-group-sequential-design-boundary","chapter":"7 Weighted logrank test","heading":"7.2 Type I error, \\(\\alpha\\)-spending function and group sequential design boundary","text":"Define test boundary interim analysis\n\\(\\alpha\\)-spending function Lan DeMets (1983)\n\\(\\alpha\\)-spending function Lan DeMets (1983)Bounds \\(-\\infty \\le a_k \\le b_k \\le \\infty\\) \\(k=1,\\dots,K\\)\nnon-binding futility bound \\(a_k = -\\infty\\) \\(k\\)\nnon-binding futility bound \\(a_k = -\\infty\\) \\(k\\)Upper boundary crossing probabilities\n\\(u_k = \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j < b_j\\})\\)\n\\(u_k = \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j < b_j\\})\\)Lower boundary crossing probabilities\n\\(l_k = \\text{Pr}(\\{Z_k < a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j < b_j\\})\\)\n\\(l_k = \\text{Pr}(\\{Z_k < a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j < b_j\\})\\)null hypothesis, probability reject null hypothesis.\n\\(\\alpha = \\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\} \\mid H_0)\\)\nspending function family \\(f(t,\\gamma)\\),\ngsDesign::sfLDOF() (O’Brien-Fleming bound approximation) functions starts sf gsDesign\n\\(\\alpha = \\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\} \\mid H_0)\\)spending function family \\(f(t,\\gamma)\\),gsDesign::sfLDOF() (O’Brien-Fleming bound approximation) functions starts sf gsDesignFor simplicity, directly use gsDesign boundary chapter.\ninflate Type Error WLR tests.\ndiscuss fix issue next chapter.Lower boundUpper bound","code":"\nx <- gsDesign::gsSurv(\n  k = 3, test.type = 4, alpha = 0.025,\n  beta = 0.1, astar = 0, timing = c(1),\n  sfu = gsDesign::sfLDOF, sfupar = c(0),\n  sfl = gsDesign::sfLDOF, sflpar = c(0),\n  lambdaC = c(0.1),\n  hr = 0.6, hr0 = 1, eta = 0.01,\n  gamma = c(10),\n  R = c(12), S = NULL,\n  T = 36, minfup = 24, ratio = 1\n)\nx$lower$bound#> [1] -0.6945842  1.0023997  1.9929702\nx$upper$bound#> [1] 3.710303 2.511407 1.992970"},{"path":"wlr.html","id":"power","chapter":"7 Weighted logrank test","heading":"7.3 Power","text":"Given lower upper bound group sequential design, can calculate overall power design.Power: (local) alternative hypothesis, probability reject null hypothesis.\\[1 - \\beta =\\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\} \\mid H_1)\\]lower bound, formula can simplified \\[\\beta = \\text{Pr}(\\cap_{j=1}^{K} \\{Z_j \\le b_j\\} \\mid H_1)\\]can calculate sample size required group sequential design solving\npower equation given lower upper bound power, type error power.note, can calculate futility probability similarly:\\[\\sum_{k=1}^{K} l_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k < a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\}\\mid H_1)\\]","code":""},{"path":"wlr.html","id":"weighted-logrank-test-1","chapter":"7 Weighted logrank test","heading":"7.4 Weighted logrank test","text":"Similar fixed design, can define test statistics weighted logrank test using counting process formula \\[ Z_k=\\sqrt{\\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\\int_{0}^{t_k}w(t)\\frac{\\overline{Y}_{0}(t)\\overline{Y}_{1}(t)}{\\overline{Y}_{0}(t)+\\overline{Y}_{0}(t)}\\left\\{ \\frac{d\\overline{N}_{1}(t)}{\\overline{Y}_{1}(t)}-\\frac{d\\overline{N}_{0}(t)}{\\overline{Y}_{0}(t)}\\right\\} \\]\\(n_i\\) number subjects group \\(\\).\n\\(\\overline{Y}_{}(t)\\) number subjects group \\(\\) risk time \\(t\\).\n\\(\\overline{N}_{}(t)\\) number events group \\(\\) including time \\(t\\).Note, difference test statistics fixed analysis \\(t_k\\) \\(k\\)th interim analysisFor simplicity, illustrate sample size power calculation based boundary logrank test.\nboundary different type analysis fair comparison.\nHowever, different WLR can different information fraction interim analysis time.\ndiscussion spending function based actual information fraction provided later.","code":""},{"path":"wlr.html","id":"example-scenario","chapter":"7 Weighted logrank test","heading":"7.5 Example scenario","text":"considered example scenario similar fixed design.\nkey difference considered 2 interim analysis 12 24 month final analysis 36 month.","code":"\nenrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500 / 12)\n\nfailRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15, # Median survival 15 month\n  hr = c(1, 0.6),\n  dropoutRate = 0.001\n)\n# Randomization Ratio is 1:1\nratio <- 1\n\n# Type I error (one-sided)\nalpha <- 0.025\n\n# Power (1 - beta)\nbeta <- 0.2\npower <- 1 - beta\n\n# Interim Analysis Time\nanalysisTimes <- c(12, 24, 36)"},{"path":"wlr.html","id":"sample-size-under-logrank-test-or-fh0-0","chapter":"7 Weighted logrank test","heading":"7.6 Sample size under logrank test or \\(FH(0, 0)\\)","text":"gsdmvn, sample size can calculated using gsdmvn::gs_design_wlr() WLR test.\ncomparison purposes, also provided sample size calculation using gsdmvn::gs_design_ahr() logrank test.\ncalculation, compare analytical results simulation results 10,000 replications.described theoretical details sample size calculation technical details section.AHR logrank testSimulation results based 10,000 replications.\\(FH(0,0)\\)Simulation results based 10,000 replications.","code":"\ngsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2) %>%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)#> # A tibble: 3 × 7\n#>   Analysis  Time     N Events   AHR Upper Lower\n#>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1        1    12  386.   82.9  0.84  0     0.07\n#> 2        2    24  386.  190.   0.71  0.41  0.13\n#> 3        3    36  386.  256.   0.68  0.8   0.2#>      n  t events  ahr lower upper\n#> 10 386 12  82.77 0.87  0.07  0.00\n#> 11 386 24 190.05 0.72  0.14  0.41\n#> 12 386 36 255.61 0.69  0.20  0.80\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = c(12, 24, 36)\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2) %>%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)#> # A tibble: 3 × 7\n#>   Analysis  Time     N Events   AHR Upper Lower\n#>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1        1    12  383.   82.3  0.84  0     0.07\n#> 2        2    24  383.  189.   0.72  0.41  0.14\n#> 3        3    36  383.  254.   0.68  0.8   0.2#>     n  t events  ahr lower upper\n#> 4 384 12  82.36 0.86  0.07  0.00\n#> 5 384 24 189.05 0.72  0.14  0.41\n#> 6 384 36 254.30 0.69  0.20  0.80"},{"path":"wlr.html","id":"sample-size-under-modestly-wlr-cut-at-4-months","chapter":"7 Weighted logrank test","heading":"7.7 Sample size under modestly WLR cut at 4 months","text":"Modestly WLR: \\(S^{-1}(t\\land\\tau_m)\\) Magirr Burman (2019b)","code":"\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = -1, gamma = 0, tau = 4)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  365.   78.5  3.71        0     0.83  0.16  25.0  25.1\n#> 2        2 Upper    24  365.  180.   2.51        0.41  0.71  0.29  61.1  61.9\n#> 3        3 Upper    36  365.  242.   1.99        0.8   0.68  0.33  82.9  85.0\n#> 4        1 Lower    12  365.   78.5 -0.69        0.07  0.83  0.16  25.0  25.1\n#> 5        2 Lower    24  365.  180.   1           0.13  0.71  0.29  61.1  61.9\n#> 6        3 Lower    36  365.  242.   1.99        0.2   0.68  0.33  82.9  85.0"},{"path":"wlr.html","id":"sample-size-under-fh0-1","chapter":"7 Weighted logrank test","heading":"7.8 Sample size under \\(FH(0, 1)\\)","text":"Simulation results based 10,000 replications.","code":"\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2) %>%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)#> # A tibble: 3 × 7\n#>   Analysis  Time     N Events   AHR Upper Lower\n#>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1        1    12  316.   68.0  0.73  0     0.04\n#> 2        2    24  316.  156.   0.64  0.45  0.11\n#> 3        3    36  316.  210.   0.62  0.8   0.2#>      n  t events  ahr lower upper\n#> 16 317 12  68.01 0.76  0.04  0.00\n#> 17 317 24 156.13 0.65  0.12  0.45\n#> 18 317 36 210.06 0.63  0.21  0.79"},{"path":"wlr.html","id":"sample-size-under-fh0-0.5","chapter":"7 Weighted logrank test","heading":"7.9 Sample size under \\(FH(0, 0.5)\\)","text":"Simulation results based 10,000 replications.","code":"\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2) %>%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)#> # A tibble: 3 × 7\n#>   Analysis  Time     N Events   AHR Upper Lower\n#>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1        1    12  314.   67.4  0.78  0     0.05\n#> 2        2    24  314.  155.   0.67  0.44  0.12\n#> 3        3    36  314.  208.   0.64  0.8   0.2#>      n  t events  ahr lower upper\n#> 22 314 12  67.21 0.81  0.05  0.00\n#> 23 314 24 154.43 0.67  0.12  0.44\n#> 24 314 36 207.92 0.65  0.21  0.79"},{"path":"wlr.html","id":"sample-size-under-fh0.5-0.5","chapter":"7 Weighted logrank test","heading":"7.10 Sample size under \\(FH(0.5, 0.5)\\)","text":"Simulation results based 10,000 replications.","code":"\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2) %>%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %>%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)#> # A tibble: 3 × 7\n#>   Analysis  Time     N Events   AHR Upper Lower\n#>      <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1        1    12  317.   68.0  0.79  0     0.05\n#> 2        2    24  317.  156    0.68  0.43  0.12\n#> 3        3    36  317.  210.   0.65  0.8   0.2#>      n  t events  ahr lower upper\n#> 28 317 12  67.87 0.81  0.06  0.00\n#> 29 317 24 155.86 0.68  0.12  0.43\n#> 30 317 36 209.82 0.66  0.20  0.80"},{"path":"wlr.html","id":"average-hazard-ratio-comparison","chapter":"7 Weighted logrank test","heading":"7.11 Average hazard ratio comparison","text":"average hazard ratio depends weight used WLR.\nillustrate difference figure .Note average hazard ratio weighted corresponding weighted logrank weights.","code":""},{"path":"wlr.html","id":"standardized-effect-size-comparison","chapter":"7 Weighted logrank test","heading":"7.12 Standardized effect size comparison","text":"Standardized effect size \\(FH(0, 0.5)\\) larger weight function month 36.\nreason \\(FH(0.5, 0.5)\\) provides slightly smaller sample size compared \\(FH(0, 1)\\) \\(FH(0, 0.5)\\)\nmainly due smaller weight variance becomes larger \\(FH(0, 1)\\) later events.","code":""},{"path":"wlr.html","id":"information-fraction-comparison","chapter":"7 Weighted logrank test","heading":"7.13 Information fraction comparison","text":"logrank test information fraction curve close linear function time (boundary calculation approach used).\nWLR test information fraction curves convex functions.\nInformation Fraction null hypothesis smaller information fraction alternative time.","code":""},{"path":"wlr.html","id":"technical-details-1","chapter":"7 Weighted logrank test","heading":"7.14 Technical details","text":"section describes details calculating sample size\nevents required WLR group sequential design.\ncan skipped first read training material.illustrate idea using \\(FH(0, 1)\\).calculation power essentially multiple integration multivariate normal distribution, implement function gs_power() mvtnorm::pmvnorm() used take care multiple integration.utilize gs_power(), four important blocks required.expectation mean Z statistics. derive , one needs \\(\\Delta_k\\), \\(\\sigma_k^{2}\\) sample size ratio interim analysis.correlation matrix Z statistics (denoted \\(\\Sigma\\)).numerical vector futility bound multiple integration.numerical vector efficacy bound multiple integration.First, calculate\n\\[\n  \\Delta_k\n  =\n  \\int_{0}^{t_k}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)}\\{\\lambda_{1}(s)-\\lambda_{0}(s)\\}ds,\n\\]\n\\(p_0 = n_0/(n_0 + n_1), p_1 = n_1/(n_0 + n_1)\\) randomization ratio control treatment arm, respectively.\n\\(\\pi_0(t) = E(N_0(t)), \\pi_1(t) = E(N_1(t))\\) expected number failures control treatment arm, respectively.\n\\(\\pi(t) = p_0 \\pi_0(t) + p_1 \\pi_1(t)\\) probability events.\n\\(\\lambda_0(t), \\lambda_1(t)\\) hazard functions.\ndetailed calculation \\(\\Delta_k\\) implemented gsdmvn:::gs_delta_wlr():Second, calculate\n\\[\\sigma_k^{2}=\\int_{0}^{t_k}w(s)^{2}\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^{2}}dv(s), \\]\n\\(v(t)= p_0 E(Y_0(t)) + p_1 E(Y_1(t))\\) probability people risk.\ndetailed calculation \\(\\sigma_k^{2}\\) implemented gsdmvn:::gs_sigma2_wlr():Third, calculate sample size ratio interim analysis can calculated byAfter getting \\(\\Delta_k\\), \\(\\sigma_k^{2}\\) sample size ratio, one can calculate expectation mean Z statistics asThen, calculate correlation matrix \\(\\Sigma\\) asFinally, numerical vector futility bound, simply","code":"\nweight <- function(x, arm0, arm1) {\n  gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n}\n# Define study design object in each arm\ngs_arm <- gsdmvn:::gs_create_arm(\n  enrollRates, failRates,\n  ratio = 1, # Randomization ratio\n  total_time = 36 # Total study duration\n)\narm0 <- gs_arm[[\"arm0\"]]\narm1 <- gs_arm[[\"arm1\"]]\n#' Power and futility of group sequential design\n#'\n#' @param z Numerical vector of Z statistics\n#' @param corr Correlation matrix of Z statistics\n#' @param futility_bound Numerical vector of futility bound.\n#'                       -Inf indicates non-binding futility bound.\n#' @param efficacy_bound Numerical vector of efficacy bound.\n#'                       Inf indicates no early stop to declare superiority.\ngs_power <- function(z, corr, futility_bound, efficacy_bound) {\n  p <- c()\n  p[1] <- 1 - pnorm(efficacy_bound[1], mean = z[1], sd = 1)\n\n  for (k in 2:length(z)) {\n    lower_k <- c(futility_bound[1:(k - 1)], efficacy_bound[k])\n    upper_k <- c(efficacy_bound[1:(k - 1)], Inf)\n    p[k] <- mvtnorm::pmvnorm(\n      lower = lower_k, upper = upper_k,\n      mean = z[1:k], corr = corr[1:k, 1:k]\n    )\n  }\n\n  p\n}\ndelta <- abs(sapply(analysisTimes, function(x) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = x, weight = weight)\n}))\ndelta#> [1] 0.002227119 0.013851909 0.026237755\nsigma2 <- abs(sapply(analysisTimes, function(x) {\n  gsdmvn:::gs_sigma2_wlr(arm0, arm1, tmax = x, weight = weight)\n}))\nsigma2#> [1] 0.001411557 0.010443360 0.024267396\n# Group sequential sample size ratio over time\ngs_n_ratio <- (npsurvSS::paccr(analysisTimes, arm0) +\n  npsurvSS::paccr(analysisTimes, arm1)) / 2\ngs_n_ratio#> [1] 1 1 1\nn <- 500 # Assume a sample size to get power\ndelta / sqrt(sigma2) * sqrt(gs_n_ratio * n)#> [1] 1.325499 3.030920 3.766172\ncorr <- outer(sqrt(sigma2), sqrt(sigma2), function(x, y) pmin(x, y) / pmax(x, y))\ncorr#>           [,1]      [,2]      [,3]\n#> [1,] 1.0000000 0.3676453 0.2411779\n#> [2,] 0.3676453 1.0000000 0.6560071\n#> [3,] 0.2411779 0.6560071 1.0000000\nx$lower$bound#> [1] -0.6945842  1.0023997  1.9929702\nx$upper$bound#> [1] 3.710303 2.511407 1.992970"},{"path":"wlr.html","id":"power-and-sample-size-calculation","chapter":"7 Weighted logrank test","heading":"7.14.1 Power and sample size calculation","text":"PowerWith blocks prepared, one can calculate power\n\\[\n  1 -\\beta\n  =\n  \\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\})\n\\]\nbyType errorOne can also calculate type error byFutility probabilitySimilarly, can calculate futility probability\\[\\sum_{k=1}^{K} l_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k < a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\})\\]\nbySample sizeIn additional power futility probability, one can also calculate sample size byNumber eventsWith sample size available, one can calculate number events byAverage hazard ratioThe function implement connection \\(\\Delta\\) average hazard ratio using\nTaylor series expansion.info column \\(\\sigma^2_k\\) times \\(N\\)info0 column \\(\\sigma^2_k\\) times \\(N\\) null use arm0 calculation active control group.","code":"\ncumsum(gs_power(\n  delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),\n  corr,\n  x$lower$bound,\n  x$upper$bound\n))#> [1] 0.00854411 0.69113520 0.93587399\ncumsum(gs_power(\n  rep(0, length(delta)),\n  corr,\n  rep(-Inf, length(delta)),\n  x$upper$bound\n))#> [1] 0.0001035057 0.0061027684 0.0266460809\n#' @rdname power_futility\ngs_futility <- function(z, corr, futility_bound, efficacy_bound) {\n  p <- c()\n  p[1] <- pnorm(futility_bound[1], mean = z[1], sd = 1)\n\n  for (k in 2:length(z)) {\n    lower_k <- c(futility_bound[1:(k - 1)], -Inf)\n    upper_k <- c(efficacy_bound[1:(k - 1)], futility_bound[k])\n    p[k] <- mvtnorm::pmvnorm(\n      lower = lower_k, upper = upper_k,\n      mean = z[1:k], corr = corr[1:k, 1:k]\n    )\n  }\n\n  p\n}\ncumsum(gs_futility(\n  delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),\n  corr, x$lower$bound, x$upper$bound\n))#> [1] 0.02168739 0.04054411 0.06413098\nefficacy_bound <- x$upper$bound\nfutility_bound <- x$lower$bound\n\n# Sample size to event\nn <- uniroot(\n  function(n, power) {\n    power - sum(gs_power(\n      delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),\n      corr, futility_bound, efficacy_bound\n    ))\n  },\n  interval = c(1, 1e5), power = 1 - beta\n)$root\nn_subject <- n * ratio / sum(ratio)\nn_subject * gs_n_ratio#> [1] 316.448 316.448 316.448\nn0 <- n1 <- n_subject / 2\ngsdmvn:::prob_event.arm(arm0, tmax = analysisTimes) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = analysisTimes) * n1#> [1]  67.9694 155.8718 209.6727\nlog_ahr <- sapply(analysisTimes, function(t_k) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t_k, weight = weight) /\n    gsdmvn:::gs_delta_wlr(arm0, arm1,\n      tmax = t_k, weight = weight,\n      approx = \"generalized schoenfeld\",\n      normalization = TRUE\n    )\n})\nexp(log_ahr)#> [1] 0.7342540 0.6372506 0.6174103\nn_subject * sigma2#> [1] 0.4466843 3.3047804 7.6793688"},{"path":"wlr.html","id":"cross-comparison-with-gs_design_wlr","chapter":"7 Weighted logrank test","heading":"7.14.2 Cross comparison with gs_design_wlr()","text":"","code":"\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  316.   68.0  3.71        0     0.73  1.58  0.45  0.45\n#> 2        2 Upper    24  316.  156.   2.51        0.45  0.64  1.33  3.3   3.42\n#> 3        3 Upper    36  316.  210.   1.99        0.8   0.62  1.08  7.68  8.2 \n#> 4        1 Lower    12  316.   68.0 -0.69        0.04  0.73  1.58  0.45  0.45\n#> 5        2 Lower    24  316.  156.   1           0.11  0.64  1.33  3.3   3.42\n#> 6        3 Lower    36  316.  210.   1.99        0.2   0.62  1.08  7.68  8.2"},{"path":"wlr.html","id":"illustration-of-gs_info_wlr","chapter":"7 Weighted logrank test","heading":"7.14.3 Illustration of gs_info_wlr()","text":"necessary information also summarized data frame using gsdmvn::gs_info_wlr().N based information enrollRatesEvents probability events times \\(N\\)AHR average hazard ratiodelta, sigma2, theta defined aboveinfo column \\(\\sigma^2_k\\) times \\(N\\)info0 column \\(\\sigma^2_k\\) times \\(N\\) null.","code":"\ngs_info <- gsdmvn::gs_info_wlr(\n  enrollRates, failRates, ratio,\n  analysisTimes = analysisTimes,\n  weight = weight\n)\n\ngs_info %>% mutate_if(is.numeric, round, digits = 2)#>   Analysis Time   N Events  AHR delta sigma2 theta  info info0\n#> 1        1   12 500 107.39 0.73  0.00   0.00  1.58  0.71  0.71\n#> 2        2   24 500 246.28 0.64 -0.01   0.01  1.33  5.22  5.41\n#> 3        3   36 500 331.29 0.62 -0.03   0.02  1.08 12.13 12.96\nN <- sum(enrollRates$rate * enrollRates$duration)\nN#> [1] 500\nn0 <- n1 <- N / 2\ngsdmvn:::prob_event.arm(arm0, tmax = analysisTimes) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = analysisTimes) * n1#> [1] 107.3943 246.2834 331.2909\nlog_ahr <- sapply(analysisTimes, function(t_k) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t_k, weight = weight) /\n    gsdmvn:::gs_delta_wlr(\n      arm0, arm1,\n      tmax = t_k,\n      weight = weight,\n      approx = \"generalized schoenfeld\",\n      normalization = TRUE\n    )\n})\nexp(log_ahr)#> [1] 0.7342540 0.6372506 0.6174103\ndelta / sigma2#> [1] 1.577775 1.326384 1.081194\nN * sigma2#> [1]  0.7057784  5.2216800 12.1336979"},{"path":"wlr-boundary.html","id":"wlr-boundary","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8 Group sequential design boundary with weighted logrank test","text":"","code":""},{"path":"wlr-boundary.html","id":"group-sequential-design-boundary-calculation-strategy","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.1 Group sequential design boundary calculation strategy","text":"last chapter, pre-specified boundary derived gsDesign.\nType error may inflated information fraction different different WLR.chapter calculate boundary based error spending approach following Gordon Lan DeMets (1983).spending function implemented gsDesign.gsDesign::sfLDOF() (O’Brien-Fleming bound approximation) functions starting sf gsDesignThe boundary family approach. Pocock (1977), O’Brien Fleming (1979), Wang Tsiatis (1987)ways derive boundary covered:Conditional power. Lachin (2005)","code":""},{"path":"wlr-boundary.html","id":"types-of-error-probability","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.2 Types of error probability","text":"6 different type error probability implemented gsdmvn.\ntraining material, focus test.type = 4.test.type argument gsDesignUpper bound:\n\\(\\alpha_k(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k > b_k \\mid H_0)\\)\n\\(\\alpha_k^{+}(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k > b_k \\mid H_0)\\) (ignore lower bound)\n\\(\\alpha_k(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k > b_k \\mid H_0)\\)\\(\\alpha_k^{+}(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k > b_k \\mid H_0)\\) (ignore lower bound)Lower bound:\n\\(\\beta_k(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k < a_k \\mid H_0)\\) (null)\n\\(\\beta_k(\\delta) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k < a_k \\mid H_1)\\) (alternative)\ntest.type\nUpper bound\nLower bound\n1\n\\(\\alpha_k^{+}(0)\\)\nNone\n2\n\\(\\alpha(0)\\)\n\\(\\beta_k(0)\\)\n3\n\\(\\alpha_k(0)\\)\n\\(\\beta_k(\\delta)\\)\n4\n\\(\\alpha_k^{+}(0)\\)\n\\(\\beta_k(\\delta)\\)\n5\n\\(\\alpha(0)\\)\n\\(\\beta_k(0)\\)\n6\n\\(\\alpha^{+}(0)\\)\n\\(\\beta_k(0)\\)\n\\(\\beta_k(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k < a_k \\mid H_0)\\) (null)\\(\\beta_k(\\delta) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k < a_k \\mid H_1)\\) (alternative)test.type = 1, 2, 5, 6: sample size boundaries can computed single step.test.type = 3 test.type = 4: sample size boundaries set simultaneously using iterative algorithm.section last section focus test.type = 4.","code":""},{"path":"wlr-boundary.html","id":"information-fraction","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.3 Information fraction","text":"design assumption,\ninformation fraction different different weight parameters WLR.continue use example scenario last chapter.","code":""},{"path":"wlr-boundary.html","id":"spending-function-based-on-information-fraction","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.4 Spending function based on information fraction","text":"spending function based information fraction.\nconsidered Lan-DeMets spending function approximate O’Brien-Fleming bound Gordon Lan DeMets (1983). (gsDesign::sfLDOF())., \\(t\\) information fraction formula .\\[f(t; \\alpha)=2-2\\Phi\\left(\\Phi^{-1}\\left(\\frac{1-\\alpha/2}{t^{\\rho/2}}\\right)\\right)\\]","code":""},{"path":"wlr-boundary.html","id":"spending-function-in-gsdesign","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.4.1 Spending function in gsDesign","text":"spending function selected, can calculate lower upper bound \ngroup sequential design.test type 4, lower bound non-binding. set lower bound -Inf calculate probability cross upper bound.first use alpha spending function determine upper bound group sequential designLet \\((a_k, b_k), k=1,\\dots, K\\) denotes lower upper bound.gsDesign logrank test,\nconsidered equal increments information fraction t = 1:3 / 3 .upper bound lower bound based Lan-DeMets spending function\ncan calculated using gsDesign::sfLDOF().Upper bound:Lower bound:considered different WLR tests weight functions: \\(FH(0, 0)\\), \\(FH(0.5, 0.5)\\), \\(FH(0, 0.5)\\), \\(FH(0, 1)\\)test type 4, alpha beta defined .\\(\\alpha_k^{+}(0) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k > b_k \\mid H_0)\\) (ignore lower bound)\n\\(\\alpha_k^{+}(0)\\) calculated null. different sample size.\n\\(\\alpha_k^{+}(0)\\) calculated null. different sample size.\\(\\beta_k(\\delta) = \\text{Pr}(\\cap_{=1}^{=k-1} a_i < Z_i < b_i, Z_k < a_k \\mid H_1)\\) (alternative)\n\\(\\beta_k(\\delta)\\) calculated alternative. depends sample size.\nIteration required find proper sample size boundary.\n\\(\\beta_k(\\delta)\\) calculated alternative. depends sample size.Iteration required find proper sample size boundary.table provide cumulative alpha beta different analysis time WLR test.draw Alpha spending (\\(\\alpha=0.025\\)) function\nbased information fraction 12, 24 36 monthsSimilarly, draw Beta spending (\\(\\beta=0.2\\)) function\nbased information fraction 12, 24 36 months","code":"\nalpha_spend <- gsDesign::sfLDOF(alpha = 0.025, t = 1:3 / 3)$spend\nalpha_spend#> [1] 0.0001035057 0.0060483891 0.0250000000\nbeta_spend <- gsDesign::sfLDOF(alpha = 0.2, t = 1:3 / 3)$spend\nbeta_spend#> [1] 0.02643829 0.11651432 0.20000000\nweight_fun <- list(\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  }\n)\n\n# Weight name\nweight_name <- data.frame(rho = c(0, 0.5, 0, 0), gamma = c(0, 0.5, 0.5, 1))\nweight_name <- with(weight_name, paste0(\"rho = \", rho, \"; gamma = \", gamma))\nanalysisTimes <- c(12, 24, 36)\ngs_spend <- lapply(weight_fun, function(weight) {\n  tmp <- gsdmvn::gs_info_wlr(\n    enrollRates, failRates,\n    analysisTimes = analysisTimes,\n    weight = weight\n  )\n\n  tmp %>% mutate(\n    theta = abs(delta) / sqrt(sigma2),\n    info = info / max(info),\n    info0 = info0 / max(info0),\n    alpha = gsDesign::sfLDOF(alpha = 0.025, t = info0)$spend,\n    beta = gsDesign::sfLDOF(alpha = 0.20, t = info)$spend\n  )\n})\nnames(gs_spend) <- weight_name\nbind_rows(gs_spend, .id = \"weight\") %>%\n  select(weight, Time, alpha, beta) %>%\n  mutate_if(is.numeric, round, digits = 3)#>                    weight Time alpha  beta\n#> 1      rho = 0; gamma = 0   12 0.000 0.025\n#> 2      rho = 0; gamma = 0   24 0.009 0.139\n#> 3      rho = 0; gamma = 0   36 0.025 0.200\n#> 4  rho = 0.5; gamma = 0.5   12 0.000 0.003\n#> 5  rho = 0.5; gamma = 0.5   24 0.006 0.118\n#> 6  rho = 0.5; gamma = 0.5   36 0.025 0.200\n#> 7    rho = 0; gamma = 0.5   12 0.000 0.000\n#> 8    rho = 0; gamma = 0.5   24 0.003 0.088\n#> 9    rho = 0; gamma = 0.5   36 0.025 0.200\n#> 10     rho = 0; gamma = 1   12 0.000 0.000\n#> 11     rho = 0; gamma = 1   24 0.001 0.051\n#> 12     rho = 0; gamma = 1   36 0.025 0.200"},{"path":"wlr-boundary.html","id":"lower-and-upper-bound","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.5 Lower and upper bound","text":"Let’s calculate lower upper bound first interim analysis.First interim analysis upper bound: \\(\\text{Pr}(Z_1 > b_1 \\mid H_0)\\)First interim analysis lower bound \\(\\text{Pr}(Z_1 < b_1 \\mid H_1)\\)lower bound calculated alternative hypothesis depends sample size.figure illustrate lower upper bound different sample sizeLarger sample size larger lower bound (solid line compared dashed line)Iteration required find proper sample size boundary.","code":"\n-qnorm(gs_spend[[1]]$alpha[1])#> [1] 3.790778\nn <- 400\nmean <- gs_spend[[1]]$theta[1] * sqrt(n)\nqnorm(gs_spend[[1]]$beta[1], mean = mean, sd = 1)#> [1] -1.159606\nn <- 500\nmean <- gs_spend[[1]]$theta[1] * sqrt(n)\nqnorm(gs_spend[[1]]$beta[1], mean = mean, sd = 1)#> [1] -1.065469"},{"path":"wlr-boundary.html","id":"sample-size-calculation-logrank-test-based-on-ahr","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.6 Sample size calculation logrank test based on AHR","text":"Sample sizeSimulation results based 10,000 replications.Type errorCompared fixed design","code":"\nx <- gsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  379.   81.4  3.77        0     0.84  0.17  20.0  20.3\n#> 2        2 Upper    24  379.  187.   2.35        0.47  0.71  0.34  45.5  46.6\n#> 3        3 Upper    36  379.  251.   2.01        0.8   0.68  0.38  61.6  62.7\n#> 4        1 Lower    12  379.   81.4 -1.19        0.02  0.84  0.17  20.0  20.3\n#> 5        2 Lower    24  379.  187.   1.13        0.14  0.71  0.34  45.5  46.6\n#> 6        3 Lower    36  379.  251.   2.01        0.2   0.68  0.38  61.6  62.7#>     n  t events  ahr lower upper\n#> 1 379 12  81.23 0.86  0.02  0.00\n#> 2 379 24 186.50 0.72  0.13  0.48\n#> 3 379 36 251.05 0.69  0.19  0.81\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]#> [1] 8.162377e-05 9.415389e-03 2.505418e-02\ngsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds#> # A tibble: 1 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    36  328.   217.  1.96         0.8 0.683 0.381  53.4  54.4"},{"path":"wlr-boundary.html","id":"sample-size-calculation-logrank-test-based-on-wlr-fh0-0","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.7 Sample size calculation logrank test based on WLR \\(FH(0, 0)\\)","text":"Sample sizeSimulation results based 10,000 replications.Type errorCompared fixed design","code":"\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  377.    81   3.79        0     0.84  0.17  20.2  20.3\n#> 2        2 Upper    24  377.   186.  2.36        0.46  0.72  0.33  46.3  46.8\n#> 3        3 Upper    36  377.   250.  2.01        0.8   0.68  0.38  61.8  63.3\n#> 4        1 Lower    12  377.    81  -1.18        0.03  0.84  0.17  20.2  20.3\n#> 5        2 Lower    24  377.   186.  1.15        0.14  0.72  0.33  46.3  46.8\n#> 6        3 Lower    36  377.   250.  2.01        0.2   0.68  0.38  61.8  63.3#>     n  t events  ahr lower upper\n#> 1 378 12  80.98 0.86  0.03  0.00\n#> 2 378 24 186.10 0.72  0.14  0.47\n#> 3 378 36 250.33 0.69  0.20  0.80\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]#> [1] 7.532364e-05 9.164191e-03 2.497474e-02\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds#> # A tibble: 1 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    36  324.   215.  1.96         0.8 0.683 0.381  53.2  54.5"},{"path":"wlr-boundary.html","id":"sample-size-calculation-fh0-1","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.8 Sample size calculation \\(FH(0, 1)\\)","text":"Sample sizeSimulation results based 10,000 replications.Type errorCompared fixed design","code":"\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events      Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl>  <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  287.   61.6 Inf           0     0.73  1.58  0.4   0.41\n#> 2        2 Upper    24  287.  141.    3.28        0.16  0.64  1.33  2.99  3.1 \n#> 3        3 Upper    36  287.  190.    1.96        0.8   0.62  1.08  6.95  7.43\n#> 4        1 Lower    12  287.   61.6  -4.18        0     0.73  1.58  0.4   0.41\n#> 5        2 Lower    24  287.  141.    0.66        0.05  0.64  1.33  2.99  3.1 \n#> 6        3 Lower    36  287.  190.    1.96        0.2   0.62  1.08  6.95  7.43#>      n  t events  ahr lower upper\n#> 10 287 12  61.64 0.77  0.00  0.00\n#> 11 287 24 141.41 0.65  0.05  0.17\n#> 12 287 36 190.20 0.63  0.20  0.80\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]#> [1] 0.0000000000 0.0005191052 0.0251730067\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds#> # A tibble: 1 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    36  264.   175.  1.96         0.8 0.617  1.08  6.41  6.85"},{"path":"wlr-boundary.html","id":"sample-size-calculation-fh0-0.5","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.9 Sample size calculation \\(FH(0, 0.5)\\)","text":"Sample sizeSimulation results based 10,000 replications.Type errorCompared fixed design","code":"\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  288.   61.9  6.18        0     0.78  0.63  2.08  2.09\n#> 2        2 Upper    24  288.  142    2.8         0.3   0.67  0.77  8.86  9.07\n#> 3        3 Upper    36  288.  191.   1.97        0.8   0.64  0.73 15.7  16.4 \n#> 4        1 Lower    12  288.   61.9 -2.43        0     0.78  0.63  2.08  2.09\n#> 5        2 Lower    24  288.  142    0.93        0.09  0.67  0.77  8.86  9.07\n#> 6        3 Lower    36  288.  191.   1.97        0.2   0.64  0.73 15.7  16.4#>     n  t events  ahr lower upper\n#> 7 289 12  62.11 0.81  0.00   0.0\n#> 8 289 24 142.28 0.68  0.09   0.3\n#> 9 289 36 191.44 0.65  0.20   0.8\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]#> [1] 3.205080e-10 2.555246e-03 2.523428e-02\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds#> # A tibble: 1 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    36  261.   173.  1.96         0.8 0.639 0.732  14.2  14.9"},{"path":"wlr-boundary.html","id":"sample-size-calculation-fh0.5-0.5","chapter":"8 Group sequential design boundary with weighted logrank test","heading":"8.10 Sample size calculation \\(FH(0.5, 0.5)\\)","text":"Sample sizeSimulation results based 10,000 replications.Type errorCompared fixed design","code":"\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  304.   65.3  5.05        0     0.79  0.68  1.76  1.77\n#> 2        2 Upper    24  304.  150.   2.51        0.42  0.68  0.93  6.17  6.28\n#> 3        3 Upper    36  304.  201.   1.99        0.8   0.65  0.97  9.16  9.44\n#> 4        1 Lower    12  304.   65.3 -1.8         0     0.79  0.68  1.76  1.77\n#> 5        2 Lower    24  304.  150.   1.12        0.12  0.68  0.93  6.17  6.28\n#> 6        3 Lower    36  304.  201.   1.99        0.2   0.65  0.97  9.16  9.44\nload(\"simulation/simu_gsd_wlr_boundary.Rdata\")\nsimu_res %>%\n  subset(rho == 0.5 & gamma == 0.5) %>%\n  select(-scenario, -rho, -gamma) %>%\n  mutate_if(is.numeric, round, digits = 2)#>     n  t events  ahr lower upper\n#> 4 304 12  65.10 0.81  0.00  0.00\n#> 5 304 24 149.51 0.68  0.12  0.42\n#> 6 304 36 201.23 0.66  0.20  0.80\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]#> [1] 2.209050e-07 6.036759e-03 2.515345e-02\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds#> # A tibble: 1 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    36  269.   178.  1.96         0.8 0.650 0.973  8.11  8.36"},{"path":"maxcombo.html","id":"maxcombo","chapter":"9 MaxCombo test","heading":"9 MaxCombo test","text":"chapter, discuss group sequential design MaxCombo test.","code":""},{"path":"maxcombo.html","id":"maxcombo-test-with-interim-analysis","chapter":"9 MaxCombo test","heading":"9.1 MaxCombo test with interim analysis","text":"\\(G_k = \\max\\{Z_{1k}, Z_{2k}, \\ldots \\}\\)\\(G_k = \\max\\{Z_{1k}, Z_{2k}, \\ldots \\}\\)Test statistics: analysis \\(t_k\\) weight \\(w_i(t)\\)Test statistics: analysis \\(t_k\\) weight \\(w_i(t)\\)\\[ Z_{ik}=\\sqrt{\\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\\int_{0}^{t_k}w_i(t)\\frac{\\overline{Y}_{0}(t)\\overline{Y}_{1}(t)}{\\overline{Y}_{0}(t)+\\overline{Y}_{0}(t)}\\left\\{ \\frac{d\\overline{N}_{1}(t)}{\\overline{Y}_{1}(t)}-\\frac{d\\overline{N}_{0}(t)}{\\overline{Y}_{0}(t)}\\right\\} \\]necessary number tests interim analysis","code":""},{"path":"maxcombo.html","id":"examples","chapter":"9 MaxCombo test","heading":"9.2 Examples","text":"continue use example scenario last chapter.","code":""},{"path":"maxcombo.html","id":"example-1","chapter":"9 MaxCombo test","heading":"9.2.1 Example 1","text":"Using logrank test interim analyses MaxCombo test \\(Z_{1k}: FH(0,0)\\), \\(Z_{2k}: FH(0,0.5)\\), \\(Z_{3k}: FH(0.5,0.5)\\) final analysis.","code":"\nfh_test1 <- rbind(\n  data.frame(\n    rho = 0, gamma = 0, tau = -1,\n    test = 1,\n    Analysis = 1:3,\n    analysisTimes = c(12, 24, 36)\n  ),\n  data.frame(\n    rho = c(0, 0.5), gamma = 0.5, tau = -1,\n    test = 2:3,\n    Analysis = 3, analysisTimes = 36\n  )\n)\nfh_test1#>   rho gamma tau test Analysis analysisTimes\n#> 1 0.0   0.0  -1    1        1            12\n#> 2 0.0   0.0  -1    1        2            24\n#> 3 0.0   0.0  -1    1        3            36\n#> 4 0.0   0.5  -1    2        3            36\n#> 5 0.5   0.5  -1    3        3            36"},{"path":"maxcombo.html","id":"example-2","chapter":"9 MaxCombo test","heading":"9.2.2 Example 2","text":"Using \\(Z_{1k}: FH(0,0)\\) \\(Z_{2k}: FH(0,0.5)\\).","code":"\nfh_test2 <- data.frame(\n  rho = c(0, 0), gamma = c(0, 0.5), tau = -1,\n  analysisTimes = rep(c(12, 24, 36), each = 2),\n  Analysis = rep(1:3, each = 2),\n  test = rep(1:2, 3)\n)\nfh_test2#>   rho gamma tau analysisTimes Analysis test\n#> 1   0   0.0  -1            12        1    1\n#> 2   0   0.5  -1            12        1    2\n#> 3   0   0.0  -1            24        2    1\n#> 4   0   0.5  -1            24        2    2\n#> 5   0   0.0  -1            36        3    1\n#> 6   0   0.5  -1            36        3    2"},{"path":"maxcombo.html","id":"sample-size-calculation-1","chapter":"9 MaxCombo test","heading":"9.3 Sample size calculation","text":"first consider user-defined lower upper bound using gsDesign boundIn general, gsDesign bound directly used MaxCombo test:Multiple test statistics considered interim analysis final analysis.explain way derive bound using spending function next chapter.","code":"\nx <- gsDesign::gsSurv(\n  k = 3, test.type = 4, alpha = 0.025,\n  beta = 0.2, astar = 0, timing = c(1),\n  sfu = sfLDOF, sfupar = c(0), sfl = sfLDOF,\n  sflpar = c(0), lambdaC = c(0.1),\n  hr = 0.6, hr0 = 1, eta = 0.01,\n  gamma = c(10),\n  R = c(12), S = NULL,\n  T = 36, minfup = 24, ratio = 1\n)\nx$upper$bound#> [1] 3.710303 2.511407 1.992970\nx$lower$bound#> [1] -0.2361874  1.1703638  1.9929702"},{"path":"maxcombo.html","id":"example-1-1","chapter":"9 MaxCombo test","heading":"9.3.1 Example 1","text":"Sample size can calculated using gsdmvn::gs_design_combo().Simulation results based 10,000 replications.Compared group sequential design log rank test (based AHR).","code":"\ngsdmvn::gs_design_combo(enrollRates,\n  failRates,\n  fh_test1,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE,\n  upar = x$upper$bound,\n  lpar = x$lower$bound\n) %>%\n  mutate(`Probability_Null (%)` = Probability_Null * 100) %>%\n  select(-Probability_Null) %>%\n  mutate_if(is.numeric, round, digits = 2)#>   Analysis Bound Time      N Events     Z Probability Probability_Null (%)\n#> 1        1 Upper   12 444.81  95.54  3.71        0.00                 0.01\n#> 3        2 Upper   24 444.81 219.10  2.51        0.47                 0.61\n#> 5        3 Upper   36 444.81 294.72  1.99        0.80                 3.26\n#> 2        1 Lower   12 444.81  95.54 -0.24        0.14                   NA\n#> 4        2 Lower   24 444.81 219.10  1.17        0.19                   NA\n#> 6        3 Lower   36 444.81 294.72  1.99        0.20                   NA#>     n      d analysisTimes lower upper\n#> 1 445  95.46            12  0.14  0.00\n#> 2 445 219.07            24  0.19  0.47\n#> 3 445 294.71            36  0.20  0.80\ngsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  469.   101.  3.71        0     0.84  0.17  24.7  25.2\n#> 2        2 Upper    24  469.   231.  2.51        0.49  0.71  0.34  56.3  57.7\n#> 3        3 Upper    36  469.   311.  1.99        0.8   0.68  0.38  76.3  77.6\n#> 4        1 Lower    12  469.   101. -0.24        0.13  0.84  0.17  24.7  25.2\n#> 5        2 Lower    24  469.   231.  1.17        0.17  0.71  0.34  56.3  57.7\n#> 6        3 Lower    36  469.   311.  1.99        0.2   0.68  0.38  76.3  77.6"},{"path":"maxcombo.html","id":"example-2-1","chapter":"9 MaxCombo test","heading":"9.3.2 Example 2","text":"Simulation results based 10,000 replications.Compared group sequential design FH(0, 0.5)Compared group sequential design FH(0.5, 0.5)","code":"\ngs_design_combo(enrollRates,\n  failRates,\n  fh_test2,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE,\n  upar = x$upper$bound,\n  lpar = x$lower$bound\n) %>%\n  mutate(`Probability_Null (%)` = Probability_Null * 100) %>%\n  select(-Probability_Null) %>%\n  mutate_if(is.numeric, round, digits = 2)#>   Analysis Bound Time      N Events     Z Probability Probability_Null (%)\n#> 1        1 Upper   12 348.21  74.79  3.71        0.00                 0.02\n#> 3        2 Upper   24 348.21 171.52  2.51        0.49                 0.84\n#> 5        3 Upper   36 348.21 230.72  1.99        0.80                 3.28\n#> 2        1 Lower   12 348.21  74.79 -0.24        0.10                   NA\n#> 4        2 Lower   24 348.21 171.52  1.17        0.15                   NA\n#> 6        3 Lower   36 348.21 230.72  1.99        0.20                   NA#>     n      d analysisTimes lower upper\n#> 4 349  74.85            12  0.10  0.00\n#> 5 349 171.79            24  0.16  0.48\n#> 6 349 231.20            36  0.20  0.80\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  368.   79.0  3.71        0     0.78  0.63  2.65  2.66\n#> 2        2 Upper    24  368.  181.   2.51        0.5   0.67  0.77 11.3  11.6 \n#> 3        3 Upper    36  368.  244.   1.99        0.8   0.64  0.73 20    20.9 \n#> 4        1 Lower    12  368.   79.0 -0.24        0.1   0.78  0.63  2.65  2.66\n#> 5        2 Lower    24  368.  181.   1.17        0.16  0.67  0.77 11.3  11.6 \n#> 6        3 Lower    36  368.  244.   1.99        0.2   0.64  0.73 20    20.9\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  372.   79.8  3.71        0     0.79  0.68  2.15  2.16\n#> 2        2 Upper    24  372.  183.   2.51        0.5   0.68  0.93  7.55  7.68\n#> 3        3 Upper    36  372.  246.   1.99        0.8   0.65  0.97 11.2  11.5 \n#> 4        1 Lower    12  372.   79.8 -0.24        0.11  0.79  0.68  2.15  2.16\n#> 5        2 Lower    24  372.  183.   1.17        0.16  0.68  0.93  7.55  7.68\n#> 6        3 Lower    36  372.  246.   1.99        0.2   0.65  0.97 11.2  11.5"},{"path":"maxcombo.html","id":"outline-of-technical-details","chapter":"9 MaxCombo test","heading":"9.4 Outline of technical details","text":"describe details calculating\nsample size events required WLR fixed design.can skipped first read training material.","code":""},{"path":"maxcombo.html","id":"with-a-pre-defined-upper-and-lower-bound","chapter":"9 MaxCombo test","heading":"9.4.1 With a pre-defined upper and lower bound","text":"Derive correlations test analysis time pointDerive effect sizePower sample size calculation","code":""},{"path":"maxcombo.html","id":"maxcombo-sequential-test-correlation-matrix","chapter":"9 MaxCombo test","heading":"9.5 MaxCombo sequential test correlation matrix","text":"Reference: Section 3.1 Wang et al (2019)\\(Z_{ij}\\): \\(\\)-th test \\(j\\)-th analysis.","code":""},{"path":"maxcombo.html","id":"between-test-correlation","chapter":"9 MaxCombo test","heading":"9.6 Between test correlation","text":"Within interim analysis, correlation tests.\nRecall discussion fixed design\nRecall discussion fixed design\\[\\hbox{Cov}(Z_{1k}, Z_{2k}) = \\hbox{Var}(Z_k(\\frac{\\rho_1 + \\rho_{2}}{2}, \\frac{\\gamma_1 + \\gamma_2}{2}, \\tau))\\]Even one test used interim analysis, correlation tests needed interim analysis.","code":""},{"path":"maxcombo.html","id":"example-1-2","chapter":"9 MaxCombo test","heading":"9.6.1 Example 1","text":"","code":"\nu_fh_test1 <- unique(fh_test1[, c(\"test\", \"rho\", \"gamma\", \"tau\")])\nu_fh_test1#>   test rho gamma tau\n#> 1    1 0.0   0.0  -1\n#> 4    2 0.0   0.5  -1\n#> 5    3 0.5   0.5  -1\ncorr_test1 <- with(\n  u_fh_test1,\n  lapply(analysisTimes, function(tmax) {\n    cov2cor(gsdmvn:::gs_sigma2_combo(arm0, arm1,\n      tmax = tmax,\n      rho = rho, gamma = gamma, tau = tau\n    ))\n  })\n)\nnames(corr_test1) <- analysisTimes\ncorr_test1#> $`12`\n#>           [,1]      [,2]      [,3]\n#> [1,] 1.0000000 0.9277654 0.9415781\n#> [2,] 0.9277654 1.0000000 0.9986153\n#> [3,] 0.9415781 0.9986153 1.0000000\n#> \n#> $`24`\n#>           [,1]      [,2]      [,3]\n#> [1,] 1.0000000 0.9407774 0.9612878\n#> [2,] 0.9407774 1.0000000 0.9955313\n#> [3,] 0.9612878 0.9955313 1.0000000\n#> \n#> $`36`\n#>           [,1]      [,2]      [,3]\n#> [1,] 1.0000000 0.9417454 0.9690488\n#> [2,] 0.9417454 1.0000000 0.9894930\n#> [3,] 0.9690488 0.9894930 1.0000000"},{"path":"maxcombo.html","id":"example-2-2","chapter":"9 MaxCombo test","heading":"9.6.2 Example 2","text":"","code":"\nu_fh_test2 <- unique(fh_test2[, c(\"test\", \"rho\", \"gamma\", \"tau\")])\nu_fh_test2#>   test rho gamma tau\n#> 1    1   0   0.0  -1\n#> 2    2   0   0.5  -1\ncorr_test2 <- with(\n  unique(fh_test2[, c(\"rho\", \"gamma\", \"tau\")]),\n  lapply(analysisTimes, function(tmax) {\n    cov2cor(gsdmvn:::gs_sigma2_combo(arm0, arm1,\n      tmax = tmax,\n      rho = rho, gamma = gamma, tau = tau\n    ))\n  })\n)\nnames(corr_test2) <- analysisTimes\ncorr_test2#> $`12`\n#>           [,1]      [,2]\n#> [1,] 1.0000000 0.9277654\n#> [2,] 0.9277654 1.0000000\n#> \n#> $`24`\n#>           [,1]      [,2]\n#> [1,] 1.0000000 0.9407774\n#> [2,] 0.9407774 1.0000000\n#> \n#> $`36`\n#>           [,1]      [,2]\n#> [1,] 1.0000000 0.9417454\n#> [2,] 0.9417454 1.0000000"},{"path":"maxcombo.html","id":"between-analysis-correlation","chapter":"9 MaxCombo test","heading":"9.7 Between analysis correlation","text":"Within test, correlation among interim analysis.\nrecall discussion group sequential design weighted log rank test.\nrecall discussion group sequential design weighted log rank test.","code":""},{"path":"maxcombo.html","id":"example-1-3","chapter":"9 MaxCombo test","heading":"9.7.1 Example 1","text":"","code":"\ninfo1 <- gsdmvn:::gs_info_combo(enrollRates, failRates, ratio,\n  analysisTimes = analysisTimes,\n  rho = u_fh_test1$rho,\n  gamma = u_fh_test1$gamma\n)\ninfo1 %>% round(digits = 2)#>   test Analysis Time   N Events  AHR delta sigma2 theta  info info0\n#> 1    1        1   12 500 107.39 0.84 -0.01   0.05  0.17 26.84 26.90\n#> 2    1        2   24 500 246.28 0.72 -0.04   0.12  0.33 61.35 62.09\n#> 3    1        3   36 500 331.29 0.68 -0.06   0.16  0.38 81.92 83.94\n#> 4    2        1   12 500 107.39 0.78  0.00   0.01  0.63  3.60  3.62\n#> 5    2        2   24 500 246.28 0.67 -0.02   0.03  0.77 15.37 15.74\n#> 6    2        3   36 500 331.29 0.64 -0.04   0.05  0.73 27.21 28.48\n#> 7    3        1   12 500 107.39 0.79  0.00   0.01  0.68  2.90  2.91\n#> 8    3        2   24 500 246.28 0.68 -0.02   0.02  0.93 10.15 10.33\n#> 9    3        3   36 500 331.29 0.65 -0.03   0.03  0.97 15.07 15.53\ninfo <- info1\ninfo_split <- split(info, info$test)\ncorr_time1 <- lapply(info_split, function(x) {\n  corr <- with(x, outer(sqrt(info), sqrt(info), function(x, y) pmin(x, y) / pmax(x, y)))\n  rownames(corr) <- analysisTimes\n  colnames(corr) <- analysisTimes\n  corr\n})\ncorr_time1#> $`1`\n#>           12        24        36\n#> 12 1.0000000 0.6614295 0.5724133\n#> 24 0.6614295 1.0000000 0.8654185\n#> 36 0.5724133 0.8654185 1.0000000\n#> \n#> $`2`\n#>           12        24        36\n#> 12 1.0000000 0.4842835 0.3640177\n#> 24 0.4842835 1.0000000 0.7516625\n#> 36 0.3640177 0.7516625 1.0000000\n#> \n#> $`3`\n#>           12        24        36\n#> 12 1.0000000 0.5341938 0.4385035\n#> 24 0.5341938 1.0000000 0.8208697\n#> 36 0.4385035 0.8208697 1.0000000"},{"path":"maxcombo.html","id":"example-2-3","chapter":"9 MaxCombo test","heading":"9.7.2 Example 2","text":"","code":"\ninfo2 <- gsdmvn:::gs_info_combo(enrollRates, failRates, ratio,\n  analysisTimes = analysisTimes,\n  rho = u_fh_test2$rho,\n  gamma = u_fh_test2$gamma\n)\ninfo2 %>% round(digits = 2)#>   test Analysis Time   N Events  AHR delta sigma2 theta  info info0\n#> 1    1        1   12 500 107.39 0.84 -0.01   0.05  0.17 26.84 26.90\n#> 2    1        2   24 500 246.28 0.72 -0.04   0.12  0.33 61.35 62.09\n#> 3    1        3   36 500 331.29 0.68 -0.06   0.16  0.38 81.92 83.94\n#> 4    2        1   12 500 107.39 0.78  0.00   0.01  0.63  3.60  3.62\n#> 5    2        2   24 500 246.28 0.67 -0.02   0.03  0.77 15.37 15.74\n#> 6    2        3   36 500 331.29 0.64 -0.04   0.05  0.73 27.21 28.48\ninfo <- info2\ninfo_split <- split(info, info$test)\ncorr_time2 <- lapply(info_split, function(x) {\n  corr <- with(x, outer(sqrt(info), sqrt(info), function(x, y) pmin(x, y) / pmax(x, y)))\n  rownames(corr) <- analysisTimes\n  colnames(corr) <- analysisTimes\n  corr\n})\ncorr_time2#> $`1`\n#>           12        24        36\n#> 12 1.0000000 0.6614295 0.5724133\n#> 24 0.6614295 1.0000000 0.8654185\n#> 36 0.5724133 0.8654185 1.0000000\n#> \n#> $`2`\n#>           12        24        36\n#> 12 1.0000000 0.4842835 0.3640177\n#> 24 0.4842835 1.0000000 0.7516625\n#> 36 0.3640177 0.7516625 1.0000000"},{"path":"maxcombo.html","id":"correlation-matrix-for-all-tests-across-analysis","chapter":"9 MaxCombo test","heading":"9.8 Correlation matrix for all tests across analysis","text":"Reference: Section 3.1 Wang et al (2019)\\(Z_{ij}\\): \\(\\)-th test \\(j\\)-th analysis.\\[ \\hbox{Cor}(Z_{11}, Z_{22}) = \\hbox{Cor}(Z_{22}, Z_{11}) \\approx \\hbox{Cor}(Z_{11}, Z_{21}) \\hbox{Cor}(Z_{21}Z_{22}) \\]implies\\[ \\hbox{Cor}(Z_{11}, Z_{22}) = \\frac{\\hbox{Cov}(Z_{11}, Z_{21})} {\\sqrt{\\hbox{Var}(Z_{11})\\hbox{Var}(Z_{22})}}\\]","code":""},{"path":"maxcombo.html","id":"example-1-4","chapter":"9 MaxCombo test","heading":"9.8.1 Example 1","text":"Compared simulation results based 10,000 replications.","code":"\ncorr_test <- corr_test1\ncorr_time <- corr_time1\ninfo <- info1\n# Overall Correlation\ncorr_combo <- diag(1, nrow = nrow(info))\nfor (i in 1:nrow(info)) {\n  for (j in 1:nrow(info)) {\n    t1 <- as.numeric(info$Analysis[i])\n    t2 <- as.numeric(info$Analysis[j])\n    if (t1 <= t2) {\n      test1 <- as.numeric(info$test[i])\n      test2 <- as.numeric(info$test[j])\n      corr_combo[i, j] <- corr_test[[t1]][test1, test2] * corr_time[[test2]][t1, t2]\n      corr_combo[j, i] <- corr_combo[i, j]\n    }\n  }\n}\ncorr_combo1 <- corr_combo\ncorr_combo1 %>% round(2)#>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n#>  [1,] 1.00 0.66 0.57 0.93 0.45 0.34 0.94 0.50 0.41\n#>  [2,] 0.66 1.00 0.87 0.61 0.94 0.71 0.62 0.96 0.79\n#>  [3,] 0.57 0.87 1.00 0.53 0.81 0.94 0.54 0.83 0.97\n#>  [4,] 0.93 0.61 0.53 1.00 0.48 0.36 1.00 0.53 0.44\n#>  [5,] 0.45 0.94 0.81 0.48 1.00 0.75 0.48 1.00 0.82\n#>  [6,] 0.34 0.71 0.94 0.36 0.75 1.00 0.36 0.75 0.99\n#>  [7,] 0.94 0.62 0.54 1.00 0.48 0.36 1.00 0.53 0.44\n#>  [8,] 0.50 0.96 0.83 0.53 1.00 0.75 0.53 1.00 0.82\n#>  [9,] 0.41 0.79 0.97 0.44 0.82 0.99 0.44 0.82 1.00"},{"path":"maxcombo.html","id":"example-2-4","chapter":"9 MaxCombo test","heading":"9.8.2 Example 2","text":"Compared simulation results based 10,000 replications.","code":"\ncorr_test <- corr_test2\ncorr_time <- corr_time2\ninfo <- info2\n# Overall Correlation\ncorr_combo <- diag(1, nrow = nrow(info))\nfor (i in 1:nrow(info)) {\n  for (j in 1:nrow(info)) {\n    t1 <- as.numeric(info$Analysis[i])\n    t2 <- as.numeric(info$Analysis[j])\n    if (t1 <= t2) {\n      test1 <- as.numeric(info$test[i])\n      test2 <- as.numeric(info$test[j])\n      corr_combo[i, j] <- corr_test[[t1]][test1, test2] * corr_time[[test2]][t1, t2]\n      corr_combo[j, i] <- corr_combo[i, j]\n    }\n  }\n}\ncorr_combo2 <- corr_combo\ncorr_combo2 %>% round(2)#>      [,1] [,2] [,3] [,4] [,5] [,6]\n#> [1,] 1.00 0.66 0.57 0.93 0.45 0.34\n#> [2,] 0.66 1.00 0.87 0.61 0.94 0.71\n#> [3,] 0.57 0.87 1.00 0.53 0.81 0.94\n#> [4,] 0.93 0.61 0.53 1.00 0.48 0.36\n#> [5,] 0.45 0.94 0.81 0.48 1.00 0.75\n#> [6,] 0.34 0.71 0.94 0.36 0.75 1.00"},{"path":"maxcombo.html","id":"power-1","chapter":"9 MaxCombo test","heading":"9.9 Power","text":"First interim analysis\\[\\text{Pr}( G_1 > b_1 \\mid H_1) = 1 - \\text{Pr}(G_1 < b_1 \\mid H_1) \\]Second interim analysis\\[\\text{Pr}( a_1 < G_1 < b_1, G_2 > b_2  \\mid H_1) = \\text{Pr}(G_1 < a_1, G_2 < b_2 \\mid H_1)\\]\n\\[- \\text{Pr}(G_1 < b_1, G_2 < b_2 \\mid H_1) - \\text{Pr}(G_1 < a_1, G_2 < \\infty \\mid H_1)\\]\n\\[ + \\text{Pr}(G_1 < b_1, G_2 < \\infty \\mid H_1)\\]General interim analysisGeneral interim analysisDenote \\(l = (a_1, a_{k-1}, b_k)\\) \\(u = (b_1, b_{k-1}, \\infty)\\)Denote \\(l = (a_1, a_{k-1}, b_k)\\) \\(u = (b_1, b_{k-1}, \\infty)\\)\\(\\xi = \\{\\xi_j; \\; j=1,\\dots, 2^k\\}\\): \\(2^k\\) possible combination elements \\(l\\) \\(u\\)\\(\\xi = \\{\\xi_j; \\; j=1,\\dots, 2^k\\}\\): \\(2^k\\) possible combination elements \\(l\\) \\(u\\)\\[\\text{Pr}( \\cap_{=1}^{k-1} a_i < G_i < b_i, G_k > b_k \\mid H_1)  = \\sum_{j=1}^{2^k} (-1)^{\\sum_{=1}^{k} (\\xi_i = l_i)} \\text{Pr}(\\cap_{=1}^k G_k < \\xi_i) \\]computation can simplified \\(a_i = - \\infty\\) \\(k\\)-th interim analysis.computation can simplified \\(a_i = - \\infty\\) \\(k\\)-th interim analysis.computation can simplified \\(G_i\\) contains one test \\(k\\)-th interim analysisThe computation can simplified \\(G_i\\) contains one test \\(k\\)-th interim analysis","code":""},{"path":"maxcombo.html","id":"example-1-5","chapter":"9 MaxCombo test","heading":"9.9.1 Example 1","text":"","code":"\nn <- 500\n# Restricted to actual analysis\ninfo_fh <- merge(info1, fh_test1, all = TRUE)\ncorr_fh <- corr_combo1[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\ninfo_fh <- subset(info_fh, !is.na(gamma))\ntheta_fh <- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\npower <- gsdmvn:::gs_prob_combo(\n  upper_bound = x$upper$bound,\n  lower_bound = x$lower$bound,\n  fh_test = fh_test1,\n  analysis = info_fh$Analysis,\n  theta = theta_fh * sqrt(n),\n  corr = corr_fh\n)\npower#>   Bound Probability\n#> 1 Upper 0.002411474\n#> 2 Upper 0.525796329\n#> 3 Upper 0.828236945\n#> 4 Lower 0.129688918\n#> 5 Lower 0.162696908\n#> 6 Lower 0.171765915"},{"path":"maxcombo.html","id":"example-2-5","chapter":"9 MaxCombo test","heading":"9.9.2 Example 2","text":"","code":"\nn <- 500\n# Restricted to actual analysis\ninfo_fh <- merge(info2, fh_test2, all = TRUE)\ncorr_fh <- corr_combo2[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\ninfo_fh <- subset(info_fh, !is.na(gamma))\ntheta_fh <- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\npower <- gsdmvn:::gs_prob_combo(\n  upper_bound = x$upper$bound,\n  lower_bound = x$lower$bound,\n  fh_test = fh_test2,\n  analysis = info_fh$Analysis,\n  theta = theta_fh * sqrt(n),\n  corr = corr_fh\n)\npower#>   Bound Probability\n#> 1 Upper 0.006333066\n#> 2 Upper 0.674440184\n#> 3 Upper 0.896148614\n#> 4 Lower 0.068860093\n#> 5 Lower 0.089484249\n#> 6 Lower 0.103856466"},{"path":"maxcombo.html","id":"sample-size-1","chapter":"9 MaxCombo test","heading":"9.10 Sample size","text":"Root finding based target power","code":""},{"path":"maxcombo.html","id":"example-1-6","chapter":"9 MaxCombo test","heading":"9.10.1 Example 1","text":"","code":"\nfun <- function(n) {\n  info_fh <- merge(info1, fh_test1, all = TRUE)\n  corr_fh <- corr_combo1[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\n  info_fh <- subset(info_fh, !is.na(gamma))\n  theta_fh <- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\n  power <- gsdmvn:::gs_prob_combo(\n    upper_bound = x$upper$bound,\n    lower_bound = x$lower$bound,\n    fh_test = fh_test1,\n    analysis = info_fh$Analysis,\n    theta = theta_fh * sqrt(n),\n    corr = corr_fh\n  )\n  1 - beta - max(subset(power, Bound == \"Upper\")$Probability)\n}\n\nuniroot(fun, c(1, 1000), extendInt = \"yes\")$root#> [1] 444.81"},{"path":"maxcombo.html","id":"example-2-6","chapter":"9 MaxCombo test","heading":"9.10.2 Example 2","text":"","code":"\nfun <- function(n) {\n  info_fh <- merge(info2, fh_test2, all = TRUE)\n  corr_fh <- corr_combo2[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\n  info_fh <- subset(info_fh, !is.na(gamma))\n  theta_fh <- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\n  power <- gsdmvn:::gs_prob_combo(\n    upper_bound = x$upper$bound,\n    lower_bound = x$lower$bound,\n    fh_test = fh_test2,\n    analysis = info_fh$Analysis,\n    theta = theta_fh * sqrt(n),\n    corr = corr_fh\n  )\n\n  1 - beta - max(subset(power, Bound == \"Upper\")$Probability)\n}\n\nuniroot(fun, c(1, 1000), extendInt = \"yes\")$root#> [1] 348.2112"},{"path":"maxcombo-boundary.html","id":"maxcombo-boundary","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10 Group sequential design boundary with MaxCombo test","text":"chapter, calculate boundary based error spending approach following Gordon Lan DeMets (1983).continue use example scenario last chapter.Example 1 previous chapter","code":"\nenrollRates <- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500 / 12)\n\nfailRates <- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15, # Median survival is 15 months\n  hr = c(1, .6),\n  dropoutRate = 0.001\n)\n# Randomization Ratio is 1:1\nratio <- 1\n\n# Type I error (one-sided)\nalpha <- 0.025\n\n# Power (1 - beta)\nbeta <- 0.2\npower <- 1 - beta\n\n# Interim analysis time\nanalysisTimes <- c(12, 24, 36)\nfh_test <- rbind(\n  data.frame(\n    rho = 0, gamma = 0, tau = -1,\n    test = 1,\n    Analysis = 1:3,\n    analysisTimes = c(12, 24, 36)\n  ),\n  data.frame(\n    rho = c(0, 0.5), gamma = 0.5, tau = -1,\n    test = 2:3,\n    Analysis = 3, analysisTimes = 36\n  )\n)\nfh_test#>   rho gamma tau test Analysis analysisTimes\n#> 1 0.0   0.0  -1    1        1            12\n#> 2 0.0   0.0  -1    1        2            24\n#> 3 0.0   0.0  -1    1        3            36\n#> 4 0.0   0.5  -1    2        3            36\n#> 5 0.5   0.5  -1    3        3            36"},{"path":"maxcombo-boundary.html","id":"sample-size-calculation-based-on-spending-function","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.1 Sample size calculation based on spending function","text":"Implementation gsdmvnSimulation results based 10,000 replications.Compared \\(FH(0, 0)\\) using boundary based test.type = 4Compared \\(FH(0.5, 0.5)\\) using boundary based test.type = 4Compared \\(FH(0, 0.5)\\) using boundary based test.type = 4","code":"\ngs_design_combo(enrollRates,\n  failRates,\n  fh_test,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE, # test.type = 4 non-binding futility bound\n  upper = gs_spending_combo,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025), # alpha spending\n  lower = gs_spending_combo,\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2), # beta spending\n) %>%\n  mutate(`Probability_Null (%)` = Probability_Null * 100) %>%\n  select(-Probability_Null) %>%\n  mutate_if(is.numeric, round, digits = 2)#>   Analysis Bound Time      N Events     Z Probability Probability_Null (%)\n#> 1        1 Upper   12 301.22  64.70  6.18        0.00                 0.00\n#> 3        2 Upper   24 301.22 148.37  2.80        0.22                 0.26\n#> 5        3 Upper   36 301.22 199.58  2.10        0.80                 2.50\n#> 2        1 Lower   12 301.22  64.70 -2.72        0.00                   NA\n#> 4        2 Lower   24 301.22 148.37  0.65        0.08                   NA\n#> 6        3 Lower   36 301.22 199.58  2.10        0.20                   NA#>     n      d analysisTimes lower upper\n#> 7 302  64.78            12  0.00  0.00\n#> 8 302 148.75            24  0.08  0.22\n#> 9 302 200.18            36  0.20  0.80\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  377.    81   3.79        0     0.84  0.17  20.2  20.3\n#> 2        2 Upper    24  377.   186.  2.36        0.46  0.72  0.33  46.3  46.8\n#> 3        3 Upper    36  377.   250.  2.01        0.8   0.68  0.38  61.8  63.3\n#> 4        1 Lower    12  377.    81  -1.18        0.03  0.84  0.17  20.2  20.3\n#> 5        2 Lower    24  377.   186.  1.15        0.14  0.72  0.33  46.3  46.8\n#> 6        3 Lower    36  377.   250.  2.01        0.2   0.68  0.38  61.8  63.3\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  304.   65.3  5.05        0     0.79  0.68  1.76  1.77\n#> 2        2 Upper    24  304.  150.   2.51        0.42  0.68  0.93  6.17  6.28\n#> 3        3 Upper    36  304.  201.   1.99        0.8   0.65  0.97  9.16  9.44\n#> 4        1 Lower    12  304.   65.3 -1.8         0     0.79  0.68  1.76  1.77\n#> 5        2 Lower    24  304.  150.   1.12        0.12  0.68  0.93  6.17  6.28\n#> 6        3 Lower    36  304.  201.   1.99        0.2   0.65  0.97  9.16  9.44\nx <- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %>%\n  mutate_if(is.numeric, round, digits = 2)\nx#> # A tibble: 6 × 11\n#>   Analysis Bound  Time     N Events     Z Probability   AHR theta  info info0\n#>      <dbl> <chr> <dbl> <dbl>  <dbl> <dbl>       <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1        1 Upper    12  288.   61.9  6.18        0     0.78  0.63  2.08  2.09\n#> 2        2 Upper    24  288.  142    2.8         0.3   0.67  0.77  8.86  9.07\n#> 3        3 Upper    36  288.  191.   1.97        0.8   0.64  0.73 15.7  16.4 \n#> 4        1 Lower    12  288.   61.9 -2.43        0     0.78  0.63  2.08  2.09\n#> 5        2 Lower    24  288.  142    0.93        0.09  0.67  0.77  8.86  9.07\n#> 6        3 Lower    36  288.  191.   1.97        0.2   0.64  0.73 15.7  16.4"},{"path":"maxcombo-boundary.html","id":"information-fraction-1","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.2 Information fraction","text":"design assumption, information fraction different different weight parameters WLR.two potential strategies calculate information fraction:Option 1: Use minimal information fraction candidate test (implemented gsdmvn).Option 2: Use weighted average information fraction.","code":"\nutility <- gsdmvn:::gs_utility_combo(enrollRates, failRates, fh_test = fh_test, ratio = 1)\nutility$info_all %>% mutate_if(is.numeric, round, digits = 3)#>   test Analysis Time   N  Events   AHR  delta sigma2 theta   info  info0\n#> 1    1        1   12 500 107.394 0.842 -0.009  0.054 0.172 26.841 26.899\n#> 2    1        2   24 500 246.283 0.716 -0.041  0.123 0.333 61.352 62.087\n#> 3    1        3   36 500 331.291 0.683 -0.062  0.164 0.381 81.918 83.944\n#> 4    2        1   12 500 107.394 0.781 -0.005  0.007 0.626  3.605  3.624\n#> 5    2        2   24 500 246.283 0.666 -0.024  0.031 0.765 15.371 15.737\n#> 6    2        3   36 500 331.291 0.639 -0.040  0.054 0.732 27.205 28.484\n#> 7    3        1   12 500 107.394 0.790 -0.004  0.006 0.676  2.898  2.910\n#> 8    3        2   24 500 246.283 0.675 -0.019  0.020 0.927 10.155 10.335\n#> 9    3        3   36 500 331.291 0.650 -0.029  0.030 0.973 15.070 15.526\ninfo_frac <- tapply(utility$info_all$info0, utility$info_all$test, function(x) x / max(x))\ninfo_frac#> $`1`\n#> [1] 0.3204395 0.7396239 1.0000000\n#> \n#> $`2`\n#> [1] 0.1272245 0.5525079 1.0000000\n#> \n#> $`3`\n#> [1] 0.1874599 0.6656420 1.0000000\nmin_info_frac <- apply(do.call(rbind, info_frac), 2, min)\nmin_info_frac#> [1] 0.1272245 0.5525079 1.0000000"},{"path":"maxcombo-boundary.html","id":"spending-function","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.3 Spending function","text":"gsDesign bound can directly used MaxCombo test:\nMultiple test statistics considered interim analysis final analysis.\ngsDesign bound can directly used MaxCombo test:Multiple test statistics considered interim analysis final analysis.Design example:\n\\(\\alpha = 0.025\\)\n\\(\\beta = 0.2\\)\n\\(K=3\\) total analysis.\ntest.type=4: two-sided, asymmetric, beta-spending non-binding lower bound.\nDesign example:\\(\\alpha = 0.025\\)\\(\\beta = 0.2\\)\\(K=3\\) total analysis.test.type=4: two-sided, asymmetric, beta-spending non-binding lower bound.Lan-DeMets spending function approximate O’Brien-Fleming bound Gordon Lan DeMets (1983). (gsDesign::sfLDOF)Lan-DeMets spending function approximate O’Brien-Fleming bound Gordon Lan DeMets (1983). (gsDesign::sfLDOF)\\(t\\) information fraction formula .\\(t\\) information fraction formula .\\[f(t; \\alpha)=2-2\\Phi\\left(\\Phi^{-1}\\left(\\frac{1-\\alpha/2}{t^{\\rho/2}}\\right)\\right)\\]spending functions discussed gsDesign technical manual\nimplemented gsDesign::sf*() functions.spending functions discussed gsDesign technical manual\nimplemented gsDesign::sf*() functions.Upper bound:\nNon-binding lower bound: lower bound -Inf.\nUpper bound:Non-binding lower bound: lower bound -Inf.Lower bound:","code":"\nalpha_spend <- gsDesign::sfLDOF(alpha = 0.025, t = min_info_frac)$spend\nalpha_spend#> [1] 3.300225e-10 2.566068e-03 2.500000e-02\nbeta_spend <- gsDesign::sfLDOF(alpha = 0.2, t = min_info_frac)$spend\nbeta_spend#> [1] 0.0003269604 0.0846866352 0.2000000000"},{"path":"maxcombo-boundary.html","id":"technical-details-2","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.4 Technical details","text":"section describes details calculating sample size\nevents required WLR group sequential design.\ncan skipped first read training material.","code":""},{"path":"maxcombo-boundary.html","id":"upper-bound-in-group-sequential-design","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.5 Upper bound in group sequential design","text":"","code":""},{"path":"maxcombo-boundary.html","id":"one-test-in-each-interim-analysis","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.5.1 One test in each interim analysis","text":"First interim analysis\\[\\alpha_1 = \\text{Pr}(Z_1 > b_1 \\mid H_0)\\]General formula (non-binding futility bound)\\[\\alpha_k = \\text{Pr}(\\cap_{=1}^{=k-1} Z_i < b_i, Z_k > b_k \\mid H_0)\\]","code":"\nqnorm(1 - alpha_spend[1])#> [1] 6.17538"},{"path":"maxcombo-boundary.html","id":"maxcombo-upper-bound","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.5.2 MaxCombo upper bound","text":"First interim analysis upper bound\\[\\alpha_1 = \\text{Pr}(G_1 > b_1 \\mid H_0) = 1 - \\text{Pr}(\\cap_i Z_{i1} < b_1 \\mid H_0)\\]General formula (non-binding futility bound)\\[\\alpha_k = \\text{Pr}(\\cap_{=1}^{=k-1} G_i < b_i, G_k > b_k \\mid H_0)\\]\n\\[ = \\text{Pr}(\\cap_{=1}^{=k-1} G_i < b_i \\mid H_0) - \\text{Pr}(\\cap_{=1}^{=k} G_i < b_i \\mid H_0)\\]gsdmvn implementation MaxCombo testCompared upper bound calculated gsDesign","code":"\ngsdmvn:::gs_bound(alpha_spend, beta_spend,\n  analysis = utility$info$Analysis, # Analysis indicator\n  theta = rep(0, nrow(fh_test)), # Under the null hypothesis\n  corr = utility$corr # Correlation\n)$upper#> [1] 6.175397 2.798651 2.097039\nx <- gsDesign::gsSurv(\n  k = 3, test.type = 4, alpha = 0.025,\n  beta = 0.2, astar = 0, timing = c(1),\n  sfu = sfLDOF, sfupar = c(0), sfl = sfLDOF,\n  sflpar = c(0), lambdaC = c(0.1),\n  hr = 0.6, hr0 = 1, eta = 0.01,\n  gamma = c(10),\n  R = c(12), S = NULL,\n  T = 36, minfup = 24, ratio = 1\n)\nx$upper$bound#> [1] 3.710303 2.511407 1.992970"},{"path":"maxcombo-boundary.html","id":"lower-bound-in-group-sequential-design","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.6 Lower bound in group sequential design","text":"","code":""},{"path":"maxcombo-boundary.html","id":"one-test-in-each-interim-analysis-1","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.6.1 One test in each interim analysis","text":"First interim analysis\\[\\beta_1 = \\text{Pr}(Z_1 < a_1 \\mid H_1)\\]General formula\n\\[\\beta_k = \\text{Pr}(\\cap_{=1}^{=k-1}  a_i < Z_i < b_i, Z_k < a_k \\mid H_1)\\]","code":"\nn <- 400\nqnorm(beta_spend[1], mean = utility$theta[1] * sqrt(n))#> [1] -2.610665\nn <- 500\nqnorm(beta_spend[1], mean = utility$theta[1] * sqrt(n))#> [1] -2.516528"},{"path":"maxcombo-boundary.html","id":"maxcombo-lower-bound","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.6.2 MaxCombo lower bound","text":"First interim analysis upper bound\\[\\beta_1 = \\text{Pr}(G_1 < a_1 \\mid H_1) = \\text{Pr}(\\cap_i Z_{i1} < a_1 \\mid H_0)\\]General formula (non-binding futility bound)\\[\\beta_k = \\text{Pr}(\\cap_{=1}^{=k-1} a_i <G_i < b_i, G_k < a_k \\mid H_0)\\]gsdmvn implementation MaxCombo testCompared lower bound calculated gsDesign","code":"\nn <- 400\nutility <- gsdmvn:::gs_utility_combo(enrollRates, failRates, fh_test = fh_test, ratio = 1)\nbound <- gsdmvn:::gs_bound(alpha_spend, beta_spend,\n  analysis = utility$info$Analysis, # Analysis indicator\n  theta = utility$theta * sqrt(n), # Under the alternative hypothesis\n  corr = utility$corr # Correlation\n)\n\nbound$lower#> [1] -2.6106678  0.9619422  2.0974300\nx$lower$bound#> [1] -0.2361874  1.1703638  1.9929702"},{"path":"maxcombo-boundary.html","id":"sample-size-calculation-2","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.7 Sample size calculation","text":"sample size boundaries set simultaneously using iterative algorithm.","code":""},{"path":"maxcombo-boundary.html","id":"initiate-the-calculation-from-lower-bound-derived-at-n-400","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.7.1 Initiate the calculation from lower bound derived at \\(N = 400\\)","text":"","code":"\nbound#>      upper      lower\n#> 1 6.175397 -2.6106678\n#> 2 2.798651  0.9619422\n#> 3 2.097430  2.0974300\ngs_design_combo(enrollRates,\n  failRates,\n  fh_test,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE,\n  upar = bound$upper,\n  lpar = bound$lower,\n) %>% mutate_if(is.numeric, round, digits = 2)#>   Analysis Bound Time      N Events     Z Probability Probability_Null\n#> 1        1 Upper   12 324.97  69.80  6.18        0.00             0.00\n#> 3        2 Upper   24 324.97 160.07  2.80        0.24             0.00\n#> 5        3 Upper   36 324.97 215.32  2.10        0.80             0.03\n#> 2        1 Lower   12 324.97  69.80 -2.61        0.00               NA\n#> 4        2 Lower   24 324.97 160.07  0.96        0.13               NA\n#> 6        3 Lower   36 324.97 215.32  2.10        0.20               NA"},{"path":"maxcombo-boundary.html","id":"update-bound-based-on-newly-calculated-sample-size","chapter":"10 Group sequential design boundary with MaxCombo test","heading":"10.7.2 Update bound based on newly calculated sample size","text":"Repeat proceduce sample size lower bound converge","code":"\nn <- 355\nutility <- gsdmvn:::gs_utility_combo(enrollRates, failRates, fh_test = fh_test, ratio = 1)\nbound <- gsdmvn:::gs_bound(alpha_spend, beta_spend,\n  analysis = utility$info$Analysis, # Analysis indicator\n  theta = utility$theta * sqrt(n), # Under the alternative hypothesis\n  corr = utility$corr # Correlation\n)\n\nbound#>      upper      lower\n#> 1 6.175397 -2.6568642\n#> 2 2.798651  0.8266125\n#> 3 2.097570  2.0975699"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"[1] Miettinen, T. ., Pyörälä, K., Olsson, . G., Musliner, T. ., Cook, T. J., Faergeman, O., Berg, K., Pedersen, T., Kjekshus, J. Group, S. S. S. (1997). Cholesterol-lowering therapy women elderly patients myocardial infarction angina pectoris: Findings scandinavian simvastatin survival study (4S). Circulation 96 4211–8.[2] Lan, K. K. G. DeMets, D. L. (1983). Discrete sequential boundaries clinical trials. Biometrika 70 659–63.[3] Haybittle, J. (1971). Repeated assessment results clinical trials cancer treatment. British Journal Radiology 44 793–7.[4] Wang, S. K. Tsiatis, . . (1987). Approximately optimal one-parameter boundaries group sequential trials. Biometrics 193–9.[5] Pocock, S. J. (1977). Group sequential methods design analysis clinical trials. Biometrika 64 191–9.[6] O’Brien, P. C. Fleming, T. R. (1979). multiple testing procedure clinical trials. Biometrics 549–56.[7] Jennison, C. Turnbull, B. W. (2000). Group sequential methods applications clinical trials. Chapman; Hall/CRC, Boca Raton, FL.[8] Gandhi, L., Rodrı́guez-Abreu, D., Gadgeel, S., Esteban, E., Felip, E., De Angelis, F., Domine, M., Clingan, P., Hochmair, M. J., Powell, S. F. others. (2018). Pembrolizumab plus chemotherapy metastatic non–small-cell lung cancer. New England Journal Medicine 378 2078–92.[9] Downs, J. R., Beere, P. ., Whitney, E., Clearfield, M., Weis, S., Rochen, J., Stein, E. ., Shapiro, D. R., Langendorfer, . Gotto Jr, . M. (1997). Design & rationale air force/texas coronary atherosclerosis prevention study (afcaps/texcaps). American Journal Cardiology 80 287–93.[10] Downs, J. R., Clearfield, M., Weis, S., Whitney, E., Shapiro, D. R., Beere, P. ., Langendorfer, ., Stein, E. ., Kruyer, W., Gotto Jr, . M. others. (1998). Primary prevention acute coronary events lovastatin men women average cholesterol levels: Results afcaps/texcaps. Journal American Medical Association 279 1615–22.[11] White, W. B., Cannon, C. P., Heller, S. R., Nissen, S. E., Bergenstal, R. M., Bakris, G. L., Perez, . T., Fleck, P. R., Mehta, C. R., Kupfer, S. others. (2013). Alogliptin acute coronary syndrome patients type 2 diabetes. New England Journal Medicine 369 1327–35.[12] Lachin, J. M. Foulkes, M. . (1986a). Evaluation sample size power analyses survival allowance nonuniform patient entry, losses follow-, noncompliance, stratification. Biometrics 42 507–19.[13] Kim, K. Tsiatis, . . (1990). Study duration clinical trials survival response early stopping rule. Biometrics 46 81–92.[14] Schoenfeld, D. (1981). asymptotic properties nonparametric tests comparing survival distributions. Biometrika 68 316–9.[15] Maurer, W. Bretz, F. (2013). Multiple testing group sequential trials using graphical approaches. Statistics Biopharmaceutical Research 5 311–20.[16] Hwang, . K., Shih, W. J. De Cani, J. S. (1990). Group sequential designs using family type error probability spending functions. Statistics Medicine 9 1439–45.[17] White, W. B., Bakris, G. L., Bergenstal, R. M., Cannon, C. P., Cushman, W. C., Fleck, P., Heller, S., Mehta, C., Nissen, S. E., Perez, . others. (2011). EXamination cArdiovascular outcoMes alogliptIN versus standard carE patients type 2 diabetes mellitus acute coronary syndrome (examine): cardiovascular safety study dipeptidyl peptidase 4 inhibitor alogliptin patients type 2 diabetes acute coronary syndrome. American Heart Journal 162 620–6.[18] De Castro, M., Cancho, V. G. Rodrigues, J. (2010). hands-approach fitting long-term survival models gamlss framework. Computer Methods Programs Biomedicine 97 168–77.[19] Zhang, Y. Shao, Y. (2018). Concordance measure discriminatory accuracy transformation cure models. Biostatistics 19 14–26.[20] Zeng, D., Yin, G. Ibrahim, J. G. (2006). Semiparametric transformation models survival data cure fraction. Journal American Statistical Association 101 670–84.[21] Hellmann, M. D., Ma, J., Garon, E. B., Hui, R., Gandhi, L., Soria, J.-C., Anderson, K. M., Lubiniecki, G. M., Piperdi, B. Herbst, R. S. (2017). Estimating long-term survival pd-l1-expressing, previously treated, non-small cell lung cancer patients received pembrolizumab keynote-001 -010.[22] Lan, K. K. G. DeMets, D. L. (1989). Group sequential procedures: Calendar versus information time. Statistics Medicine 8 1191–8.[23] Harrington, D. P. Fleming, T. R. (1982). class rank test procedures censored survival data. Biometrika 69 553–66.[24] Magirr, D. Burman, C.-F. (2019a). Modestly weighted logrank tests. Statistics Medicine 38 3782–90.[25] Lin, R. S., Lin, J., Roychoudhury, S., Anderson, K. M., Hu, T., Huang, B., Leon, L. F., Liao, J. J., Liu, R., Luo, X. others. (2020). Alternative analysis methods time event endpoints nonproportional hazards: comparative analysis. Statistics Biopharmaceutical Research 12 187–98.[26] Roychoudhury, S., Anderson, K. M., Ye, J. Mukhopadhyay, P. (2021). Robust design analysis clinical trials non-proportional hazards: straw man guidance cross-pharma working group. Statistics Biopharmaceutical Research 1–37.[27] Yung, G. Liu, Y. (2019). Sample size power weighted log-rank test kaplan-meier based tests allowance nonproportional hazards. Biometrics.[28] Lachin, J. M. Foulkes, M. . (1986b). Evaluation sample size power analyses survival allowance nonuniform patient entry, losses follow-, noncompliance, stratification. Biometrics 507–19.[29] Magirr, D. Burman, C.-F. (2019b). Modestly weighted logrank tests. Statistics Medicine 38 3782–90.[30] Karrison, T. G. (2016). Versatile tests comparing survival curves based weighted log-rank statistics. Stata Journal 16 678–90.[31] Wang, L., Luo, X. Zheng, C. (2019). simulation-free group sequential design max-combo tests presence non-proportional hazards. arXiv preprint arXiv:1911.05684.[32] Scharfstein, D. O., Tsiatis, . . Robins, J. M. (1997). Semiparametric efficiency implication design analysis group-sequential studies. Journal American Statistical Association 92 1342–50.[33] Gordon Lan, K. DeMets, D. L. (1983). Discrete sequential boundaries clinical trials. Biometrika 70 659–63.[34] Lachin, J. M. (2005). review methods futility stopping based conditional power. Statistics Medicine 24 2747–64.","code":""}]
