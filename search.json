[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Group Sequential Design Under Non-Proportional Hazards",
    "section": "",
    "text": "Welcome\nWelcome to the online version of “Group Sequential Design Under Non-Proportional Hazards” by Keaven M. Anderson, Yilong Zhang, Nan Xiao, and Yujie Zhao.\nThis is the material used in the 2021 Deming Conference course on group sequential design.\nWe consider group sequential design for time-to-event endpoints under both proportional and non-proportional hazards assumptions for randomized clinical trials. While the primary focus will be on logrank testing due to its regulatory acceptance, weighted logrank test, combination tests and RMST will also be considered. Timing of analyses and boundary setting for efficacy and futility are critical topics to be discussed at length. A simple, piecewise model that can be used to approximate arbitrary scenarios is proposed. In addition to 2-arm comparisons for a single endpoint, we will also discuss graphical methods for strong control of Type I error when there are hypotheses for multiple endpoints and/or multiple populations. Asymptotic theory will be briefly noted as background, but the focus will be on applications, including software to quickly compare designs and scenarios. Throughout the course, we will develop designs incorporating each key new concept.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Training overview\nIn this course, we will present concepts, theory, software, and a Shiny interface. Mainly we will focus on designs that you might consider for time-to-event endpoints. In addition to classical approaches assuming a proportional hazards assumption, we will provide methods for designing under non-proportional hazards assumptions. While most studies still use a logrank test, we will also touch on some alternatives along with their potential advantages and disadvantages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#training-overview",
    "href": "preface.html#training-overview",
    "title": "Preface",
    "section": "",
    "text": "Disclaimer\n\n\n\nAll opinions expressed are those of the presenters and not Merck & Co., Inc., Rahway, NJ, USA.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#chapters-and-training-sections",
    "href": "preface.html#chapters-and-training-sections",
    "title": "Preface",
    "section": "Chapters and training sections",
    "text": "Chapters and training sections\n\n\nBackground theory (30 minutes)\n\nExtension to non-proportional hazards\nGroup sequential design asymptotic distribution\nSpending function bounds\n\n\n\nProportional hazards applications with Shiny app (40 minutes)\n\nLachin and Foulkes method for sample size derivation\nDesign setup with exponential distribution\nDesign setup with cure model\nUpdating bounds at time of analysis\nEvent-based and calendar-based spending bounds\nExercise\n\n\nBreak (15 minutes)\n\nNon-proportional hazards model with logrank test (60 minutes)\n\nPiecewise model\nAverage hazard ratio\nStatistical information and time\nIntroduction to gsdmvn, gsDesign2 and simtrial\n\n\nBreak (10 minutes)\n\nWeighted logrank and combination tests (55 minutes)\n\nIntroduction to methods\nWeighted logrank\nMaxCombo\nExercise",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#software-and-supporting-materials",
    "href": "preface.html#software-and-supporting-materials",
    "title": "Preface",
    "section": "Software and supporting materials",
    "text": "Software and supporting materials\n\nUseful directories in course repository at https://github.com/keaven/gsd-deming:\n\n\ndata/: contains design files for examples; also simulation results\n\nvignettes/: reports produced by Shiny app to summarize designs\n\nsimulation/: R code and simulation data for the last part of the course",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "preface.html#installing-r-packages",
    "href": "preface.html#installing-r-packages",
    "title": "Preface",
    "section": "Installing R packages",
    "text": "Installing R packages\nIf you choose to install R packages locally:\n\nThe gsDesign package (v3.2.1) is available at CRAN\n\n\ninstall.packages(\"gsDesign\")\n\n\nFor non-proportional hazards, the following 3 R packages would be useful to install\n\n\nremotes::install_github(\"Merck/simtrial@87cd828\")\nremotes::install_github(\"Merck/gsDesign2@fc3a2d3\")\nremotes::install_github(\"Merck/gsdmvn@ef2bb74\", upgrade = \"never\")\n\nYou will need reasonably recent versions of R and packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "\n1  Background\n",
    "section": "",
    "text": "1.1 Non-proportional hazards\nNon-proportional hazards (NPH) are very common to see when there is a delayed effect. One example is given by KEYNOTE-040, which targets to cure recurrent head and neck squamous cell carcinoma. There are two arms, one is pembrolizumab, and the other is standard of care (SOC). The randomization ratio is 1:1 and the total sample size is 495. The primary endpoint is overall survival (OS) with 90% power for HR=0.7, 1-sided \\(\\alpha=0.025\\) (The detailed design can be referred to the protocol).\nThere are two interim analysis planned, one at 144 deaths (with information fraction as 0.42) and the other at 216 deaths (with information fraction as 0.64). The efficacy boundary is Hwang-Shih-DeCani (HSD) spending with \\(\\gamma=-4\\), and the futility boundary is non-binding \\(\\beta\\)-spending HSD with \\(\\gamma = -16\\). The final analysis finds there are 388 deaths, compared with 340 planned death. The 1-sided nominal \\(p\\)-value for OS at the final analysis is 0.0161.\nFigure 1.1: KEYNOTE-040\nFrom the figure above, we can see there is a delay of the treatment effect.\nThe delayed effect is not just in oncology, it also lies in other TA. For example, in cholesterol lowering and mortality (Miettinen et al. 1997), there is also a delayed effect.\nFigure 1.2: Scandinavian Simvastatin Survival Study\nIn addition to delayed effect, NPH also exists when there is a cross over of survival curves. For example, in KEYNOTE-061 (recurrent advanced gastric or gastro-oesophageal junction cancer), there is a cross over of the pembrolizumab arm and paclitaxel arm (see figure below). It is 1:1 randomization with 360 planned events (395 actual events) at the end of the trial in CPS \\(\\ge\\) 1 population. The first primary endpoints is OS in PD-L1 CPS \\(\\ge\\) 1. Its power is 91% for HR=0.67 with 1-sided \\(\\alpha=0.0215\\). The secondary primary endpoints is PFS in PD-L1 CPS \\(\\ge\\) 1. (The detailed design can be found in the protocol.)\nWhen it proceeded to final analysis, there are 326 deaths with 290 planned. One interim analysis is conducted with 240 events at information fraction of 0.83. During the group sequential design, the efficacy boundary is Hwang-Shih-DeCani (HSD) spending with \\(\\gamma=-4\\) and there is not a futility boundary. The 1-sided nominal \\(p\\)-value for OS is 0.0421 (threshold: \\(p\\)=0.0135). And Post hoc FH(\\(\\rho=1,\\gamma=1\\)) is \\(p\\)=0.0009.\nFigure 1.3: KEYNOTE 061: Overall Survival in CPS 1.\nThe above two examples motivate us to think about the following questions:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#sec-nph",
    "href": "background.html#sec-nph",
    "title": "\n1  Background\n",
    "section": "",
    "text": "The impact of (potentially) delayed treatment effect on trial design and analysis.\nTest statistics used (logrank, weighted logrank test, and max-combo test)\nSample size vs. duration of follow-up\nTiming of analyses\nFutility bounds\nUpdating bounds\nMultiplicity adjustments\n\n\n1.1.1 Group sequential design\nIn group sequential design, groups of observations are collected and repeatedly analyzed, while controlling error rates. While the group sequential design is ongoing, a stopping rule specifies when and why a trial might be halted at these points. This stopping rule usually consists of two components, a test statistic and a threshold. The threshold is usually decided by the spending functions, which we introduced in Section 1.2. And the test statistics are usually the Z-process and B-process, which are used to monitor the treatment effect under group sequential design. If a Z-process (B-process) cross the boundary, the trial is stopped.\n\n1.1.2 Z-process and B-process\nIn this section, we will introduce the Z-process and B-process. Suppose in the group sequential design, there are totally \\(K\\) looks. In the \\(k\\)-th look, there are \\(n_k\\) available observations. For notation simplicity, we denote the final number of available observations as \\(N\\), i.e., \\(N \\triangleq n_K\\).\n\nDefinition 1.1 The Z-process is defined as the standardized treatment effect, i.e., \\[\n  Z_{k}\n  =\n  \\frac{\\widehat\\theta_k}{\\sqrt{\\hbox{Var}(\\hat\\theta_k)}},\n\\tag{1.1}\\] where \\(k\\) is the index of the looks, and we assume there are totally \\(K\\) number of looks, i.e., \\(k = 1, 2, \\ldots, K\\). The numerator \\(\\widehat\\theta_k\\) is the treatment effect estimated at the \\(k\\)-th look.\n\nFor \\(\\widehat\\theta_k\\), its form depends on the outcome of the clinical trials. In clinical trials with continuous outcome \\(X_1, X_2, \\ldots \\in \\mathbb R\\), the treatment effect estimated at the \\(k\\)-th look is \\[\n  \\widehat{\\theta}_k\n  =\n  \\frac{\\sum_{i=1}^{n_k} X_{i}}{n_k}\\equiv \\bar X_{k},\n\\tag{1.2}\\] where \\(n_k\\) is the number of available observations at the \\(k\\)-th look. In clinical trials with binary outcome \\(X_1, X_2, \\ldots \\in \\mathbb \\{0, 1\\}\\), \\(\\widehat\\theta_k\\) can be estimated the same as in Equation 1.2. In clinical trials with survival outcome, \\(\\widehat\\theta_k\\) would typically represent a Cox model coefficient representing the logarithm of the hazard ratio for experimental vs. control treatment. And \\(n_k\\) would represent the planned number of events at \\(k\\)-th look.\nAfter discussing the Z-process, let us take a look at the B-process.\n\nDefinition 1.2 The B-process is defined as \\[\n  B_{k} = \\sqrt{t_k} Z_k,\n\\tag{1.3}\\] where \\(Z_k\\) is defined in Definition 1.1 and \\(t_k\\) is the information fraction at the \\(k\\)-th look, i.e., \\[\n  t_k = \\mathcal I_k / \\mathcal I_K,\n\\] where \\(\\mathcal I_k\\) is the information at the \\(k\\)-th look, defined as \\[\n  \\mathcal I_k \\triangleq \\frac{1}{\\text{Var}(\\widehat\\theta_k)}.\n\\]\n\nThe information sequence plays an important role in the group sequential design. For example, in a clinical trial with continuous outcome \\(X_1, X_2, \\ldots \\overset{i.i.d.}{\\sim} N(\\mu, \\sigma^2)\\), the information at the \\(k\\)-th look is \\(\\mathcal I_k = n_k\\) with \\(n_k\\) as the number of available observations at the \\(k\\)-th look. The same logic applies to the binary outcome. Another example is a clinical trial with survival outcome. In this case, the information at the \\(k\\)-th look is \\(\\mathcal I_k = n_k\\) with \\(n_k\\) as the number of events at the \\(k\\)-th look.\n\n1.1.3 Canonical form\nIn the group sequential design, given the total \\(K\\) look, there are sequences of Z-process and B-process, i.e., \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) and \\(\\{B_k\\}_{k = 1, \\ldots, K}\\). The CF refers to the joint distribution of \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) and \\(\\{B_k\\}_{k = 1, \\ldots, K}\\), including the distribution, the expectation mean, variance and covariance. Please note that this distribution is asymptotic, and the asymptotic conditions usually requires the sample size to be relative large. Properly speaking, both \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) and \\(\\{B_k\\}_{k = 1, \\ldots, K}\\) depends on the sample size, which can be re-denoted as \\(\\{ Z_{k,N} \\}_{k = 1, \\ldots, K}\\) and \\(\\{ B_{k, N} \\}_{k = 1, \\ldots, K}\\). When the sample size is relatively large, we can get rid of the sample size, i.e., \\[\n  Z_k = \\lim\\limits_{N \\to \\infty} Z_{k,N}.\n\\] This asymptotic assumed throughout this book.\nAn important assumption of CF is \\(E(\\widehat\\theta_k) = \\theta\\), where \\(\\theta\\) is a constant and \\(\\widehat\\theta\\) is defined in Equation 1.2.\n\nTheorem 1.1 Suppose in a group sequential design, there are totally \\(K\\) looks. At the \\(k\\)-th look, there are \\(n_k\\) available observations, i.e., \\(\\{X_i\\}_{i = 1, \\ldots, n_k}\\). Assume that \\(\\text{Var}(X_i) = 1\\) for \\(i = 1, 2, \\ldots, N\\). And denote \\(\\mathcal I_k \\triangleq \\frac{1}{\\text{Var}(\\widehat\\theta(t_k))}\\) and \\(E(\\widehat\\theta_k) = \\theta\\). The joint distribution of Z-process \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) is \\[\n\\left\\{\n\\begin{array}{l}\n  (Z_1, \\ldots, Z_K) \\text{ is multivariate normal} \\\\\n  E(Z_k) =  \\sqrt{\\mathcal{I}_k}\\theta \\\\\n  \\hbox{Var}(Z_k)  =  1 \\\\\n  \\text{Cov}(Z_i, Z_j)  =  \\sqrt{t_i/t_j} \\;\\; \\forall 1 \\leq i \\leq j \\leq K\n\\end{array}\n\\right..\n\\] where \\(t_i = n_i / N\\) with \\(N \\triangleq n_K\\) as the final number of observations at the last look. If a statistics have the above distribution, we declare that it has the canonical joint distribution with information levels \\(\\{\\mathcal I_1, \\ldots, \\mathcal I_K \\}\\) for the parameter \\(\\theta\\).\n\nFrom the above theorem, we found that CF treats \\(E(\\widehat\\theta)\\) as a constant, i.e., \\(\\theta\\). Following the similar, we can also summarize the distribution of B-process.\n\nTheorem 1.2 Following the same setting of Theorem 1.1, the distribution of B-process is \\[\n\\left\\{\n\\begin{array}{l}\n  (B_1, \\ldots, B_K) \\text{ is multivariate normal} \\\\\n  E(B_k) = \\sqrt{t_{k}\\mathcal{I}_k}\\theta = t_k \\sqrt{\\mathcal{I}_K} \\theta = \\mathcal{I}_k \\theta / \\sqrt{\\mathcal{I}_K} \\\\\n  \\hbox{Var}(B_k) = t_k \\\\\n  \\text{Cov}(B_i, B_j) = t_i \\;\\; \\forall 1 \\leq i \\leq j \\leq K\n\\end{array}\n\\right.,\n\\] where \\(t_i = n_i/N\\) with \\(N \\triangleq n_K\\) as the final number of observations at the last look.\n\nIt should be noted that the correlation of B-process is the same as the covariance of Z-process, i.e., \\[\\begin{equation}\n  Corr(B_i, B_j) = Cov(Z_i, Z_j) = \\sqrt{t_i/t_j} \\;\\; \\forall 1 \\leq i \\leq j \\leq K.\n\\end{equation}\\] To learn this covariance/correlation better, let us take the continuous outcome as an example. Suppose there are \\(K\\) looks in a clinical trial, and at the \\(k\\)-th look, there are \\(n_k\\) available observations with outcome \\(\\{ X_i \\}_{i = 1, \\ldots, n_k}\\). In this example, we have \\[\\begin{eqnarray}\n  B_k\n  & = &\n  \\sqrt{t_k} Z_k\n  =\n  \\sqrt{\\mathcal I_k / \\mathcal I_K} \\frac{\\widehat\\theta_k}{\\sqrt{Var(\\widehat\\theta_k)}}\n  =\n  \\mathcal I_k\\sqrt{ 1 / \\mathcal I_K}  \\widehat\\theta_k \\\\\n  & = &\n  \\mathcal I_k\\sqrt{ 1 / \\mathcal I_K}  \\frac{\\sum_{i=1}^{n_k} X_i}{n_k}\n  =\n  \\frac{\\sum_{i=1}^{n_k} X_i}{\\sqrt{I_K}}\n  =\n  \\frac{\\sum_{i=1}^{n_k} X_i}{\\sqrt{N}}.\n\\end{eqnarray}\\] Given the above B-process, for any \\(1 \\leq i \\leq j \\leq K\\), we have \\[\n  B_j - B_i \\sim N(\\mathcal I_j \\theta(t_j) - \\mathcal I_i \\theta(t_i), t_j - t_i)\n\\] independent of \\(B_1, B_2, \\ldots, B_i\\). So we have \\[\\begin{eqnarray}\n  Corr(B_i, B_j)\n  & = &\n  Cov(B_i, B_j) \\bigg / \\sqrt{Var(B_i) Var(B_j)} \\\\\n  & = &\n  t_i \\bigg / \\sqrt{t_i t_j} = \\sqrt{t_i / t_j}   \\\\\n  & = &\n  Cov(Z_i, Z_j)\n\\end{eqnarray}\\]\n\n1.1.4 Extended canonical form\nThe main difference between extended canonical form (ECF) and canonical form (CF) is that, ECF treats \\(E(\\widehat\\theta_k)\\) as a time-varying parameter, i.e., \\(\\theta(t_k)\\). While CF treats it as a constant \\(\\theta\\). So we summarize the joint distribution of Z-process and B-process under ECF as follows\n\nTheorem 1.3 Suppose in a group sequential design, there are totally \\(K\\) looks. At the \\(k\\)-th look, there are \\(n_k\\) available observations, i.e., \\(\\{X_i\\}_{i = 1, \\ldots, n_k}\\). Assume that \\(\\text{Var}(X_i) = 1\\) for \\(i = 1, 2, \\ldots, N\\). And denote \\(\\mathcal I_k \\triangleq \\frac{1}{\\text{Var}(\\widehat\\theta(t_k))}\\) and \\(E(\\widehat\\theta_k) = \\theta(t_k)\\). The joint distribution of Z-process \\(\\{Z_k\\}_{k = 1, \\ldots, K}\\) is \\[\n\\left\\{\n\\begin{array}{l}\n  (Z_1, \\ldots, Z_K) \\text{ is multivariate normal} \\\\\n  E(Z_k) =  \\sqrt{\\mathcal{I}_k} \\theta(t_k) \\\\\n  \\hbox{Var}(Z_k)  =  1 \\\\\n  \\text{Cov}(Z_i, Z_j)  =  \\sqrt{t_i/t_j} \\;\\; \\forall 1 \\leq i \\leq j \\leq K\n\\end{array}\n\\right..\n\\] And the joint distribution of B-process \\(\\{B_k\\}_{k = 1, \\ldots, K}\\) is \\[\n\\left\\{\n\\begin{array}{l}\n  (B_1, \\ldots, B_K) \\text{ is multivariate normal} \\\\\n  E(B_k) = \\sqrt{t_{k}\\mathcal{I}_k}\\theta(t_k) = t_k \\sqrt{\\mathcal{I}_K} \\theta(t_k) = \\mathcal{I}_k\\theta(t_k)/\\sqrt{\\mathcal{I}_K}\\\\\n  \\hbox{Var}(B_k) = t_k \\\\\n  \\text{Cov}(B_i, B_j) = t_i \\;\\; \\forall 1 \\leq i \\leq j \\leq K\n\\end{array}\n\\right.,\n\\] where \\(t_i = n_i / N\\) with \\(N \\triangleq n_K\\) as the final number of observations at the last look. If a statistics have the above distribution, we declare that it has the canonical joint distribution with information levels \\(\\{\\mathcal I_1, \\ldots, \\mathcal I_K \\}\\) for the parameter \\(\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#sec-sf",
    "href": "background.html#sec-sf",
    "title": "\n1  Background\n",
    "section": "\n1.2 Spending function bounds",
    "text": "1.2 Spending function bounds\nIn this section, we introduce the spending function bounds in group sequential design. We start with it definitions and categories. And then introduce three frequently used spending functions, (1) Haybittle boundary, (2) Pocock boundary and (3) O’Brien-Fleming boundary.\nSuppose we have \\(K\\) analyses conducted at information fraction of \\(t_1, t_2, \\ldots, t_k, \\ldots, t_K\\). We call the \\(\\{a_k, b_k \\}_{k = 1,\\ldots, K}\\) as the decision boundaries, if we reject \\(H_0\\) when \\(Z(t_k) &lt; a_k\\) or \\(Z(t_k) &gt; b_k\\). And we call \\(\\{b_k\\}_{k=1,\\ldots,K}\\) as the efficiency boundary, which are decided by the control of type I error, and \\(\\{a_k\\}_{k=1,\\ldots,K}\\) as the futility boundary, which are used to control boundary crossing probabilities.\nFor the calculation of boundaries, there are typically two methods. The first one is the so-called error spending approach, which specifies boundary crossing probabilities at each analysis. This is most commonly done with the error spending function approach (Lan and DeMets 1983). The second one is the so-called boundary family approach, which specifies how big boundary values should be relative to each other and adjust these relative values by a constant multiple to control overall error rates. The commonly applied boundary family include:\n\nHaybittle boundary (Haybittle 1971);\nWang-Tsiatis boundary (Wang and Tsiatis 1987):\n\nPocock boundary (Pocock 1977);\nO’Brien and Fleming boundary (O’Brien and Fleming 1979).\n\n\n\nIn the remainder of this section, we will introduce the above spending functions one by one.\n\n1.2.1 Haybittle boundary\nThe main idea of the Haybittle boundary is to set the interim Z-score boundary as 3 and the final boundary as 1.96, which is visualized in the figure below.\n\n\n\n\n\n\n\nFigure 1.4: Haybittle boundary\n\n\n\n\nThe above Haybittle boundary has three modified versions.\nThe first modified version uses Bonferroni adjustment: for the first \\(K-1\\) analyses, the significant \\(p\\)-value is set as 0.001; and for the final analysis, the significant \\(p\\)-value is set as \\(0.05 - 0.01 \\times (K-1)\\). This modification helps to avoid type I error inflation. Besides, it does not require them to be equally spaced in terms of information and can be used regardless of the joint distribution of Z-score, i.e., \\(Z(t_1),...,Z(t_k)\\).\nThe second modified version sets the Z-score as 4 for the first half of the study and sets the Z-score boundary as 3 thereafter.\nThe third modified version requires crossing the boundary at two successive looks.\nIn summary, the Haybittle boundary uses a piecewise constant boundary, which makes it very easy to implement. But it is kind of conservative, and the selection of the constant boundary (e.g., 3 and 1.96) needs justifications.\n\n1.2.2 Wang-Tsiatis boundary\nFor 2-sided testing, Wang and Tsiatis (1987) defined the boundary function for the \\(k\\)-th look as \\[\n  \\Gamma(\\alpha, K, \\Delta) k^{\\Delta - 0.5},\n\\] where \\(\\Gamma(\\alpha, K, \\Delta)\\) is a constant chosen so that the level of significance is equal to \\(\\alpha\\).\nWith two selection of \\(\\Delta\\), the Wang-Tsiatis boundary gives two special cases. When \\(\\Delta = 0.5\\), it is the Pocock bounds. When \\(\\Delta = 0\\), it is the O’Brien-Fleming bounds.\n\n1.2.3 Pocock boundary\nFor 2-sided testing, the Pocock procedure rejects at the \\(k\\)-th of \\(K\\) looks if \\[\n  |Z(k/K)| &gt; c_P(K),\n\\] where \\(c_P(K)\\) is fixed given the number of total looks \\(K\\) and chosen such that \\(\\text{Pr}(\\cup_{k=1}^{K} |Z(k/K)| &gt; c_P(K)) = \\alpha\\).\n\n\n\n\n\n\n\nFigure 1.5: Pocock boundary\n\n\n\n\nHere is an example of the Pocock boundary.\n\n\n\n\n\n\n\n\ntotal number of looks(K)\n\\(\\alpha = 0.01\\)\n\\(\\alpha = 0.05\\)\n\\(\\alpha = 0.1\\)\n\n\n\n1\n2.576\n1.960\n1.645\n\n\n2\n2.772\n2.178\n1.875\n\n\n4\n2.939\n2.361\n2.067\n\n\n8\n3.078\n2.512\n2.225\n\n\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\\(\\infty\\)\n\n\n\nWe will reject \\(H_0\\) if \\(|Z(k/4)| &gt; 2.361\\) for \\(k = 1,2,3,4\\) (final analysis).\nIn summary, the Pocock boundary is a special case of Wang-Tsiatis boundary (\\(\\Delta = 0.5\\)) and has constant Z-score boundaries, i.e., \\(|Z(k/K)| &gt; c_P(K)\\). And the constant boundary \\(c_P(K)\\) is decided by the type I error rate and the total number of looks. Its main weakness is the high price for the end of the trial and the requirement of equally spaced looks.\n\n1.2.4 O’Brien-Fleming boundary\nO’Brien-Fleming boundary is very conservative at the beginning, which gives a relatively large boundary first. But it gives a nominal value close to the overall value of the design \\(\\approx 1.96\\) when two-side \\(\\alpha = 0.05\\) for the final stage. Its visualization can be found as follows.\n\n\n\n\n\n\n\nFigure 1.6: O’Brien-Fleming boundary\n\n\n\n\nHere is an example of the O’Brien-Fleming boundary.\n\n\n\n\n\n\n\n\ntotal number of looks(K)\n\\(\\alpha = 0.01\\)\n\\(\\alpha = 0.05\\)\n\\(\\alpha = 0.1\\)\n\n\n\n1\n2.576\n1.960\n1.645\n\n\n2\n2.580\n1.977\n1.678\n\n\n4\n2.609\n2.024\n1.733\n\n\n8\n2.648\n2.072\n1.786\n\n\n16\n2.684\n2.114\n1.830\n\n\n\\(\\infty\\)\n2.807\n2.241\n1.960\n\n\n\nSince the tabled value 2.024 is the flat B-value boundary. The B-value boundary can be easily transformed into decreasing Z-score boundary by \\(Z(t) = B(t)/\\sqrt{t}\\):\n\n\\(2.024/\\sqrt{1/4} = 4.05\\)\n\\(2.024/\\sqrt{2/4} = 2.86\\)\n\\(2.024/\\sqrt{3/4} = 2.34\\)\n\\(2.024/\\sqrt{4/4} = 2.02\\)\n\nIn summary, the O’Brien-Fleming boundary designs a decreasing Z-score boundary, with large boundaries at the early stage but ~1.96 boundary when it approaches the final stage.\nAlthough both the Pocock boundary and OBF boundary belong to the Wang-Tsiatis boundary’s family, there are two differences between them.\nThe first difference is, the Pocock boundary has flat Z-score boundaries, while the O’Brien-Fleming has decreasing Z-score boundary. Accordingly, the O’Brien-Fleming boundary makes it much more difficult to stop early than the Pocock boundary. However, the O’Brien-Fleming boundary extracts a much smaller price at the end.\n\n\n\n\n\n\n\nFigure 1.7: Pocock (circles) and O’Brien-Fleming (squares) z-score boundaries for four looks\n\n\n\n\nThe second difference is, they have different performance in cumulative type I error rate (the probability of rejection at or before \\(t_j\\), i.e., \\(\\text{Pr}(\\cup_{i=1}^j |Z(t_i)| &gt; c_i)\\)).\n\nThe Pocock cumulative type I error rate increases sharply at first, but much less so toward the end.\nThe O’Brien-Fleming behaves in just the opposite way.\n\n\n\n\n\n\n\n\nFigure 1.8: Cumulative type I error rate used by the Pocock (circles) and O’Brien-Fleming (squares) procedures with four looks.\n\n\n\n\nIn conclusion, we visualize the three spending functions in the following figure.\n\n\n\n\n\n\n\nFigure 1.9: Three boundaries",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#statistical-information-and-time",
    "href": "background.html#statistical-information-and-time",
    "title": "\n1  Background\n",
    "section": "\n1.3 Statistical information and time",
    "text": "1.3 Statistical information and time\nStatistical information and time is a key component in group sequential design. In this section, we introduce the statistical information under three different outcomes, (1) continuous outcome, (2) binary outcome and (3) survival outcome.\nWe start with the continuous outcome. Suppose in a \\(T\\)-month trial, and the interim analysis is conducted after \\(\\tau\\) month with \\(n\\) of \\(N\\) planned observations evaluated in each arm. These \\(n\\) available observation is denoted as \\(\\{X_i\\}_{i = 1, \\ldots, n}\\) for the control arm and \\(\\{Y_i\\}_{i = 1, \\ldots, n}\\) for the treatment arm. Thus, the treatment effect can be estimated as \\[\n  \\widehat\\delta(\\tau) = \\frac{\\sum_{i=1}^n (Y_i - X_i)}{n}.\n\\] For \\(Y_i - X_i\\), we commonly assume that \\(Y_i - X_i \\overset{i.i.d.}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) for any \\(i = 1, \\ldots, n\\), with \\(\\mu = 0\\) under the null hypothesis and \\(\\mu \\neq 0\\) under the alternative hypothesis.\nThe information at the \\(\\tau\\)-th month is \\[\n  I(\\tau) = 1/\\text{Var}(\\widehat\\delta(\\tau)) = n/\\sigma^2.\n\\] Accordingly, we define the information fraction as the ratio between interim information and final information, i.e., \\[\n  t = I(\\tau)/I(T)= \\frac{n/\\sigma^2}{N/\\sigma^2} = \\frac{n}{N}.\n\\]\nFor binary outcome, it follows the similar logic as that in continuous outcome. We commonly assume that \\(Y_i - X_i \\overset{i.i.d.}{\\sim} \\text{Bernoulli}(p)\\) for any \\(i = 1, \\ldots, n\\), with \\(p = 0.5\\) under the null hypothesis and \\(p \\neq 0.5\\) under the alternative hypothesis. The information at the \\(\\tau\\)-th month is \\[\n  I(\\tau) = 1/\\text{Var}(\\widehat\\delta(\\tau)) = \\frac{n}{p(1-p)}.\n\\] Accordingly, the information fraction is \\[\n  t = I(\\tau)/I(T)= \\frac{\\frac{n}{p(1-p)}}{\\frac{N}{p(1-p)}} = \\frac{n}{N}.\n\\]\nFor survival outcome, it is a little bit complicated. Suppose the interim analysis is conducted after \\(\\tau\\) month with \\(d\\) of \\(D\\) planned events evaluated in each arm. The treatment effect under survival outcome is usually measured by logrank test, i.e., \\[\n  \\widehat\\delta(\\tau)\n  =\n  \\frac{\\sum_{i=1}^d (O_i - E_i)}{\\sum_{i = 1}^d V_i}\n  \\triangleq\n  \\frac{S(\\tau)}{\\sum_{i = 1}^d V_i}.\n\\] Accordingly, the information fraction is \\[\n\\begin{array}{ccl}\n  t\n  & = & \\text{Var}(S(\\tau)) / \\text{Var}(S(1)) \\\\\n  & = & \\text{Var}(\\sum_{i =1}^{d} (O_i - E_i)) \\bigg / \\text{Var}(\\sum_{i =1}^{D} (O_i - E_i)) \\\\\n  & = & \\sum_{i =1}^{d} \\text{Var}(O_i - E_i) \\bigg / \\sum_{i =1}^{D} \\text{Var}(O_i - E_i) \\\\\n  & = & \\sum_{i =1}^{d} E_i(1-E_i) \\bigg / \\sum_{i =1}^{D} E_i(1-E_i) \\\\\n  & \\approx & \\sum_{i =1}^{d} 0.5(1-0.5) \\bigg / \\sum_{i =1}^{D} 0.5(1-0.5) \\\\\n  & = & d/D.\n\\end{array}\n\\]\nComparing survival outcome and continuous/binary outcome, we found the information (fraction) of survival outcome is decided by the number of events, while it is decided by the number of available observations under continuous/binary outcome.\nIf there are multiple interim analysis at \\(\\tau_1\\)-th, \\(\\tau_2\\)-th, \\(\\ldots\\), \\(\\tau_K\\)-th month, we get a sequence of information as \\(I(\\tau_1), I(\\tau_2), \\ldots, I(\\tau_K)\\) and a sequence of information fraction \\(t_1, t_2, \\ldots, t_K\\). Please note that the information sequence is sometimes simplified as \\(I_1, I_2, \\ldots, I_K\\) (Jennison and Turnbull 2000).\n\n\n\n\nHaybittle, JL. 1971. “Repeated Assessment of Results in Clinical Trials of Cancer Treatment.” The British Journal of Radiology 44 (526): 793–97.\n\n\nJennison, Christopher, and Bruce W. Turnbull. 2000. Group Sequential Methods with Applications to Clinical Trials. Boca Raton, FL: Chapman; Hall/CRC.\n\n\nLan, K. K. G., and David L. DeMets. 1983. “Discrete Sequential Boundaries for Clinical Trials.” Biometrika 70: 659–63.\n\n\nMiettinen, Tatu A, Kalevi Pyörälä, Anders G Olsson, Thomas A Musliner, Thomas J Cook, Ole Faergeman, Kåre Berg, Terje Pedersen, John Kjekshus, and for the Scandinavian Simvastatin Study Group. 1997. “Cholesterol-Lowering Therapy in Women and Elderly Patients with Myocardial Infarction or Angina Pectoris: Findings from the Scandinavian Simvastatin Survival Study (4S).” Circulation 96 (12): 4211–18.\n\n\nO’Brien, Peter C, and Thomas R Fleming. 1979. “A Multiple Testing Procedure for Clinical Trials.” Biometrics, 549–56.\n\n\nPocock, Stuart J. 1977. “Group Sequential Methods in the Design and Analysis of Clinical Trials.” Biometrika 64 (2): 191–99.\n\n\nWang, Samuel K, and Anastasios A Tsiatis. 1987. “Approximately Optimal One-Parameter Boundaries for Group Sequential Trials.” Biometrics, 193–99.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "proportional-hazards.html",
    "href": "proportional-hazards.html",
    "title": "2  Proportional hazards",
    "section": "",
    "text": "2.1 Lachin and Foulkes method\nThe gsDesign R package focuses on sample size and power derivation primarily on the method of Lachin and Foulkes (Lachin and Foulkes 1986). This allows simple specification of piecewise constant enrollment, failure rates and dropout rates which can approximate general distributions for each. While the sample size implementation focuses primarily on increasing enrollment rate to achieve power within a targeted trial duration, there is also an option to fix a maximum enrollment rate and extend follow-up until power is achieved as in Kim and Tsiatis (1990); this method will not always work without some adjustment in assumptions. The methods all focus on 2-arm trials with a constant treatment effect over time (proportional hazards). It is assumed that a (stratified) logrank test or Wald test with a Cox model coefficient are used for statistical testing. The number of events drives power, regardless of study duration, as noted by Schoenfeld (1981); that approximation for event requirements is provided in the nSurv() function for fixed design and gsSurv() for group sequential design as can be seen by code generated in the app or in gsDesign package documentation.\nOne place where we deviate from the Lachin and Foulkes method is when we approximate the hazard ratio at study bounds. This should always be considered an approximation that is informative to those reviewing the design rather than a requirement to cross a bound. In any case, the approximation is that of Schoenfeld (1981), which is quite simple and implemented in the function gsDesign::gsHR().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportional hazards</span>"
    ]
  },
  {
    "objectID": "proportional-hazards.html#sec-median-survival",
    "href": "proportional-hazards.html#sec-median-survival",
    "title": "2  Proportional hazards",
    "section": "\n2.2 Metastatic oncology example",
    "text": "2.2 Metastatic oncology example\nThe KEYNOTE-189 trial (Gandhi et al. (2018)) evaluated progression-free survival (PFS) and overall survival (OS) in patients with previously untreated metastatic non-small cell lung cancer (NSCLC). Patients were all treated with chemotherapy and randomized 2:1 to an add-on of pembrolizumab or placebo. The most important finding was: After a median follow-up of 10.5 months, the estimated rate of overall survival at 12 months was 69.2% (95% confidence interval [CI], 64.1 to 73.8) in the pembrolizumab-combination group versus 49.4% (95% CI, 42.1 to 56.2) in the placebo-combination group (hazard ratio for death, 0.49; 95% CI, 0.38 to 0.64; P&lt;0.001) (Gandhi et al. 2018).\nThe design allocated 0.025 family-wise error rate (FWER, also referred to as 1-sided Type I error) between PFS (\\(\\alpha=0.095\\)) and OS (\\(\\alpha=0.015\\)). The graphical method \\(\\alpha\\)-control for group sequential design of Maurer and Bretz (2013) was used to control Type I error. We further review the design for OS here but do not attempt to exactly reproduce all details. Key aspects of the design as documented in the protocol accompanying Gandhi et al. (2018) were:\n\n\n\\(\\alpha=0.0155\\).\nControl group survival follows an exponential distribution with a median of 13 months.\nExponential dropout rate of 0.1% per month.\n90% power to detect a hazard ratio of 0.7.\n2:1 randomization, experimental:control.\nEnrollment over 1 year; while not specified in the protocol, we have further assumed.\nTrial duration of approximately 35 months.\nObserved deaths of 240, 332, and 416 at the 3 planned analyses; this yields information fractions of 0.5769231 and 0.7980769 at the interim analyses.\nA one-sided bound using the Lan and DeMets (1983) spending function approximating an O’Brien-Fleming bound.\n\nWe adjusted the exponential dropout rate to 0.133% per month and hazard ratio to 0.70025 in the following so that both targeted events and targeted sample size would be close to integers. This sort of adjustment can limit any confusion over the computation of interim bounds, as will be demonstrated.\nThe design is derived as:\n\nlibrary(gsDesign)\nlibrary(dplyr)\nlibrary(gt)\nlibrary(ggplot2)\n\n\nKEYNOTE189design &lt;- gsSurv(\n  # Number of analyses\n  k = 3,\n  # One-sided design\n  test.type = 1,\n  # 1-sided Type I error\n  alpha = 0.0155,\n  # Type II error (1 - targeted power)\n  beta = 0.1,\n  # Timing (information fraction) at interim analyses\n  timing = c(0.5769231, 0.7980769),\n  # Efficacy bound spending function (no spending parameter needed)\n  sfu = sfLDOF,\n  # Control group exponential failure rate to get median OS of 13 months\n  lambdaC = log(2) / 13,\n  # Alternate hypothesis hazard ratio\n  hr = 0.70025,\n  # Dropout rate (exponential rate)\n  eta = 0.00133,\n  # Enrollment rates during ramp-up period\n  gamma = c(2.5, 5, 7.5, 10),\n  # Relative enrollment rate time period durations\n  R = c(2, 2, 2, 6),\n  # Calendar time of final analysis\n  T = 35,\n  # Minimum follow-up time after enrollment complete\n  minfup = 23,\n  # Randomization ratio (experimental/placebo)\n  ratio = 2\n)\nKEYNOTE189design %&gt;%\n  gsBoundSummary(ratio = 2, digits = 5, ddigits = 2, tdigits = 1, timename = \"Month\") %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"KEYNOTE189 OS design\")\n\n\n\n\n\n\n\nKEYNOTE189 OS design\n\n\nAnalysis\nValue\nEfficacy\n\n\n\n\nIA 1: 58%\nZ\n2.98048\n\n\nN: 617\np (1-sided)\n0.00144\n\n\nEvents: 240\n~HR at bound\n0.66490\n\n\nMonth: 19.2\nP(Cross) if HR=1\n0.00144\n\n\n\nP(Cross) if HR=0.7\n0.36500\n\n\nIA 2: 80%\nZ\n2.49564\n\n\nN: 617\np (1-sided)\n0.00629\n\n\nEvents: 332\n~HR at bound\n0.74785\n\n\nMonth: 26.1\nP(Cross) if HR=1\n0.00674\n\n\n\nP(Cross) if HR=0.7\n0.73080\n\n\nFinal\nZ\n2.21487\n\n\nN: 617\np (1-sided)\n0.01338\n\n\nEvents: 416\n~HR at bound\n0.79425\n\n\nMonth: 35\nP(Cross) if HR=1\n0.01550\n\n\n\nP(Cross) if HR=0.7\n0.90000\n\n\n\n\n\n\n\nWe further describe the information in this table as follows:\n\nThe IA percent is the percent of statistical information (planned events) at the interim analysis relative to the final analysis. This matches the timing input.\nWe note that while sample size (N) and events are rounded up in the table, the design stores continuous values for these numbers to exactly match the input timing of analyses. Thus, calculating with rounded up numbers in the table may not exactly match the IA % in the table.\nThe calendar timing of the analysis is the expected timing relative to opening of enrollment.\nZ-values are for standard normal test statistics with positive values representing a treatment benefit for the experimental group.\n\n\\(p\\)-values are nominal \\(p\\)-values corresponding to the Z-values.\n~HR at bound is an asymptotic approximation for the hazard ratio required to cross a bound. This is not a criterion that must be met for Type I error control.\nP(Cross) if HR=1 is the cumulative probability of crossing a bound (efficacy or futility) at or before a given analysis. We note that for the default non-binding Type I error option that P(Cross) if HR=1 will be less than the specified Type I error. This is because Type I error is computed ignoring the futility bound since it is advisory only. The actual cumulative Type I error spending is 0.001439, 0.00674, 0.0155.\nP(Cross) if HR =0.7 is the cumulative probability of crossing a bound under the alternate hypothesis.\n\nThe design is updated at the interim analysis with 235 deaths as follows. The code can be written by the app for you, this is provided only if you wish to dive into details. We note that the function gsDesign() is used for the update while gsSurv() was used for the original design. The gsSurv() function is useful to approximate calendar timing of event accrual, something that is no longer needed at the time of interim analysis. Note that the gsBoundSummary() call below is specifically state treatment differences in the table on the hazard ratio scale as well as to interpret the event count as the key to calculating information fraction at the analysis.\n\nx &lt;- KEYNOTE189design\nxu &lt;- gsDesign(\n  # First 3 parameters are all updated\n  k = x$k, # This must be at least 2\n  n.I = c(235, ceiling(x$n.I[2:3])), # Enter integers for actual events\n  # This parameter sets spending (information) fraction as\n  # planned final event count from the original trial design\n  maxn.IPlan = x$n.I[x$k],\n  # REMAINING PARAMETERS COPIED FROM ORIGINAL DESIGN\n  test.type = x$test.type, alpha = x$alpha, beta = x$beta,\n  astar = x$astar, sfu = x$upper$sf, sfupar = x$upper$param,\n  sfl = x$lower$sf, sflpar = x$lower$param,\n  delta = x$delta, delta1 = x$delta1, delta0 = x$delta0\n)\n\n# Now we summarize bounds in a table\nxu %&gt;%\n  gsBoundSummary(\n    deltaname = \"HR\", # Name of treatment difference measure\n    logdelta = TRUE, # For survival, delta is on a log scale; this transforms\n    Nname = \"Events\", # \"N\" for analyses is events for survival analysis\n    digits = 5, # Decimals for numbers in body of table\n    ddigits = 2, # Decimals for natural parameter; HR in this case\n    tdigits = 1, # Time digits (not needed here)\n    # We select key parameters for printing\n    exclude = c(\n      \"B-value\", \"CP\", \"CP H1\", \"PP\",\n      paste0(\"P(Cross) if HR=\", round(c(x$hr0, x$hr), digits = 2))\n    )\n  ) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Updated bounds for interim analysis\",\n    subtitle = \"KEYNOTE-189 trial\"\n  )\n\n\n\n\n\n\n\nUpdated bounds for interim analysis\n\n\nKEYNOTE-189 trial\n\n\nAnalysis\nValue\nEfficacy\n\n\n\n\nIA 1: 56%\nZ\n3.01615\n\n\nEvents: 235\np (1-sided)\n0.00128\n\n\n\n~HR at bound\n0.67469\n\n\n\nSpending\n0.00128\n\n\nIA 2: 80%\nZ\n2.49300\n\n\nEvents: 332\np (1-sided)\n0.00633\n\n\n\n~HR at bound\n0.76060\n\n\n\nSpending\n0.00546\n\n\nFinal\nZ\n2.21461\n\n\nEvents: 416\np (1-sided)\n0.01339\n\n\n\n~HR at bound\n0.80480\n\n\n\nSpending\n0.00876\n\n\n\n\n\n\n\nThe \\(p\\)-value reported at interim 1 was &lt;0.001 (Gandhi et al. 2018), establishing statistical significance relative to the bound of 0.00128 in the summary table above as well as in the publication.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportional hazards</span>"
    ]
  },
  {
    "objectID": "proportional-hazards.html#cardiovascular-outcomes-reduction",
    "href": "proportional-hazards.html#cardiovascular-outcomes-reduction",
    "title": "2  Proportional hazards",
    "section": "\n2.3 Cardiovascular outcomes reduction",
    "text": "2.3 Cardiovascular outcomes reduction\nThe AFCAPS/TexCAPS trial evaluated the use of lovastatin to reduce cardiovascular outcomes. The design was described in Downs et al. (1997) while results were reported in Downs et al. (1998). We approximate the design and evaluation of statistical significance here. Software and some assumptions of the model were not completely clear, so this is not an exact reproduction of the design, but is very close. Key assumptions we replicate here:\n\n5 years minimum follow-up of all patients enrolled.\nInterim analyses after 0.375 and 0.75 of final planned event count has accrued.\n2-sided bound using the Hwang, Shih, and De Cani (1990) spending function with parameter \\(\\gamma = -4\\) to approximate an O’Brien-Fleming bound.\nWe arbitrarily set the following parameters to match the design:\n\nPower of 90% for a hazard ratio of 0.6921846; this is slightly different than the 0.7 hazard ratio suggested in Downs et al. (1997) due to the use of the Lachin and Foulkes (1986) method used in gsSurv() routine below.\nEnrollment duration of 1/2 year with constant enrollment.\nAn exponential failure rate of 0.01131 per year which is nearly identical to the annual dropout rate of 0.01125.\nAn exponential dropout rate of 0.004 per year which is nearly identical to the annual dropout rate of 0.00399.\n\n\n\nThe design has been saved and can be reloaded from the gsDesign Shiny app from the book website in the data directory from the file AFCAPSDesign.rds. We provide code generated there for the design. This begins with loading the gsDesign package and setting parameters for the design with associated comments to help explain; default comments have been updated here. More parameters than needed are given than sometimes needed when code is generated by the app. Comments indicate where this is the case. Code and comments are largely copied from the app, so this may help the reader interpret what they see there.\n\n# Number of analyses\nk &lt;- 3\n# See gsDesign() help for description of boundary types\ntest.type &lt;- 2 # This indicates a 2-sided design\n# 1-sided Type I error\nalpha &lt;- 0.025\n# Type II error (1 - targeted power)\nbeta &lt;- 0.1\n# If test.type = 5 or 6, this sets maximum spending for futility\n# under the null hypothesis. Otherwise, this is ignored.\n# (astar did not need to be specified in this case)\nastar &lt;- 0\n# Timing (information fraction) at interim analyses\ntiming &lt;- c(0.375, 0.75)\n# Efficacy bound spending function\nsfu &lt;- sfHSD\n# Upper bound spending function parameters, if any\nsfupar &lt;- -4\n# NOTE THAT THE NEXT 2 PARAMETERS ARE NOT NEEDED AS THE\n# LOWER BOUND WAS SPECIFIED IN sfu and sfupar SINCE test.type=2 was specified.\n# Lower bound spending function, if used (test.type &gt; 2)\nsfl &lt;- sfLDOF\n# Lower bound spending function parameters, if any\nsflpar &lt;- c(0)\n# Assumed hazard ratio under alternate hypothesis\nhr &lt;- 0.6921846\n# Null hypothesis hazard ratio (this is a default; for superiority trials)\nhr0 &lt;- 1\n# Dropout rate (exponential rate)\neta &lt;- 0.004\n# Enrollment rate (will be inflated by gsSurv to achieve power)\ngamma &lt;- 10\n# Relative enrollment rates by time period\nR &lt;- 0.5\n# Interval durations for piecewise failure rates\n# not needed since there is only 1 rate;\n# last interval always extends indefinitely (to infininity),\n# so it does not need to be specified\nS &lt;- NULL\n# Calendar time of final analysis\nT &lt;- 5.5\n# Minimum follow-up time after enrollment complete\nminfup &lt;- 5\n# Relative enrollment (experimental/placebo)\nratio &lt;- 1\n# This gets the single control group failure rate\n# (complicated by the fact that the app has to allow piecewise rates)\nobs &lt;- matrix(c(100, 0.01131), ncol = 2)\nobs &lt;- obs[(!is.na(obs[, 1])) & (!is.na(obs[, 2])), 2]\nlambdaC &lt;- obs\n\nNow we plug the above parameters into gsSurv() to generate the design. See comments above for interpretation of the parameters.\n\nx &lt;- gsSurv(\n  k = k, test.type = test.type, alpha = alpha, beta = beta,\n  astar = astar, timing = timing, sfu = sfu, sfupar = sfupar, sfl = sfl,\n  sflpar = sflpar, lambdaC = lambdaC, hr = hr, hr0 = hr0, eta = eta,\n  gamma = gamma, R = R, S = S, T = T, minfup = minfup, ratio = ratio\n)\n\nThe reader can see summaries of the above with the following commands which we do not run here:\n\nsummary(x) # Textual summary of design\ngsBoundSummary(x) # Tabular summary of design\n\nAll details are available through the list of items returned; see the help file or try names(x) to see what is available. As examples, we look at the number of events and nominal \\(p\\)-value bounds for the design. Note that the events planned are continuous numbers rather than integers. Parameters have been chosen for the design so that these are very close to but less than whole numbers. This is not required, but can limit misinterpretations.\n\nx$n.I # Events at planned analyses\n#&gt; [1] 119.9993 239.9986 319.9981\n\nHere are the nominal 1-sided \\(p\\)-value bounds which could also be seen with the gsBoundSummary(x) command above.\n\npnorm(-x$upper$bound)\n#&gt; [1] 0.001623978 0.007976344 0.022198916\n\nDoubling these 1-sided bounds, we can compare to bounds from Downs et al. (1997): The group sequential boundary (2-sided, \\(\\alpha = 0.05\\)) for the scheduled analyses is 2.947, 2.411, and 2.011, which correspond to \\(p\\)-values of 0.0032, 0.0159, and 0.0443, respectively. We see the results are nearly identical.\nFrom Downs et al. (1998), we have *After an average follow-up of 5.2 years, lovastatin reduced the incidence of first acute major coronary events (183 vs 116 first events; relative risk [RR], 0.63; 95% confidence interval [CI], 0.50-0.79; P&lt;.001).` This was at the second interim analysis. We see immediately that the 299 events were less than the second planned analysis after 332 events. The bounds are easily adapted in the app. We provide the code generated with the design update provided by the app. We assumed the first interim was performed with the planned 240 events. First, we enter the updated event counts.\n\nn.I &lt;- c(120, 299, 320)\nku &lt;- length(n.I)\n# The following are not needed here\n# They just specify the spending time as information fraction based on\n# observed events (integers) divided by final planned number of events\n# from the design (continuous)\nusTime &lt;- n.I / x$n.I[x$k]\nlsTime &lt;- usTime\n\nBased on the above we can update the design. Note that we are now using gsDesign() rather than gsSurv() as calendar time is no longer relevant.\n\nxu &lt;- gsDesign(\n  # Updated parameters\n  k = ku, # Number of analyses\n  n.I = n.I, # Number of events\n  # Spending time; you don't really need this if you are just using\n  # information (event) fraction\n  usTime = usTime,\n  lsTime = lsTime,\n  # Remaining parameters from original design\n  test.type = x$test.type, alpha = x$alpha, beta = x$beta,\n  astar = x$astar, sfu = x$upper$sf, sfupar = x$upper$param,\n  sfl = x$lower$sf, sflpar = x$lower$param,\n  # maxn.IPlan is key for update to set max planned information\n  maxn.IPlan = x$n.I[x$k],\n  delta = x$delta, delta1 = x$delta1, delta0 = x$delta0\n)\n\nTo inspect the resulting bounds, we run gsBoundSummary() again. Several parameters are useful to get the desired results, as noted in the comments. We note that either the Z or \\(p\\)-value bound is what is needed. For analysis 2, the nominal 1-sided bound 0.0183 has clearly been exceeded by the p &lt; 0.001 reported in the manuscript, confirming statistical significance for the trial. Since we did not know the actual events at interim 1 were not reported, the final bound for the trial may have been slightly different. You can see how much this changes by trying different event counts in the updated design specification in the app.\n\ngsBoundSummary(\n  xu, # Updated design\n  deltaname = \"HR\", # Name of treatment difference measure\n  logdelta = TRUE, # For survival, delta is on a log scale; this transforms\n  Nname = \"Events\", # \"N\" for analyses is events for survival analysis\n  digits = 4, # Decimals for numbers in body of table\n  ddigits = 2, # Decimals for natural parameter; HR in this case\n  tdigits = 1, # Time digits (not needed here)\n  # We select key parameters for printing\n  exclude = c(\n    \"B-value\", \"CP\", \"CP H1\", \"PP\",\n    paste0(\"P(Cross) if HR=\", round(c(hr0, hr), digits = 2))\n  )\n) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Updated AFCAPS bounds at IA 2\")\n\n\n\n\n\n\n\nUpdated AFCAPS bounds at IA 2\n\n\nAnalysis\nValue\nEfficacy\nFutility\n\n\n\n\nIA 1: 38%\nZ\n2.9432\n-2.9432\n\n\nEvents: 120\np (1-sided)\n0.0016\n0.0016\n\n\n\n~HR at bound\n0.5843\n1.7115\n\n\n\nSpending\n0.0016\n0.0016\n\n\nIA 2: 93%\nZ\n2.0908\n-2.0908\n\n\nEvents: 299\np (1-sided)\n0.0183\n0.0183\n\n\n\n~HR at bound\n0.7852\n1.2736\n\n\n\nSpending\n0.0175\n0.0175\n\n\nFinal\nZ\n2.0451\n-2.0451\n\n\nEvents: 320\np (1-sided)\n0.0204\n0.0204\n\n\n\n~HR at bound\n0.7956\n1.2569\n\n\n\nSpending\n0.0059\n0.0059",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportional hazards</span>"
    ]
  },
  {
    "objectID": "proportional-hazards.html#cardiovascular-outcomes-non-inferiority",
    "href": "proportional-hazards.html#cardiovascular-outcomes-non-inferiority",
    "title": "2  Proportional hazards",
    "section": "\n2.4 Cardiovascular outcomes non-inferiority",
    "text": "2.4 Cardiovascular outcomes non-inferiority\nThe EXAMINE trial (White et al. 2013) established non-inferiority for major cardiovascular outcomes (MACE) for the treatment of diabetes using the DPP4 inhibitor alogliptin compared to placebo. The design was described in White et al. (2011). We approximate the design and primary analysis evaluation here. Software and some assumptions of the model were not completely clear, so this is not an exact reproduction of Downs et al. (1997), but is very close. Key assumptions we replicate here:\n\nThe primary analysis evaluates treatment effect using a Cox proportional hazards model of the primary endpoint MACE stratified by geographic region and screening renal function.\n\n1-sided repeated confidence interval for HR at each analysis.\nFocus on analysis to rule out HR \\(\\ge 1.3\\), but also a test for superiority.\nAnalyses planned after 550, 600, 650 MACE events.\nO’Brien-Fleming-like spending function Lan and DeMets (1983).\n2.5% Type I error.\nApproximately 91% power.\n3.5% annual MACE event rate.\nUniform enrollment over 2 years.\n4.75 years trial duration.\n1% annual loss-to-follow-up rate.\nSoftware: EAST 5 (Cytel).\n\n\n\n\nEXAMINEdesign &lt;- gsSurv(\n  k = 3, # Number of analyses\n  test.type = 1, # 1-sided testing\n  timing = c(550, 600) / 650, # Event fractions at interim analyses\n  beta = 1 - 0.90658, # Type II error (1 - power)\n  alpha = .025, # 1-sided Type I error\n  lambdaC = .035, # Exponential failure rate\n  eta = .0108, # Exponential dropout rate\n  sfu = sfLDOF, # O'Brien-Fleming-like spending\n  hr0 = 1.3, # Non-inferiority margin\n  hr = 1, # Alternate hypothesis is equal risk\n  T = 4.75, # Calendar time of final analysis\n  minfup = 2.75 # Minimum follow-up time after enrollment complete\n)\n\ngsBoundSummary(EXAMINEdesign, digits = 4, ddigits = 2, tdigits = 2, timename = \"Year\") %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"EXAMINE non-inferiority design bounds\")\n\n\n\n\n\n\n\nEXAMINE non-inferiority design bounds\n\n\nAnalysis\nValue\nEfficacy\n\n\n\n\nIA 1: 85%\nZ\n2.1748\n\n\nN: 5400\np (1-sided)\n0.0148\n\n\nEvents: 550\n~HR at bound\n1.0799\n\n\nYear: 4.13\nP(Cross) if HR=1.3\n0.0148\n\n\n\nP(Cross) if HR=1\n0.8124\n\n\nIA 2: 92%\nZ\n2.1535\n\n\nN: 5400\np (1-sided)\n0.0156\n\n\nEvents: 600\n~HR at bound\n1.0904\n\n\nYear: 4.44\nP(Cross) if HR=1.3\n0.0197\n\n\n\nP(Cross) if HR=1\n0.8654\n\n\nFinal\nZ\n2.0815\n\n\nN: 5400\np (1-sided)\n0.0187\n\n\nEvents: 650\n~HR at bound\n1.1041\n\n\nYear: 4.75\nP(Cross) if HR=1.3\n0.0250\n\n\n\nP(Cross) if HR=1\n0.9066\n\n\n\n\n\n\n\nFollowing is a plot of expected accrual of subjects and events over time under design assumptions for the alternate hypothesis. You may choose to ignore the code until you need to do a plot like this.\n\naccrual_fn &lt;- function(Time, x) {\n  xx &lt;- nEventsIA(tIA = Time, x = x, simple = FALSE)\n  data.frame(\n    Time = Time,\n    Accrual = c(xx$eNC + xx$eNE, xx$eDC + xx$eDE),\n    Count = c(\"Sample size\", \"Events\")\n  )\n}\naccrual &lt;- data.frame(Time = 0, Accrual = c(0, 0), Count = c(\"Sample size\", \"Events\"))\n\nxtime &lt;- (0:50) / 50 * max(EXAMINEdesign$T)\nfor (i in seq_along(xtime[xtime &gt; 0])) {\n  accrual &lt;- rbind(accrual, accrual_fn(Time = xtime[i + 1], x = EXAMINEdesign))\n}\n\nggplot(accrual, aes(x = Time, y = Accrual, col = Count)) +\n  geom_line() +\n  ylab(\"Expected Accrual\") +\n  ggtitle(\"Expected accrual of enrollment and events for EXAMINE trial\")\n\n\n\n\n\n\n\nFindings for the trial: the primary end point occurred at similar rates in the alogliptin and placebo groups (in 11.3% and 11.8% of patients, respectively, after a median exposure of 18 months; hazard ratio, 0.96; upper boundary of the one-sided repeated CI, 1.16; P&lt;0.001 for noninferiority (White et al. 2013). This was the first interim for testing the non-inferiority hypothesis to rule out an excess risk with HR=1.3. This analysis was performed with 621 events rather than the planned 550. Such an overrun is not atypical when setting a database cutoff prior to final data cleanup and (blinded) endpoint review. In any case, here is the updated design. We enter the events at the interim analysis for the trial. Since gsDesign() requires at least 2 analyses, we also provide the final analysis planned event count even though that was not needed at the time of the interim analysis and could be updated at the time of the final analysis, if needed.\n\nn.I &lt;- c(621, 650)\nku &lt;- length(n.I)\n# This is just specifying event fraction vs final planned\n# is used for computing spending.\n# If calendar spending were specified, calendar fraction\n# would be used here.\nusTime &lt;- n.I / EXAMINEdesign$n.I[x$k]\nlsTime &lt;- usTime\n\nNow we incorporate this to update the original design. As in previous examples, most of the arguments are copied from the original design and we use gsDesign() to update rather than gsSurv() which was used for the original design.\n\nx &lt;- EXAMINEdesign\nxu &lt;- gsDesign(\n  k = ku,\n  test.type = test.type,\n  alpha = x$alpha,\n  beta = x$beta,\n  astar = astar,\n  timing = timing,\n  sfu = sfu,\n  sfupar = sfupar,\n  sfl = sfl,\n  sflpar = sflpar,\n  n.I = n.I,\n  maxn.IPlan = x$n.I[x$k],\n  delta = x$delta,\n  delta1 = x$delta1,\n  delta0 = x$delta0,\n  usTime = usTime,\n  lsTime = lsTime\n)\n\nNext we document the updated bounds. The first code line here is needed for any case where hr0 != 1; i.e. for non-inferiority trials or, in the case of vaccines, super-superiority trials.\n\n# The first line is required to make gsBoundSummary to work correctly\n# for non-inferiority and super-superiority trials\nxu$hr0 &lt;- EXAMINEdesign$hr0\ngsBoundSummary(\n  xu,\n  deltaname = \"HR\",\n  logdelta = TRUE,\n  Nname = \"Events\",\n  digits = 4,\n  ddigits = 2,\n  tdigits = 1,\n  exclude = c(\n    \"B-value\", \"CP\", \"CP H1\", \"PP\"\n  )\n) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"EXAMINE trial design\",\n    subtitle = \"Update at time of analysis\"\n  )\n\n\n\n\n\n\n\nEXAMINE trial design\n\n\nUpdate at time of analysis\n\n\nAnalysis\nValue\nEfficacy\nFutility\n\n\n\n\nIA 1: 96%\nZ\n2.0367\n-2.0367\n\n\nEvents: 621\np (1-sided)\n0.0208\n0.0208\n\n\n\n~HR at bound\n1.1040\n1.5309\n\n\n\nSpending\n0.0208\n0.0208\n\n\n\nP(Cross) if HR=1.3\n0.0208\n0.0208\n\n\n\nP(Cross) if HR=1\n0.8881\n0.0000\n\n\nFinal\nZ\n2.0389\n-2.0389\n\n\nEvents: 650\np (1-sided)\n0.0207\n0.0207\n\n\n\n~HR at bound\n1.1079\n1.5255\n\n\n\nSpending\n0.0042\n0.0042\n\n\n\nP(Cross) if HR=1.3\n0.0250\n0.0250\n\n\n\nP(Cross) if HR=1\n0.9111\n0.0000\n\n\n\n\n\n\n\n\n2.4.1 Exercise\nSee the following link for the Moderna COVID-19 design replication: https://medium.com/@yipeng_39244/reverse-engineering-the-statistical-analyses-in-the-moderna-protocol-2c9fd7544326\nCan you reproduce this using the Shiny interface?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportional hazards</span>"
    ]
  },
  {
    "objectID": "proportional-hazards.html#sec-poisson-mixture",
    "href": "proportional-hazards.html#sec-poisson-mixture",
    "title": "2  Proportional hazards",
    "section": "\n2.5 Cure model",
    "text": "2.5 Cure model\nWe consider a Poisson mixture model De Castro, Cancho, and Rodrigues (2010) which has also been referred to as a transformation cure model Zhang and Shao (2018) or promotion time cure model Zeng, Yin, and Ibrahim (2006). This model can be useful to approximate event accrual in a trial where there is a substantial portion of patients who are expected to have good long-term result. Example applications used to estimate long-term survival are provided by in Hellmann et al. (2017) as well as De Castro, Cancho, and Rodrigues (2010). Because of the nature and uncertainty of the event accumulation, we choose to implement with calendar-based spending Lan and DeMets (1989). This is a choice that would need to be carefully evaluated. Part of the suggested rationale is that the trial needs to be completed in a certain timeframe and the event distribution in the range described over the course of the planned survival distribution should be adequate, and the extreme long-term proportional hazards assumption may be less safe than over a shorter window.\nMuch simpler models such as a 2- or 3-piece piecewise exponential model could be equally effective for a design of this nature, so this is just one example of an approach to a situation of this nature. This is our longest example; a simpler version of this is demonstrated in the app. Readers not immediately interested in the approach can simply skip the rest of this chapter.\n\n2.5.1 Cure model parameters\nThe basic Poisson mixture cure model we consider takes the form\n\\[S(t)= \\exp(-\\theta (1 - \\exp(-\\lambda t)).\\]\nWe note that:\n\n\n\\(1-\\exp(-\\lambda t)\\) is the cumulative distribution function for an exponential distribution and can be replaced by an arbitrary continuous cumulative distribution function.\nAs \\(t\\) approaches \\(\\infty\\), \\(S(t)\\) approaches \\(\\exp(-\\theta)\\) which will be referred to as the cure rate.\n\nGenerally, this can be useful when historical data for a proposed group suggests a plateau in survival after some time. The parameter \\(\\lambda\\) above can be used to determine how quickly events occur, basically setting the bend in the survival curve. For instance, we assume here that historical data suggests a plateau of 0.7 event-free survival for some indication such as adjuvant or neoadjuvant treatment of some cancer. We assume further that by 18 months, the event-free survival is already 0.75. Using the Poisson mixture cure model above, we can back-calculate the parameters as:\n\\[\\theta = -\\log(0.7)\\] \\[\\lambda = -\\log(1 + \\log(0.75) / \\theta) / 18.\\]\nFor a proportional hazards model with hazard ratio \\(HR\\) we can assume control survival \\(S_C(t)=S(t)\\) as above and experimental survival of \\(S_E(t)=S_C(t)^{HR}\\). Assuming \\(HR=0.72\\) we plot assumed survival through 48 months for each treatment group.\n\n# Assumed long-term survival rate\ncureRate &lt;- 0.7\n# Survival rate at specific time\nsurvRate &lt;- 0.75\nsurvRateTime &lt;- 18\n# Maximum time for plot and study duration\nmaxTime &lt;- 60\n# Assumed hazard ratio to power trial\nhr &lt;- 0.72\n\nNext we translate the above inputs into the parameters in the Poison mixture model for the control group.\n\n# Compute theta for Poisson mixture cure rate model\ntheta &lt;- -log(cureRate)\n# Compute rate parameter lambda for Poisson mixture cure rate model\nlambda &lt;- -log(1 + log(survRate) / theta) / survRateTime\n\nNow we plot assumed survival curves under the model parameters over the targeted time period specified above. We use a fairly refined grid to ensure the plot is smooth.\n\nMonth &lt;- 0:maxTime\n# Control survival\nS &lt;- exp(-theta * (1 - exp(-lambda * Month)))\n# Experimental survival\nS_E &lt;- S^hr\n# Put in a data frame and plot\ncure_model &lt;- rbind(\n  data.frame(Treatment = \"Control\", Month = Month, Survival = S),\n  data.frame(Treatment = \"Experimental\", Month = Month, Survival = S_E)\n)\nggplot(cure_model, aes(x = Month, y = Survival, col = Treatment)) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, maxTime, 6)) +\n  scale_y_continuous(breaks = seq(.6, 1, .1), limits = c(.6, 1))\n\n\n\n\n\n\n\nTo perform asymptotic calculations we approximate the survival rates in the above matrix with piecewise constant hazard rates that match the survival rates just computed at discrete points above. Since the cumulative hazard \\(H(t)= - \\log(S(t))\\) and \\(H(t)=\\int_0^t h(s)ds\\) where \\(h()\\) is the hazard rate, we approximate with \\[\\tilde h(t)=(H(t_m)-H(t_{m-1}))/(t_m - t_{m-1}), m=1,\\ldots,M.\\]\n\ncure_model &lt;- cure_model %&gt;%\n  mutate(\n    H = -log(Survival),\n    duration = Month - lag(Month, default = 0),\n    h = (H - lag(H, default = 0) / duration)\n  )\n\n\n2.5.2 Expected event accumulation over time\nEvent accumulation over time can be very sensitive to many trial design assumptions. Generally, we are trying to mimic a slowing of event accumulation over time. We assume the following for relative enrollment rates end enrollment duration.\n\n# Relative enrollment rates\nenrollRates &lt;- 1:4\n# Total duration for each enrollment rate\n# Here they total 18 months planned to complete enrollment\nenrollDurations &lt;- c(2, 2, 2, 12)\n\nWe plot the event accumulation in the above scenario by calendar time post study initiation. We then overlay the percent of final events at 1, 2, 3, 4, 5 and 6 years start of study.\n\n# Calendar times\nti &lt;- seq(0, maxTime, 1) # maxTime is study duration from above\n# Placeholder for expected event counts\nn &lt;- ti\n\n# Begin with putting parameters into a design with no interim analysis\n# This will enable calculation of expected event accumulation\nxx &lt;- nSurv(\n  alpha = 0.025, # 1-sided Type I error\n  beta = NULL, # Here we will NOT compute sample size\n  # Control group exponential failure rate to get median OS of 13 months\n  lambdaC = cure_model$h[-1],\n  # Time period durations for cure model; length 1 less than lambdaC\n  S = cure_model$duration[-c(1, nrow(cure_model))],\n  # Alternate hypothesis hazard ratio\n  hr = hr,\n  # Dropout rate per month (exponential rate)\n  eta = 0.001,\n  # Enrollment rates during ramp-up period\n  gamma = enrollRates,\n  # Relative enrollment rate time period durations\n  R = enrollDurations,\n  # Calendar time of final analysis\n  T = maxTime,\n  # Minimum follow-up time after enrollment complete\n  minfup = maxTime - sum(enrollDurations)\n)\n\nNext we can compute the expected accrual of events over time relative to complete event accrual at the end of the study. This actual number of events and sample size will increase proportionately as long as the relative enrollment rates and durations remain unchanged.\n\nfor (i in seq_along(ti[-1])) {\n  n[i + 1] &lt;- nEventsIA(tIA = ti[i + 1], x = xx)\n}\n# Now do a line plot of % of final events by month\nev &lt;- tibble(Month = ti, ev = n / max(n) * 100)\np &lt;- ggplot(ev, aes(x = Month, y = ev)) +\n  geom_line() +\n  ylab(\"% of Final Events\") +\n  xlab(\"Study Time\") +\n  scale_x_continuous(breaks = seq(0, maxTime, 12)) +\n  scale_y_continuous(breaks = seq(0, 100, 20))\n# Add text overlay at targeted analysis times\nsubti &lt;- c(12, 18, 24, 36, 48, 60)\nsubpct &lt;- n[subti + 1] / n[maxTime + 1] * 100\ntxt &lt;- tibble(Month = subti, ev = subpct, txt = paste(as.character(round(subpct, 1)), \"%\", sep = \"\"))\np + geom_label(data = txt, aes(x = Month, y = ev, label = txt))\n\n\n\n\n\n\n\nWe note that we will use calendar time as above for spending:\n\n# Spending fraction at interim analyses\nusTime &lt;- subti / maxTime\n# Information fraction is required for gsSurv input\n# This was computed above\ntiming &lt;- n[subti + 1] / n[maxTime + 1]\n\n\n2.5.3 The design\nNow we design the trial with both efficacy and futility bounds determined with calendar spending.\n\ncure_model_design &lt;-\n  gsSurv(\n    k = length(timing), # Specify number of analyses\n    alpha = 0.025, # 1-sided Type I error\n    beta = .1, # 1 - .1 = .9 or 90% power\n    # Control group exponential failure rate to get median OS of 13 months\n    lambdaC = cure_model$h[-1],\n    # Time period durations for cure model; length 1 less than lambdaC\n    S = cure_model$duration[-c(1, nrow(cure_model))],\n    # Alternate hypothesis hazard ratio\n    hr = hr,\n    # Dropout rate per month (exponential rate)\n    eta = 0.001,\n    # Enrollment rates during ramp-up period\n    gamma = enrollRates,\n    # Relative enrollment rate time period durations\n    R = enrollDurations,\n    # Calendar time of final analysis\n    T = maxTime,\n    # Minimum follow-up time after enrollment complete\n    minfup = maxTime - sum(enrollDurations),\n    timing = timing, # Information fraction from above\n    usTime = usTime, # Calendar time fraction of analyses from above\n    lsTime = usTime,\n    # Efficacy spending\n    # Lan-DeMets spending function to approximate O'Brien-Fleming design\n    sfu = sfLDOF,\n    # Futility spending\n    sfl = sfHSD, # Hwang-Shih-DeCani (HSD) spending\n    sflpar = -8, # Spending function parameter can be adjusted to get appropriate bound\n    # Specify non-binding futility bounds\n    test.type = 4\n  )\ncure_model_design %&gt;%\n  gsBoundSummary() %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Calendar-based spending\", subtitle = \"Design with cure model\")\n\n\n\n\n\n\n\nCalendar-based spending\n\n\nDesign with cure model\n\n\nAnalysis\nValue\nEfficacy\nFutility\n\n\n\n\nIA 1: 22%\nZ\n4.8769\n-2.1306\n\n\nN: 912\np (1-sided)\n0.0000\n0.9834\n\n\nEvents: 86\n~HR at bound\n0.3487\n1.5844\n\n\nMonth: 12\nP(Cross) if HR=1\n0.0000\n0.0166\n\n\n\nP(Cross) if HR=0.72\n0.0004\n0.0001\n\n\nIA 2: 50%\nZ\n3.9309\n-1.2291\n\n\nN: 1520\np (1-sided)\n0.0000\n0.8905\n\n\nEvents: 196\n~HR at bound\n0.5696\n1.1924\n\n\nMonth: 18\nP(Cross) if HR=1\n0.0000\n0.1144\n\n\n\nP(Cross) if HR=0.72\n0.0502\n0.0003\n\n\nIA 3: 73%\nZ\n3.3710\n-0.4999\n\n\nN: 1520\np (1-sided)\n0.0004\n0.6914\n\n\nEvents: 286\n~HR at bound\n0.6709\n1.0610\n\n\nMonth: 24\nP(Cross) if HR=1\n0.0004\n0.3190\n\n\n\nP(Cross) if HR=0.72\n0.2746\n0.0008\n\n\nIA 4: 92%\nZ\n2.6740\n0.4372\n\n\nN: 1520\np (1-sided)\n0.0037\n0.3310\n\n\nEvents: 361\n~HR at bound\n0.7545\n0.9550\n\n\nMonth: 36\nP(Cross) if HR=1\n0.0038\n0.6720\n\n\n\nP(Cross) if HR=0.72\n0.6696\n0.0040\n\n\nIA 5: 98%\nZ\n2.2540\n1.1568\n\n\nN: 1520\np (1-sided)\n0.0121\n0.1237\n\n\nEvents: 385\n~HR at bound\n0.7945\n0.8887\n\n\nMonth: 48\nP(Cross) if HR=1\n0.0122\n0.8765\n\n\n\nP(Cross) if HR=0.72\n0.8311\n0.0202\n\n\nFinal\nZ\n1.9611\n1.9611\n\n\nN: 1520\np (1-sided)\n0.0249\n0.0249\n\n\nEvents: 392\n~HR at bound\n0.8203\n0.8203\n\n\nMonth: 60\nP(Cross) if HR=1\n0.0250\n0.9750\n\n\n\nP(Cross) if HR=0.72\n0.9000\n0.1000\n\n\n\n\n\n\n\nWe see in this table that only 7 events are expected in the final year under design assumptions. However, it is quite possible that event rate assumptions will be incorrect and that this duration of follow-up is wanted to reasonably evaluate the possibility of a plateau in survival. We note that with 18 months enrollment duration expected, this would give complete follow-up through 42 months at the planned end of the trial.\n\n2.5.4 Updating calendar-based spending bounds at analysis\nWe provide an example where assumptions incorrectly predicted event accrual to show some potential advantages and disadvantages of calendar-based timing. The deviations can be due to enrollment being different than planned, patient event rates different than planned, or treatment effect different than planned. The desire is to have a design that adapts bounds well to a variety of scenarios, allows carrying the trial to the targeted duration, and provides reasonable power. For our example, we assume events accumulate more rapidly than under the design approximation. Analysis times are also assumed to deviate from plan which can be due to operational issues.\n\nactual_events &lt;- c(110, 230, 333, 444, 475, 500)\nactual_analysis_times &lt;- c(11.5, 17, 22.5, 37, 49, 62)\nku &lt;- length(actual_events)\nusTime &lt;- actual_analysis_times / max(cure_model_design$T)\nlsTime &lt;- usTime\n\nWe update by using actual event counts and calendar times of analysis, otherwise copying parameters from the planned design in cure_model_design.\n\ncure_model_update &lt;- gsDesign(\n  k = ku,\n  test.type = cure_model_design$test.type,\n  alpha = cure_model_design$alpha,\n  beta = cure_model_design$beta,\n  astar = cure_model_design$astar,\n  sfu = cure_model_design$upper$sf,\n  sfupar = cure_model_design$upper$param,\n  sfl = cure_model_design$lower$sf,\n  sflpar = cure_model_design$lower$param,\n  n.I = actual_events,\n  maxn.IPlan = cure_model_design$n.I[cure_model_design$k],\n  delta = cure_model_design$delta,\n  delta1 = cure_model_design$delta1,\n  delta0 = cure_model_design$delta0,\n  usTime = usTime,\n  lsTime = lsTime\n)\n\n\ngsBoundSummary(\n  cure_model_update,\n  deltaname = \"HR\",\n  logdelta = TRUE,\n  Nname = \"Events\",\n  digits = 5,\n  ddigits = 2,\n  tdigits = 1,\n  exclude = c(\n    \"B-value\", \"CP\", \"CP H1\", \"PP\",\n    paste0(\"P(Cross) if HR=\", round(c(cure_model_design$hr0, cure_model_design$hr), digits = 2))\n  )\n) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Calendar-based spending design\",\n    subtitle = \"Update at time of analysis\"\n  )\n\n\n\n\n\n\n\nCalendar-based spending design\n\n\nUpdate at time of analysis\n\n\nAnalysis\nValue\nEfficacy\nFutility\n\n\n\n\nIA 1: 28%\nZ\n4.98740\n-1.95090\n\n\nEvents: 110\np (1-sided)\n0.00000\n0.97447\n\n\n\n~HR at bound\n0.38633\n1.45067\n\n\n\nSpending\n0.00000\n0.00012\n\n\nIA 2: 59%\nZ\n4.05365\n-1.07932\n\n\nEvents: 230\np (1-sided)\n0.00003\n0.85978\n\n\n\n~HR at bound\n0.58592\n1.15296\n\n\n\nSpending\n0.00003\n0.00017\n\n\nIA 3: 85%\nZ\n3.49142\n-0.34707\n\n\nEvents: 333\np (1-sided)\n0.00024\n0.63573\n\n\n\n~HR at bound\n0.68205\n1.03877\n\n\n\nSpending\n0.00023\n0.00035\n\n\nIA 4: 113%\nZ\n2.62935\n0.83053\n\n\nEvents: 444\np (1-sided)\n0.00428\n0.20312\n\n\n\n~HR at bound\n0.77914\n0.92420\n\n\n\nSpending\n0.00406\n0.00399\n\n\nIA 5: 121%\nZ\n2.22680\n1.57246\n\n\nEvents: 475\np (1-sided)\n0.01298\n0.05792\n\n\n\n~HR at bound\n0.81518\n0.86563\n\n\n\nSpending\n0.00881\n0.01842\n\n\nFinal\nZ\n1.97178\n1.97178\n\n\nEvents: 500\np (1-sided)\n0.02432\n0.02432\n\n\n\n~HR at bound\n0.83831\n0.83831\n\n\n\nSpending\n0.01187\n0.07696\n\n\n\n\n\n\n\n\n\n\n\nDe Castro, Mario, Vicente G Cancho, and Josemar Rodrigues. 2010. “A Hands-on Approach for Fitting Long-Term Survival Models Under the GAMLSS Framework.” Computer Methods and Programs in Biomedicine 97 (2): 168–77.\n\n\nDowns, John R, Polly A Beere, Edwin Whitney, Michael Clearfield, Stephen Weis, Jeffrey Rochen, Evan A Stein, Deborah R Shapiro, Alexandra Langendorfer, and Antonio M Gotto Jr. 1997. “Design & Rationale of the Air Force/Texas Coronary Atherosclerosis Prevention Study (AFCAPS/TexCAPS).” The American Journal of Cardiology 80 (3): 287–93.\n\n\nDowns, John R, Michael Clearfield, Stephen Weis, Edwin Whitney, Deborah R Shapiro, Polly A Beere, Alexandra Langendorfer, et al. 1998. “Primary Prevention of Acute Coronary Events with Lovastatin in Men and Women with Average Cholesterol Levels: Results of AFCAPS/TexCAPS.” Journal of the American Medical Association 279 (20): 1615–22.\n\n\nGandhi, Leena, Delvys Rodrı́guez-Abreu, Shirish Gadgeel, Emilio Esteban, Enriqueta Felip, Flávia De Angelis, Manuel Domine, et al. 2018. “Pembrolizumab Plus Chemotherapy in Metastatic Non–Small-Cell Lung Cancer.” New England Journal of Medicine 378 (22): 2078–92.\n\n\nHellmann, Matthew David, Junshui Ma, Edward B Garon, Rina Hui, Leena Gandhi, Jean-Charles Soria, Keaven M Anderson, Gregory M Lubiniecki, Bilal Piperdi, and Roy S Herbst. 2017. “Estimating Long-Term Survival of PD-L1-Expressing, Previously Treated, Non-Small Cell Lung Cancer Patients Who Received Pembrolizumab in KEYNOTE-001 and-010.” American Society of Clinical Oncology.\n\n\nHwang, Irving K, Weichung J Shih, and John S De Cani. 1990. “Group Sequential Designs Using a Family of Type I Error Probability Spending Functions.” Statistics in Medicine 9 (12): 1439–45.\n\n\nKim, Kyungmann, and Anastasios A. Tsiatis. 1990. “Study Duration for Clinical Trials with Survival Response and Early Stopping Rule.” Biometrics 46: 81–92.\n\n\nLachin, John M., and Mary A. Foulkes. 1986. “Evaluation of Sample Size and Power for Analyses of Survival with Allowance for Nonuniform Patient Entry, Losses to Follow-up, Noncompliance, and Stratification.” Biometrics 42: 507–19.\n\n\nLan, K. K. G., and David L. DeMets. 1983. “Discrete Sequential Boundaries for Clinical Trials.” Biometrika 70: 659–63.\n\n\n———. 1989. “Group Sequential Procedures: Calendar Versus Information Time.” Statistics in Medicine 8: 1191–98.\n\n\nMaurer, Willi, and Frank Bretz. 2013. “Multiple Testing in Group Sequential Trials Using Graphical Approaches.” Statistics in Biopharmaceutical Research 5: 311–20.\n\n\nSchoenfeld, David. 1981. “The Asymptotic Properties of Nonparametric Tests for Comparing Survival Distributions.” Biometrika 68 (1): 316–19.\n\n\nWhite, William B, George L Bakris, Richard M Bergenstal, Christopher P Cannon, William C Cushman, Penny Fleck, Simon Heller, et al. 2011. “EXamination of cArdiovascular outcoMes with alogliptIN Versus Standard of carE in Patients with Type 2 Diabetes Mellitus and Acute Coronary Syndrome (EXAMINE): A Cardiovascular Safety Study of the Dipeptidyl Peptidase 4 Inhibitor Alogliptin in Patients with Type 2 Diabetes with Acute Coronary Syndrome.” American Heart Journal 162 (4): 620–26.\n\n\nWhite, William B, Christopher P Cannon, Simon R Heller, Steven E Nissen, Richard M Bergenstal, George L Bakris, Alfonso T Perez, et al. 2013. “Alogliptin After Acute Coronary Syndrome in Patients with Type 2 Diabetes.” New England Journal of Medicine 369: 1327–35.\n\n\nZeng, Donglin, Guosheng Yin, and Joseph G Ibrahim. 2006. “Semiparametric Transformation Models for Survival Data with a Cure Fraction.” Journal of the American Statistical Association 101 (474): 670–84.\n\n\nZhang, Yilong, and Yongzhao Shao. 2018. “Concordance Measure and Discriminatory Accuracy in Transformation Cure Models.” Biostatistics 19 (1): 14–26.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Proportional hazards</span>"
    ]
  },
  {
    "objectID": "ahr.html",
    "href": "ahr.html",
    "title": "3  Average hazard ratio",
    "section": "",
    "text": "3.1 Piecewise model\nTo model a time-varying hazard ratio, we consider a model where failure rates for experimental and control groups are piecewise constant. Since this model can use arbitrarily small piecewise intervals, this is not really a restriction. In concept, this is an extension of the piecewise proportional hazards model of Lachin and Foulkes (1986). Suppose the piecewise constant changes at the change points \\(0 = t_0 &lt; t_1 &lt; \\cdots &lt; t_M \\le \\infty\\), and for each individual interval \\((t_{m-1}, t_m]\\) for \\(m = 1, \\ldots, M\\), the hazard ratio is a constant \\(HR_m\\) (experimental:control), i.e.,\n\\[\n  \\text{hazard ratio}\n  =\n  \\left\\{\n  \\begin{array}{ll}\n    HR_1  & \\text{for } t \\in (0, t_1] \\\\\n    HR_2  & \\text{for } t \\in (t_1, t_2] \\\\\n    \\vdots \\\\\n    HR_M  & \\text{for } t \\in (t_{M-1}, t_M] \\\\\n  \\end{array}\n  \\right..\n\\]\nFor any \\(m = 1, \\ldots, M\\),\n\\[\n  HR_m = \\lambda_{1,m} / \\lambda_{0,m},\n\\] where the subscript \\(i\\) indexes the treatment group with \\(i = 0\\) for control arm and \\(i=1\\) for treatment arm. As in Lachin and Foulkes (1986), we will also assume an exponential dropout rate \\(\\eta_{i,m}\\) for treatment arm \\(i=0,1, m=1,\\ldots,M\\). While not necessary, in software implementation in the gsDesign2 and gsdmvn packages, we simplified to \\(\\eta_{0,m}=\\eta_{1,m}=\\eta_m, m=1,\\ldots,M\\).\nWe will denote\n\\[\n\\lambda_{i,m} = e^{\\gamma_{i, m}}.\n\\tag{3.1}\\]\nBy using the delta method, we get the asymptotic distribution of \\(\\widehat\\lambda_{i,m}\\) as \\[\n  \\log(\\widehat\\lambda_{i,m})\n  \\overset{\\cdot}{\\sim}\n  \\text{Normal}\n  \\left(\n    \\log(\\lambda_{i,m}), \\; 1/d_{i,m}\n  \\right),\n  \\;\\;\n  \\forall i \\in \\{0, 1\\}\n\\tag{3.2}\\]\nWith the estimation of \\(\\{\\lambda_{i,m}\\}_{i=0,1 \\text{ and } m = 1, \\ldots, M}\\), it is not complicated to get the estimation and the asymptotic distribution of \\(HR_m\\), which is defined as \\(HR_m = \\lambda_{1,m}/\\lambda_{0,m}\\). In this chapter, we are interested in the logarithm of \\(HR_m\\) and denote it as \\(\\beta_m\\). Recall that\n\\[\n  \\beta_m\n  \\triangleq\n  \\log(HR_m)\n  =\n  \\log\\left( \\frac{\\lambda_{1,m}}{\\lambda_{0,m}} \\right)\n  =\n  \\log(\\lambda_{1,m}) - \\log(\\lambda_{0,m}).\n\\tag{3.3}\\]\nFor both \\(\\lambda_{1,m}\\) and \\(\\lambda_{0,m}\\) above, we know they can be estimated by\n\\[\n\\widehat\\lambda_{i,m} = \\frac{d_{i,m}}{T_{i, m}} \\;\\; i \\in\\{0, 1\\}\n\\]\nwhere \\(d_{0,m}, d_{1,m}\\) are number of events in \\((t_{m-1}, t_m]\\) for group \\(0,1\\), respectively.\nBy plugging the asymptotic distribution of \\(\\{\\lambda_{0, m}, \\lambda_{1,m}\\}\\) in Equation 3.2 into Equation 3.3, we can derive the asymptotic distribution of \\(\\beta_m\\):\n\\[\n  \\widehat\\beta_m\n  \\overset{\\cdot}{\\sim}\n  \\text{Normal}\n  \\left(\n    \\beta_m,\n    \\frac{1}{D_{0m}} + \\frac{1}{D_{1m}}\n  \\right)\n  \\;\\; \\forall m = 1,\\ldots, M .\n\\tag{3.4}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Average hazard ratio</span>"
    ]
  },
  {
    "objectID": "ahr.html#sec-secAhr",
    "href": "ahr.html#sec-secAhr",
    "title": "3  Average hazard ratio",
    "section": "\n3.2 Average hazard ratio",
    "text": "3.2 Average hazard ratio\nIn this section, we define the average hazard ratio (AHR) and use it to derive the asymptotic normal distribution of the logrank test. This is actually a weighted geometric mean of the hazard ratio in piecewise intervals we defined above. Defining \\(\\beta_m=\\log(\\lambda_{1,m}/\\lambda_{0,m})= \\log(\\lambda_{1,m})-\\log(\\lambda_{0,m}), m=1,\\ldots,M\\), we define the logarithm of AHR as a weighted sum of the individual log hazard ratios:\n\\[\n  \\beta\n  =\n  \\sum_{m=1}^M w_m \\log(HR_m)=\\sum_{m=1}^M w_m \\beta_m=\\sum_{m=1}^Mw_m(\\log(\\lambda_{1,m})-\\log(\\lambda_{0,m})).\n\\]\nFor \\(w_m\\) we propose inverse variance weighting which is based on expected number of events expected in each treatment group in \\((t_{m-1},t_m],\\) \\(m=1,\\ldots,M\\). We denote \\(d_{i,m}, T_{i,m},\\) \\(i=0,1,\\) \\(m=1,\\ldots,M\\) the observed events and total time at risk in treatment group \\(i\\) during period \\((t_{m-1},t_m]\\). Thus, for \\(m=1,\\ldots,M\\) we have\n\\[\nw_m=\\frac{(1/E(d_{0,m})+1/E(d_{1,m}))^{-1}}{\\sum_{j=1}^M (1/E(d_{0,j})+1/E(d_{1,j}))^{-1}}.\n\\]\nThe corresponding estimate of \\(\\log(\\lambda_{i,m}), i=0,1, m=1,\\ldots,M\\) is\n\\[\n\\hat{\\gamma}_{i,m}=\\log\\hat\\lambda_{i,m} = \\log(d_{i,m}/T_{i,m})\n\\]\nwhich is asymptotically normal with variance\n\\[\n\\text{Var}(\\hat\\gamma_{i,m})=1/E(d_{i,m})\n\\]\nand variance estimate\n\\[\n\\widehat{\\text{Var}}(\\hat\\gamma_{i,m})=1/d_{i,m}.\n\\]\nWe propose to estimate \\(\\beta\\) using estimated weights under the piecewise model.\n\\[\n  \\tilde\\beta\n  =\n  \\sum_{m=1}^M\n  \\hat{w}_m\n  \\left(\\hat\\gamma_{1,m} - \\hat\\gamma_{0,m}\\right)\n\\tag{3.5}\\]\nFor the selection of weight \\(\\hat{w}_m,\\) \\(m = 1, \\ldots, M\\), we use inverse variance weighting \\[\n  \\hat{w}_m\n  =\n  \\left.\n  \\left(\n    \\frac{1}{1/d_{0,m}+1/d_{1,m}}\n  \\right)^{-1}\n  \\right/\n  \\sum_{i=1}^M\n  \\left(\n    \\frac{1}{1/d_{0,i}+1/d_{1,i}}\n  \\right)^{-1}.\n\\]\nBy plugging the above weights into Equation 3.5, \\(\\beta\\) can be estimated as\n\\[\n  \\tilde\\beta\n  =\n  \\frac{\n    \\sum_{m=1}^M\n    \\left( \\frac{1}{d_{0,m}}+\\frac{1}{d_{1,m}} \\right)^{-1}\n    \\left( \\log(d_{1,m} / T_{1,m}) - \\log(d_{0,m}/T_{0,m}) \\right)\n  }{\n    \\sum_{m=1}^M\n    \\left( \\frac{1}{d_{0,m}}+\\frac{1}{d_{1,m}} \\right)^{-1}\n  }.\n\\]\nThe corresponding variance estimate is:\n\\[\n  \\widehat{\\hbox{Var}}(\\tilde\\beta)\n  =\n  \\left(\\sum_{m=1}^M(1/d_{0,m} + 1/d_{1,m})^{-1}\\right)^{-1}.\n\\]\nBy plugging the asymptotic distribution of \\(\\hat\\beta_m\\) in Equation 3.4, one gets the asymptotic distribution of \\(\\tilde\\beta\\) as\n\\[\n  \\tilde\\beta\n  \\overset{\\cdot}{\\sim}\n  \\hbox{Normal}(\\beta, \\; \\mathcal{I}^{-1}),\n\\]\nwhere\n\\[\n\\mathcal{I} = \\sum_{m = 1}^M \\left( \\frac{1}{E(d_{0,m})} + \\frac{1}{E(d_{1,m})} \\right)^{-1}.\n\\]\nNot shown here is that this is the asymptotic distribution of the logrank test under the piecewise model which follows by the results of Schoenfeld (1981). The details for computing \\(E(d_{i,m}),\\), \\(i=0,1\\), \\(m=1,\\ldots,M\\) under the piecewise model are demonstrated in a vignette in the gsDesign2 package. These computations form the basis for the asymptotic approximations for power and sample size implemented in the gsdmvn package using the functions gsdmvn::gs_power_ahr() and gsdmvn::gs_design_ahr().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Average hazard ratio</span>"
    ]
  },
  {
    "objectID": "ahr.html#examples",
    "href": "ahr.html#examples",
    "title": "3  Average hazard ratio",
    "section": "\n3.3 Examples",
    "text": "3.3 Examples\nIn this section, we introduce 7 examples to help readers:\n\nLearn more about AHR by visualization.\nLearn the calculation of AHR by using the R package gsDesign2 and gsdmvn.\n\n\nlibrary(survival)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(simtrial)\nlibrary(gsDesign)\nlibrary(gsDesign2)\nlibrary(gsdmvn)\n\n\n3.3.1 Example 1\nIn this example, we assume the enrollment rate is a constant with \\(12\\) month targeted enrollment. And we further assume the dropout rate is exponential, i.e., \\(0.001\\) per month. Besides, the failure rate is assumed as exponential with the median of \\(15\\) months. The hazard ratio is assumed as \\(1\\) for the first 4 month, and \\(0.6\\) thereafter.\nTo calculate the AHR, we first use gsDesign2::AHR().\n\n# calculate the AHR\nxx &lt;- gsDesign2::AHR(\n  # the enrollment rate is a constant with $12$ month targeted enrollment\n  enrollRates = tibble::tibble(Stratum = \"All\", duration = 12, rate = 100 / 12),\n  failRates = tibble::tibble(\n    Stratum = \"All\",\n    # failure rate is assumed as exponential with the median of 15 months\n    failRate = log(2) / 15,\n    # hazard ratio is assumed as 1 for the first 4 month, and 0.6 thereafter.\n    hr = c(1, 0.6),\n    duration = c(4, 100),\n    # dropout rate is exponential, i.e., 0.001 per month\n    dropoutRate = rep(0.001, 2)\n  ),\n  # total follow-up from start of enrollment to data cutoff\n  totalDuration = c(.001, 4, 4.1, 4.25, 4.5, 5, seq(6, 36, 1)),\n  # ratio of experimental to control randomization\n  ratio = 1\n)\n\nIts AHR plot can be found below, where its AHR steeps drop after \\(4\\) months and leveling after about \\(24\\) months.\n\n# plot the AHR curve\nggplot(xx, aes(x = Time, y = AHR)) +\n  geom_line(linewidth = 2) +\n  ggtitle(\"AHR (Geometric Average Hazard Ratio) Over Time\") +\n  scale_x_continuous(breaks = seq(0, 36, 6)) +\n  ylab(\"AHR\") +\n  xlab(\"Month\") +\n  annotate(\n    geom = \"text\", x = 20, y = .9, size = 6,\n    label = \"Steep drop after 4 months\\n leveling after about 24 months\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = 14, colour = \"black\"),\n    axis.title = element_text(size = 18, face = \"bold\"),\n    title = element_text(size = 16)\n  )\n\n\n\n\n\n\n\nThen, we are interested to figure out the expected events per \\(100\\) enrolled and find out that we need \\(35\\)-\\(40\\) months until \\(65\\)%-\\(70\\)% have events.\n\n# plot AHR\nggplot(\n  xx %&gt;%\n    group_by(Time) %&gt;%\n    summarize(Events = sum(Events)),\n  aes(x = Time, y = Events)\n) +\n  geom_line(linewidth = 2) +\n  scale_y_continuous(breaks = seq(0, 100, 10)) +\n  annotate(\n    geom = \"text\", x = 28, y = 20, size = 6,\n    label = \"Need 35-40 months until\\n 65%-70% have events\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 42, 6)) +\n  ggtitle(\"Expected Events per 100 Enrolled\") +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = 18, colour = \"black\"),\n    axis.title = element_text(size = 18, face = \"bold\"),\n    title = element_text(size = 18)\n  ) +\n  xlab(\"Month\")\n\n\n\n\n\n\n\n\n3.3.2 Example 2\nIn this example, we assume there are \\(332\\) events and investigate the correlation between AHR and power. For AHR, we are interested in the range from \\(0.6\\) to \\(1\\) with intervals of \\(0.02\\). For power, it can be calculated by gsDesign::nEvents(). The correlation between AHR and power can be found in the following figure.\n\nggplot(\n  tibble(\n    # Interested in the range from 0.6 to 1 with intervals of 0.02\n    AHR = seq(.6, 1, .02),\n    # Power can be calculated by gsDesign::nEvents()\n    Power = gsDesign::nEvents( # assume there are 332 events\n      n = 332,\n      # If beta = NULL and n = number of events,\n      # then power is computed instead of events required\n      beta = NULL,\n      hr = seq(.6, 1, .02)\n    )\n  ),\n  aes(x = AHR, y = Power)\n) +\n  geom_line(linewidth = 2) +\n  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, .2)) +\n  ggtitle(\"Power by AHR, 332 Events, alpha=0.025, 1-sided\") +\n  annotate(\n    geom = \"text\", x = .7, y = .3, size = 6,\n    label = \"Steep power decrease\\n with increasing AHR\"\n  ) +\n  annotate(\n    geom = \"text\", x = .88, y = .83, size = 6,\n    label = \"Ensure follow-up sufficient\\n to capture meaningful AHR\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = 18, colour = \"black\"),\n    axis.title = element_text(size = 18, face = \"bold\"),\n    title = element_text(size = 16)\n  )\n\n\n\n\n\n\n\n\n3.3.3 Example 3\nIn this example, we discuss the group sequential design with spending bounds. And we will show how to calculate the interim timing, final timing, effect size, and information by gsDesign2::AHR(). We assume the analysis is conducted after \\(12, 20, 28, 36\\) months and the sample size is \\(500\\). we further assume the enrollment rate is a constant with \\(12\\) month targeted enrollment. And we further assume the dropout rate is exponential, i.e., \\(0.001\\) per month. Besides, the failure rate is assumed as exponential with the median of \\(15\\) months. The hazard ratio is assumed as \\(1\\) for the first \\(4\\) month, and \\(0.6\\) thereafter.\n\n# Assume the analysis is conducted after 12, 20, 28, 36 months\nanalysisTimes &lt;- c(12, 20, 28, 36)\n# Sample size is 500\nsampleSize &lt;- 500\n# Enrollment rates\nenrollRates &lt;- tibble(\n  Stratum = \"All\",\n  # Assume the enrollment rate is a constant with\n  # 12 month targeted enrollment.\n  duration = 12,\n  rate = sampleSize / 12\n)\n# Failure rates\nfailRates &lt;- tibble(\n  Stratum = \"All\",\n  # Failure rate is assumed as exponential with the median of 15 months.\n  failRate = log(2) / 15,\n  # Hazard ratio is assumed as $1$ for the first 4 month, and $0.6$ thereafter.\n  hr = c(1, .6),\n  duration = c(4, 100),\n  # Dropout rate is exponential, i.e., 0.001 per month\n  dropoutRate = 0.001\n)\n\nGiven the above setting, we can calculate the interim timing, final timing, effect size, and information by gsDesign2::AHR().\n\nahr &lt;- gsDesign2::AHR(\n  enrollRates = enrollRates,\n  failRates = failRates,\n  totalDuration = analysisTimes,\n  ratio = 1\n) %&gt;% mutate(timing = c(info0[1:3] / info0[4], 1))\n\nahr %&gt;%\n  gt() %&gt;%\n  fmt_number(col = 2, decimals = 2) %&gt;%\n  fmt_number(col = 3:6, decimals = 2)\n\n\n\n\n\n\nTime\nAHR\nEvents\ninfo\ninfo0\ntiming\n\n\n\n12\n0.84\n107.39\n26.37\n26.85\n0.32\n\n\n20\n0.74\n207.90\n50.67\n51.97\n0.63\n\n\n28\n0.70\n279.10\n68.23\n69.78\n0.84\n\n\n36\n0.68\n331.29\n81.38\n82.82\n1.00\n\n\n\n\n\n\n\n\n3.3.4 Example 4\nIn this example, we compare proportional hazard with non-proportional hazard, regarding of the sample size and crossing probability. In this example, we consider the one-sided design. We will discuss the other designs in the next few sections, including symmetric design in Section @ref(SecAhrEgPhNphSymmetric), asymmetric design in Section @ref(SecAhrEgPhNphAsymmetric) and design with interims at specified times in Section @ref(SecAhrEgPhNphSpecStop). For the calculation of proportional hazard, we use R function gsDesign::gsSurv(). For the calculation of non-proportional hazard, we use R function gsdmvn::gs_design_ahr().\nFirst, we calculate the sample size and crossing probability under proportional hazard. And we find the sample size is \\(444\\) with \\(297\\) events. The crossing probabilities at the first, second, third and final interim analysis are \\(0.0289, 0.4999, 0.7916, 0.9000\\) under \\(H_1\\).\n\n# Derive Group Sequential Design\nPH1sided &lt;- gsDesign::gsSurv(\n  # Number of analyses (interim + final)\n  k = 4,\n  # use this for 1-sided testing\n  test.type = 1,\n  # 1-sided Type I error\n  alpha = 0.025,\n  # Type II error (1 - power)\n  beta = 0.1,\n  # Information fraction for interim\n  timing = ahr$timing,\n  # O'Brien-Fleming spending approximation\n  sfu = sfLDOF,\n  # Piecewise control failure rates\n  lambdaC = failRates$failRate,\n  # Used final analysis AHR\n  hr = ahr$AHR[4],\n  # Piecewise exponential dropout rates\n  eta = failRates$dropoutRate,\n  # Relative enrollment\n  gamma = enrollRates$rate,\n  # Duration of piecewise enrollment rates\n  R = enrollRates$duration,\n  # Duration of piecewise failure rates (K-1)\n  S = failRates$duration[1],\n  # Study duration\n  T = max(analysisTimes),\n  # Minimum follow-up\n  minfup = max(analysisTimes) - sum(enrollRates$duration),\n  # Experimental:Control randomization ratio\n  ratio = 1\n)\n\ngsBoundSummary(PH1sided) %&gt;% gt()\n\n\n\n\n\n\nAnalysis\nValue\nEfficacy\n\n\n\nIA 1: 32%\nZ\n3.7670\n\n\nN: 444\np (1-sided)\n0.0001\n\n\nEvents: 97\n~HR at bound\n0.4636\n\n\nMonth: 13\nP(Cross) if HR=1\n0.0001\n\n\n\nP(Cross) if HR=0.68\n0.0289\n\n\nIA 2: 63%\nZ\n2.6020\n\n\nN: 444\np (1-sided)\n0.0046\n\n\nEvents: 186\n~HR at bound\n0.6828\n\n\nMonth: 21\nP(Cross) if HR=1\n0.0047\n\n\n\nP(Cross) if HR=0.68\n0.4999\n\n\nIA 3: 84%\nZ\n2.2209\n\n\nN: 444\np (1-sided)\n0.0132\n\n\nEvents: 250\n~HR at bound\n0.7549\n\n\nMonth: 28\nP(Cross) if HR=1\n0.0146\n\n\n\nP(Cross) if HR=0.68\n0.7916\n\n\nFinal\nZ\n2.0453\n\n\nN: 444\np (1-sided)\n0.0204\n\n\nEvents: 297\n~HR at bound\n0.7885\n\n\nMonth: 36\nP(Cross) if HR=1\n0.0250\n\n\n\nP(Cross) if HR=0.68\n0.9000\n\n\n\n\n\n\n\nSecond, we calculate the sample size and crossing probability under non-proportional hazard. And we find the sample size is \\(465\\) with \\(308\\) events (you will want to round up events and sample size). The crossing probabilities at the first, second, third and final interim analysis are \\(0.0019, 0.3024, 0.7329, 0.9000\\) under \\(H_1\\).\n\n# Spending function setup\nNPH1sided &lt;- gs_design_ahr(\n  enrollRates = enrollRates,\n  failRates = failRates,\n  ratio = 1, alpha = .025, beta = 0.1,\n  # Information fraction not required (but available!)\n  analysisTimes = analysisTimes,\n  # Function to enable spending bound\n  upper = gs_spending_bound,\n  # Spending function and parameters used\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),\n  # Lower bound fixed at -infinity\n  lower = gs_b, # allows input of fixed bound\n  # With gs_b, just enter values for bounds\n  lpar = rep(-Inf, 4)\n)\n\nNPH1sided$bounds %&gt;%\n  filter(Bound == \"Upper\") %&gt;%\n  gt() %&gt;%\n  fmt_number(col = c(1, 3), decimals = 0) %&gt;%\n  fmt_number(col = c(4:5, 10:11), decimals = 1) %&gt;%\n  fmt_number(col = 6:7, decimals = 4) %&gt;%\n  fmt_number(col = 8:9, decimals = 3)\n\n\n\n\n\n\nAnalysis\nBound\nTime\nN\nEvents\nZ\nProbability\nAHR\ntheta\ninfo\ninfo0\n\n\n\n1\nUpper\n12\n464.3\n99.7\n3.7670\n0.0019\n0.840\n0.175\n24.5\n24.9\n\n\n2\nUpper\n20\n464.3\n193.0\n2.6020\n0.3024\n0.738\n0.304\n47.0\n48.3\n\n\n3\nUpper\n28\n464.3\n259.2\n2.2209\n0.7329\n0.700\n0.357\n63.3\n64.8\n\n\n4\nUpper\n36\n464.3\n307.6\n2.0453\n0.9000\n0.683\n0.381\n75.6\n76.9\n\n\n\n\n\n\n\nComparing the proportional hazard with non-proportional hazard, we find that\n\nInterim boundary crossing probability much lower than with proportional hazard bounds.\nSample size larger than for proportional hazard.\n\n3.3.5 Example 5\nIn this example, we compare proportional hazard with non-proportional hazard. Here, we consider the symmetric design: \\[\n  \\begin{align}\n  f(s_k,\\alpha)-f(s_{k-1},\\alpha)\n  =& P_0(\\{Z_{k}\\geq b_{k}(\\alpha)\\}\\cap_{j=1}^{k-1}\\{-b_{j}(\\alpha)&lt; Z_{j}&lt; b_{j}(\\alpha)\\}\\\\\n  =& P_0(\\{Z_{k}\\le -b_{k}(\\alpha)\\}\\cap_{j=1}^{k-1}\\{-b_{j}(\\alpha)&lt; Z_{j}&lt; b_{j}(\\alpha)\\}\n  \\end{align}.\n\\] The common practice is to use binding upper and lower bounds. In this example, we use two one-sided tests for \\(\\alpha-\\)spending.\nFirst, we calculate the sample size and crossing probability under proportional hazard by gsDesign::gsSurv(). And we find the sample size is \\(444\\) with \\(297\\) events. The probabilities to cross the upper boundary at the first, second, third and final interim analysis are \\(0.0289, 0.4999, 0.7916, 0.9000\\) under \\(H_1\\). The probabilities to cross the lower boundary at the first, second, third and final interim analysis are \\(0.0001, 0.0047, 0.0146, 0.0250\\) under \\(H_0\\).\n\n\n\n\n\n\n\nAnalysis\nValue\nEfficacy\nFutility\n\n\n\nIA 1: 32%\nZ\n3.7670\n-3.7670\n\n\nN: 444\np (1-sided)\n0.0001\n0.0001\n\n\nEvents: 97\n~HR at bound\n0.4636\n2.1569\n\n\nMonth: 13\nP(Cross) if HR=1\n0.0001\n0.0001\n\n\n\nP(Cross) if HR=0.68\n0.0289\n0.0000\n\n\nIA 2: 63%\nZ\n2.6020\n-2.6020\n\n\nN: 444\np (1-sided)\n0.0046\n0.0046\n\n\nEvents: 186\n~HR at bound\n0.6828\n1.4646\n\n\nMonth: 21\nP(Cross) if HR=1\n0.0047\n0.0047\n\n\n\nP(Cross) if HR=0.68\n0.4999\n0.0000\n\n\nIA 3: 84%\nZ\n2.2209\n-2.2209\n\n\nN: 444\np (1-sided)\n0.0132\n0.0132\n\n\nEvents: 250\n~HR at bound\n0.7549\n1.3246\n\n\nMonth: 28\nP(Cross) if HR=1\n0.0146\n0.0146\n\n\n\nP(Cross) if HR=0.68\n0.7916\n0.0000\n\n\nFinal\nZ\n2.0453\n-2.0453\n\n\nN: 444\np (1-sided)\n0.0204\n0.0204\n\n\nEvents: 297\n~HR at bound\n0.7885\n1.2682\n\n\nMonth: 36\nP(Cross) if HR=1\n0.0250\n0.0250\n\n\n\nP(Cross) if HR=0.68\n0.9000\n0.0000\n\n\n\n\n\n\n\nSecond, we calculate the sample size and crossing probability under non-proportional hazard by gsdmvn::gs_design_ahr(). And we find the sample size is \\(465\\) with \\(308\\) events (you will want to round up events and sample size). The probabilities to cross the upper boundaries at the first, second, third and final interim analysis are \\(0.0019, 0.3024, 0.7329, 0.9000\\) under \\(H_1\\). The probabilities to cross the lower boundaries at the first, second, third and final interim analysis are \\(0.0000, 0.0000, 0.0000, 0.0000\\) under \\(H_0\\).\n\nNPHsymmetric &lt;- gsdmvn::gs_design_ahr(\n  enrollRates = enrollRates,\n  failRates = failRates,\n  ratio = 1, alpha = .025, beta = 0.1,\n  # Information fraction not required (but available!)\n  analysisTimes = analysisTimes,\n  # Function to enable spending bound\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  # Spending function and parameters used\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),\n  binding = TRUE, # set lower bound to binding\n  h1_spending = FALSE\n)\nNPHsymmetric$bounds %&gt;%\n  gt() %&gt;%\n  fmt_number(col = c(1, 3), decimals = 0) %&gt;%\n  fmt_number(col = c(4:5, 10:11), decimals = 1) %&gt;%\n  fmt_number(col = 6:7, decimals = 4) %&gt;%\n  fmt_number(col = 8:9, decimals = 3)\n\n\n\n\n\n\nAnalysis\nBound\nTime\nN\nEvents\nZ\nProbability\nAHR\ntheta\ninfo\ninfo0\n\n\n\n1\nUpper\n12\n464.3\n99.7\n3.7670\n0.0019\n0.840\n0.175\n24.5\n24.9\n\n\n2\nUpper\n20\n464.3\n193.0\n2.6020\n0.3024\n0.738\n0.304\n47.0\n48.3\n\n\n3\nUpper\n28\n464.3\n259.2\n2.2209\n0.7329\n0.700\n0.357\n63.3\n64.8\n\n\n4\nUpper\n36\n464.3\n307.6\n2.0453\n0.9000\n0.683\n0.381\n75.6\n76.9\n\n\n1\nLower\n12\n464.3\n99.7\n−3.7670\n0.0000\n0.840\n0.175\n24.5\n24.9\n\n\n2\nLower\n20\n464.3\n193.0\n−2.6020\n0.0000\n0.738\n0.304\n47.0\n48.3\n\n\n3\nLower\n28\n464.3\n259.2\n−2.2209\n0.0000\n0.700\n0.357\n63.3\n64.8\n\n\n4\nLower\n36\n464.3\n307.6\n−2.0453\n0.0000\n0.683\n0.381\n75.6\n76.9\n\n\n\n\n\n\n\n\n3.3.6 Example 6\nIn this example, we compare proportional hazard with non-proportional hazard. Here, we consider the asymmetric design. For upper boundary, we use non-binding upper bound with spending function \\(f_1(s,\\alpha)\\). For lower boundary, we use binding lower bound with spending function \\(f_2(s,\\gamma)\\) for some chosen \\(0 &lt; \\gamma \\le 1-\\alpha\\), where \\(\\gamma\\) is the type II error. The boundaries are set to satisfy \\[\n  \\begin{align}\n  f_1(s_k,\\alpha)-f_1(s_{k-1},\\alpha)\n  & = Pr(\\{Z_{k}\\geq b_{k}(\\alpha)\\}\\cap_{j=1}^{k-1}\\{Z_{j}&lt; b_{j}(\\alpha) \\;|\\; H_0\\} )\\\\\n  f_2(s_k,\\gamma)-f_2(s_{k-1},\\gamma)\n  & = Pr(\\{Z_{k}&lt; a_{k}(\\gamma)\\}\\cap_{j=1}^{k-1}\\{a_{j}(\\gamma)\\le Z_{j}&lt; b_{j}(\\alpha)  \\;|\\; H_1\\})\n  \\end{align}\n\\] For the last look, \\(K\\)-th look, generally, it is set as \\(a_K = b_K\\).\nFirst, we calculate the sample size and crossing probability under proportional hazard by gsDesign::gsSurv(). And we find the sample size is \\(476\\) with \\(319\\) events. The probabilities to cross the upper boundary at the first, second, third and final interim analysis are \\(0.0338, 0.5385, 0.8185, 0.9000\\) under \\(H_1\\). The probabilities to cross the lower boundary at the first, second, third and final interim analysis are \\(0.0143, 0.0393, 0.0687, 0.1000\\) under \\(H_1\\).\n\n# Derive Group Sequential Design\nPHasymmetric &lt;- gsDesign::gsSurv(\n  # Number of analyses (interim + final)\n  k = 4,\n  # ONLY CHANGED FROM BEFORE\n  # non-binding futility bound\n  test.type = 4,\n  # 1-sided Type I error\n  alpha = 0.025,\n  # Type II error (1 - power)\n  beta = 0.1,\n  # Information fraction for interims\n  timing = ahr$timing,\n  # O'Brien-Fleming spending approximation\n  sfu = sfLDOF,\n  # Hwang-Shih-DeCani futility spending function\n  sfl = sfHSD,\n  # Accelerate early spending somewhat\n  sflpar = -2,\n  # Piecewise control failure rates\n  lambdaC = failRates$failRate,\n  # Alternate hypothesis HR\n  hr = ahr$AHR[4],\n  # Piecewise exponential dropout rates\n  eta = failRates$dropoutRate,\n  # Relative enrollment\n  gamma = enrollRates$rate,\n  # Duration of piecewise enrollment rates\n  R = enrollRates$duration,\n  # Duration of piecewise failure rates (K-1)\n  S = failRates$duration[1],\n  # Study duration\n  T = max(analysisTimes),\n  # Minimum follow-up\n  minfup = max(analysisTimes) - sum(enrollRates$duration),\n  # Experimental:Control randomization ratio\n  ratio = 1\n)\n\ngsBoundSummary(PHasymmetric) %&gt;% gt()\n\n\n\n\n\n\nAnalysis\nValue\nEfficacy\nFutility\n\n\n\nIA 1: 32%\nZ\n3.7670\n-0.2503\n\n\nN: 476\np (1-sided)\n0.0001\n0.5988\n\n\nEvents: 104\n~HR at bound\n0.4767\n1.0505\n\n\nMonth: 13\nP(Cross) if HR=1\n0.0001\n0.4012\n\n\n\nP(Cross) if HR=0.68\n0.0338\n0.0143\n\n\nIA 2: 63%\nZ\n2.6020\n0.8440\n\n\nN: 476\np (1-sided)\n0.0046\n0.1993\n\n\nEvents: 201\n~HR at bound\n0.6922\n0.8875\n\n\nMonth: 21\nP(Cross) if HR=1\n0.0047\n0.8103\n\n\n\nP(Cross) if HR=0.68\n0.5385\n0.0393\n\n\nIA 3: 84%\nZ\n2.2209\n1.5151\n\n\nN: 476\np (1-sided)\n0.0132\n0.0649\n\n\nEvents: 269\n~HR at bound\n0.7626\n0.8312\n\n\nMonth: 28\nP(Cross) if HR=1\n0.0144\n0.9414\n\n\n\nP(Cross) if HR=0.68\n0.8185\n0.0687\n\n\nFinal\nZ\n2.0453\n2.0453\n\n\nN: 476\np (1-sided)\n0.0204\n0.0204\n\n\nEvents: 319\n~HR at bound\n0.7953\n0.7953\n\n\nMonth: 36\nP(Cross) if HR=1\n0.0225\n0.9775\n\n\n\nP(Cross) if HR=0.68\n0.9000\n0.1000\n\n\n\n\n\n\n\nSecond, we calculate the sample size and crossing probability under non-proportional hazard by gsdmvn::gs_design_ahr(). And we find the sample size is \\(502\\) with \\(333\\) events (you will want to round up events and sample size). The probabilities to cross the upper boundaries at the first, second, third and final interim analysis are \\(0.0021, 0.3318, 0.7660, 0.9000\\) under \\(H_1\\). The probabilities to cross the lower boundaries at the first, second, third and final interim analysis are \\(0.0143, 0.0387, 0.0681, 0.1000\\) under \\(H_0\\).\n\nNPHasymmetric &lt;- gsdmvn::gs_design_ahr(\n  enrollRates = enrollRates,\n  failRates = failRates,\n  ratio = 1, alpha = .025, beta = 0.1,\n  # Information fraction not required (but available!)\n  analysisTimes = analysisTimes,\n  # Function to enable spending bound\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  # Spending function and parameters used\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),\n  lpar = list(sf = gsDesign::sfHSD, total_spend = .1, param = -2)\n)\n\nNPHasymmetric$bounds %&gt;%\n  gt() %&gt;%\n  fmt_number(col = c(1, 3), decimals = 0) %&gt;%\n  fmt_number(col = c(4:5, 10:11), decimals = 1) %&gt;%\n  fmt_number(col = 6:7, decimals = 4) %&gt;%\n  fmt_number(col = 8:9, decimals = 3)\n\n\n\n\n\n\nAnalysis\nBound\nTime\nN\nEvents\nZ\nProbability\nAHR\ntheta\ninfo\ninfo0\n\n\n\n1\nUpper\n12\n501.8\n107.8\n3.7670\n0.0021\n0.840\n0.175\n26.5\n26.9\n\n\n2\nUpper\n20\n501.8\n208.6\n2.6020\n0.3318\n0.738\n0.304\n50.9\n52.2\n\n\n3\nUpper\n28\n501.8\n280.1\n2.2209\n0.7660\n0.700\n0.357\n68.5\n70.0\n\n\n4\nUpper\n36\n501.8\n332.5\n2.0453\n0.9000\n0.683\n0.381\n81.7\n83.1\n\n\n1\nLower\n12\n501.8\n107.8\n−1.2899\n0.0143\n0.840\n0.175\n26.5\n26.9\n\n\n2\nLower\n20\n501.8\n208.6\n0.3054\n0.0387\n0.738\n0.304\n50.9\n52.2\n\n\n3\nLower\n28\n501.8\n280.1\n1.3340\n0.0681\n0.700\n0.357\n68.5\n70.0\n\n\n4\nLower\n36\n501.8\n332.5\n2.0453\n0.1000\n0.683\n0.381\n81.7\n83.1\n\n\n\n\n\n\n\nThis does important adjustment to futility bounds based on possibly delayed effect! - Note IA 1 futility bound under PH with gsDesign(): \\(Z=-0.25\\)\n\n3.3.7 Example 7\nIn this example, we discuss the design with interims at specified times, say, there are futility boundary only at the first interim analysis (look for \\(p=0.05\\) in the wrong direction) and there are efficacy boundaries only after the first interim. This is a variation on asymmetric design. It should be noted that it is not easily done with gsDesign::gsSurv(), and cannot be done (at least not easily) with gsDesign package. So, we only discuss the implementation of gsdmvn::gs_design_ahr() for design with interims at specified times. We will use information fraction instead of calendar times of analysis.\n\nNPHskip &lt;- gsdmvn::gs_design_ahr(\n  enrollRates = enrollRates,\n  failRates = failRates,\n  ratio = 1, alpha = .025, beta = 0.1,\n  # Information fraction not required (but available!)\n  analysisTimes = analysisTimes,\n  # Upper spending bound\n  upper = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),\n  # Skip first efficacy analysis\n  test_upper = c(FALSE, TRUE, TRUE, TRUE),\n  # Spending function and parameters used\n  lower = gs_b,\n  lpar = c(qnorm(.05), rep(-Inf, 3))\n)\n\nNPHskip$bounds %&gt;%\n  filter(abs(Z) &lt; Inf) %&gt;% # Throw out infinite bounds\n  arrange(Analysis) %&gt;% # Arrange by analysis\n  gt() %&gt;%\n  fmt_number(columns = c(1, 3), decimals = 0) %&gt;%\n  fmt_number(columns = c(4:5, 10:11), decimals = 1) %&gt;%\n  fmt_number(columns = 6:7, decimals = 4) %&gt;%\n  fmt_number(columns = 8:9, decimals = 3)\n\n\n\n\n\n\nAnalysis\nBound\nTime\nN\nEvents\nZ\nProbability\nAHR\ntheta\ninfo\ninfo0\n\n\n\n1\nLower\n12\n467.6\n100.4\n−1.6449\n0.0060\n0.840\n0.175\n24.7\n25.1\n\n\n2\nUpper\n20\n467.6\n194.4\n2.5999\n0.3057\n0.738\n0.304\n47.4\n48.6\n\n\n3\nUpper\n28\n467.6\n261.0\n2.2207\n0.7359\n0.700\n0.357\n63.8\n65.3\n\n\n4\nUpper\n36\n467.6\n309.8\n2.0452\n0.9000\n0.683\n0.381\n76.1\n77.5\n\n\n\n\n\n\n\n\n\n\n\nLachin, John M., and Mary A. Foulkes. 1986. “Evaluation of Sample Size and Power for Analyses of Survival with Allowance for Nonuniform Patient Entry, Losses to Follow-up, Noncompliance, and Stratification.” Biometrics 42: 507–19.\n\n\nSchoenfeld, David. 1981. “The Asymptotic Properties of Nonparametric Tests for Comparing Survival Distributions.” Biometrika 68 (1): 316–19.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Average hazard ratio</span>"
    ]
  },
  {
    "objectID": "pkgs.html",
    "href": "pkgs.html",
    "title": "4  Introduction to gsdmvn, gsDesign2, and simtrial",
    "section": "",
    "text": "4.1 simtrial\nThe R package simtrial targets on time-to-event trial simulation under the piecewise model. It uses logrank and weighted logrank for analysis. It can simulate both fixed and group sequential design, also potential to simulate adaptive design. Its validation is near completion (thanks to AP colleagues and Amin Shirazi).\nIn simtrial, there are several functions to generate simulated datasets:\nThere are also functions to cut data for analysis:\nMost importantly, there are functions for analysis:\nIn simtrial, there are reverse engineered datasets:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to gsdmvn, gsDesign2, and simtrial</span>"
    ]
  },
  {
    "objectID": "pkgs.html#simtrial",
    "href": "pkgs.html#simtrial",
    "title": "4  Introduction to gsdmvn, gsDesign2, and simtrial",
    "section": "",
    "text": "simfix(): Simulation of fixed sample size design for time-to-event endpoint\n\nsimfix2simPWSurv(): Conversion of enrollment and failure rates from simfix() to simPWSurv() format\n\nsimPWSurv(): Simulate a stratified time-to-event outcome randomized trial\n\n\n\n\ncutData(): Cut a dataset for analysis at a specified date\n\ncutDataAtCount(): Cut a dataset for analysis at a specified event count\n\ngetCutDateForCount(): Get date at which an event count is reached\n\n\n\n\ntenFH(): Fleming-Harrington weighted logrank tests\n\ntenFHcorr(): Fleming-Harrington weighted logrank tests plus correlations\n\ntensurv(): Process survival data into counting process format\n\npMaxCombo(): MaxCombo p-value\n\npwexpfit(): Piecewise exponential survival estimation\n\nwMB(): Magirr and Burman modestly weighted logrank tests\n\n\n\n\nEx1delayedEffect: Time-to-event data example 1 for non-proportional hazards working group\n\nEx2delayedEffect: Time-to-event data example 2 for non-proportional hazards working group\n\nEx3curewithph: Time-to-event data example 3 for non-proportional hazards working group\n\nEx4belly: Time-to-event data example 4 for non-proportional hazards working group\n\nEx5widening: Time-to-event data example 5 for non-proportional hazards working group\n\nEx6crossing: Time-to-event data example 6 for non-proportional hazards working group\n\nMBdelayed: Simulated survival dataset with delayed treatment effect",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to gsdmvn, gsDesign2, and simtrial</span>"
    ]
  },
  {
    "objectID": "pkgs.html#gsdesign2",
    "href": "pkgs.html#gsdesign2",
    "title": "4  Introduction to gsdmvn, gsDesign2, and simtrial",
    "section": "\n4.2 gsDesign2",
    "text": "4.2 gsDesign2\nThe R package gsDesign2 has the main functions listed as follows.\n\n\nAHR(): Average hazard ratio under non-proportional hazards\n\neAccrual(): Piecewise constant expected accrual\n\neEvents_df(): Expected events observed under piecewise exponential model\n\nppwe(): Estimate piecewise exponential cumulative distribution function\n\ns2pwe(): Approximate survival distribution with piecewise exponential distribution\n\ntEvents(): Predict time at which a targeted event count is achieved",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to gsdmvn, gsDesign2, and simtrial</span>"
    ]
  },
  {
    "objectID": "pkgs.html#gsdmvn",
    "href": "pkgs.html#gsdmvn",
    "title": "4  Introduction to gsdmvn, gsDesign2, and simtrial",
    "section": "\n4.3 gsdmvn",
    "text": "4.3 gsdmvn\nThe R package gsdmvn extends the Jennison and Turnbull (2000) computational model to non-constant treatment effects. The bound supports functions include\n\n\ngs_b(): direct input of bounds;\n\ngs_spending_bound(): spending function bounds.\n\nBesides, it covers three models for analysis.\n\nAverage hazard ratio model (see detailed description in Section 3.2)\n\n\ngs_power_ahr(): Power computation\n\ngs_design_ahr(): Design computations\n\n\nWeighted logrank model (see detailed description in Chapter 6)\n\n\ngs_power_wlr(): Power computation\n\ngs_design_wlr(): Design computations\n\n\nMaxCombo model (see detailed description in Chapter 8)\n\n\ngs_power_combo(): Power computation\n\ngs_design_combo(): Design computations",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to gsdmvn, gsDesign2, and simtrial</span>"
    ]
  },
  {
    "objectID": "pkgs.html#simulation",
    "href": "pkgs.html#simulation",
    "title": "4  Introduction to gsdmvn, gsDesign2, and simtrial",
    "section": "\n4.4 Simulation",
    "text": "4.4 Simulation\nIn this section, we conduct three simulations.\nThe first simulation is to compare the simulated power and asymptotic power. Specifically speaking, we compare the results from gsdmvn::gs_power_ahr() with that from simtrial::simfix(). The details of this simulation can be found in Section 4.4.1.\nThe second simulation is to compare the simulated power and asymptotic power. Specifically speaking, we compare the results from gsDesign2::AHR() with that from simtrial::simPWSurv(). The details of this simulation can be found in Section 4.4.2.\nThe third simulation is to compare the estimation of \\(\\beta\\) by MLE and weighted summation as in Section 3.2. The details of this simulation can be found in Section 4.4.3.\n\n4.4.1 Simulation 1: Compare the IA power by gs_power_ahr() and simulation sim_fix()\n\nThis section compares the simulated power and asymptotic power. The simulated power is calculated by simtrial::simfix(), and the asymptotic power is calculated by gsdmvn::gs_power_ahr(). To conduct the comparison, we first save the output of simtrial::simfix() by running the following code chunk. After that, we simply load the simulation results and compare it with the asymptotic power.\n\nlibrary(gsDesign)\nlibrary(simtrial)\nlibrary(dplyr)\nlibrary(gsdmvn)\n\n## Set the enrollment rates\nmy_enrollRates &lt;- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(2, 2, 2, 6),\n  rate = c(6, 12, 18, 24)\n)\n## Set the failure rates\nmy_failRates &lt;- tibble::tibble(\n  Stratum = \"All\",\n  duration = 1,\n  failRate = log(2) / 9,\n  hr = 0.65,\n  dropoutRate = 0.001\n)\n## Set the number of simulations\nmy_nsim &lt;- 1e+5\n\n## Create a group sequential design for survival outcome\nmy_gsSurv &lt;- gsSurv(\n  k = 2,\n  test.type = 1,\n  alpha = 0.025,\n  beta = 0.2,\n  astar = 0,\n  timing = 0.7,\n  sfu = sfLDOF,\n  sfupar = c(0),\n  sfl = sfLDOF,\n  sflpar = c(0),\n  lambdaC = log(2) / 9,\n  hr = 0.65,\n  hr0 = 1,\n  eta = 0.001,\n  gamma = c(6, 12, 18, 24),\n  R = c(2, 2, 2, 6),\n  S = NULL,\n  T = NULL,\n  minfup = NULL,\n  ratio = 1\n)\n\n# Update my_gsSurv with gsDesign() to get integer event counts\nmy_gsDesign &lt;- gsDesign(\n  k = my_gsSurv$k,\n  test.type = 1,\n  alpha = my_gsSurv$alpha,\n  beta = my_gsSurv$beta,\n  sfu = my_gsSurv$upper$sf,\n  sfupar = my_gsSurv$upper$param,\n  n.I = ceiling(my_gsSurv$n.I),\n  maxn.IPlan = ceiling(my_gsSurv$n.I[my_gsSurv$k]),\n  delta = my_gsSurv$delta,\n  delta1 = my_gsSurv$delta1,\n  delta0 = my_gsSurv$delta0\n)\n\nset.seed(123)\nmy_simfix &lt;- simfix( # Number of simulations to perform.\n  nsim = my_nsim,\n  # Total sample size per simulation.\n  sampleSize = ceiling(max(my_gsSurv$eNC) + max(my_gsSurv$eNE)),\n  # A tibble with\n  # (1) strata specified in `Stratum`\n  # (2) probability (incidence) of each stratum in `p`.\n  strata = tibble::tibble(Stratum = \"All\", p = 1),\n  # Targeted event count for analysis.\n  # Here we only target on the 1st IA only.\n  targetEvents = my_gsDesign$n.I[1],\n  # Vector of treatments to be included in each block\n  block = c(rep(\"Control\", 2), rep(\"Experimental\", 2)),\n  # Piecewise constant enrollment rates by time period.\n  enrollRates = my_enrollRates,\n  # Piecewise constant control group failure rates,\n  # hazard ratio for experimental vs control,\n  # and dropout rates by stratum and time period.\n  failRates = my_failRates,\n  # `timingType` has up to 5 elements indicating different options for data cutoff.\n  # `timingType = 1`: uses the planned study duration\n  # `timingType = 2`: the time the targeted event count is achieved\n  # `timingType = 3`: the planned minimum follow-up after enrollment is complete\n  # `timingType = 4`: the maximum of planned study duration and targeted event count cuts (1 and 2)\n  # `timingType = 5`: the maximum of targeted event count and minimum follow-up cuts (2 and 3)\n  timingType = 2\n)\n# Save the simulation data\nsave(my_gsDesign, file = \"./data/simulation_gs_power_ahr_my_gsDesign.Rdata\")\nsave(my_simfix, file = \"./data/simulation_gs_power_ahr_my_simfix.Rdata\")\n\n\nlibrary(gsDesign)\nlibrary(simtrial)\nlibrary(gsdmvn)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\nload(\"./data/simulation_gs_power_ahr_my_simfix.Rdata\")\nload(\"./data/simulation_gs_power_ahr_my_gsDesign.Rdata\")\n\n## Set the enrollment rates\nmy_enrollRates &lt;- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(2, 2, 2, 6),\n  rate = c(6, 12, 18, 24)\n)\n## Set the failure rates\nmy_failRates &lt;- tibble::tibble(\n  Stratum = \"All\",\n  duration = 1,\n  failRate = log(2) / 9,\n  hr = 0.65,\n  dropoutRate = 0.001\n)\nmy_nsim &lt;- 1e+5\n\n# Calculate the simulated power at the 1st IA\nmy_sim_IA_power &lt;- as.numeric(my_simfix %&gt;% summarize(mean(Z &lt;= -my_gsDesign$upper$bound[1])))\n\n# Calculate the power by gs_ahr_power() at the 1st IA\nout &lt;- gs_power_ahr(\n  enrollRates = my_enrollRates,\n  failRates = my_failRates,\n  ratio = 1,\n  events = my_gsDesign$n.I, # set number of events the same as my_gsDesign above from gsDesign()\n  analysisTimes = NULL,\n  binding = FALSE,\n  upper = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL, theta = 0),\n  lower = gs_spending_bound,\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2, param = NULL, timing = NULL, theta = 0),\n  test_upper = TRUE,\n  test_lower = FALSE\n)\nmy_ahr_IA_power &lt;- out$Probability[1]\n\ncat(\"The power at the 1st IA by gs_power_ahr() is:\", my_ahr_IA_power, \"\\n\")\n#&gt; The power at the 1st IA by gs_power_ahr() is: 0.4605649\ncat(\"The power at the 1st IA by\", my_nsim, \"simulation is:\", my_sim_IA_power, \"\\n\")\n#&gt; The power at the 1st IA by 1e+05 simulation is: 0.46655\n\n\n4.4.2 Simulation 2: Compare AHR by gsDesign2::AHR() and simtrial::simPWSurv()\n\nThis section compares the simulated AHR and asymptotic AHR The simulated power is calculated by simtrial::simPWSurv(), and the asymptotic power is calculated by gsDesign2::AHR(). To conduct the comparison, we first save the output of simtrial::simPWSurv() by running the following code chunk. After that, we simply load the simulation results and compare it with the asymptotic AHR\n\nlibrary(survival)\nlibrary(dplyr)\nlibrary(simtrial)\n\n# Set the sample size\nmy_N &lt;- 500\n# Set the analysis time, i.e., there are 4 looks\nmy_analysisTimes &lt;- c(12, 20, 28, 36)\n# Set the enrollment rates\n# This format of enrollment rates is design for simtrial::simfix()\n# If it is later used to simtrial::simPWSurv(),\n# function simtrial::simfix2simPWSurv() can used for transformation\nmy_enrollRates &lt;- tibble(\n  Stratum = \"All\",\n  duration = 12,\n  rate = my_N / 12\n)\n# Set the failure rates\n# This format of failure rates is design for simtrial::simfix()\n# If it is later used to simtrial::simPWSurv(),\n# function simtrial::simfix2simPWSurv() can used for transformation\nmy_failRates &lt;- tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15,\n  hr = c(1, 0.6),\n  dropoutRate = 0.001\n)\n\n# Set number of simulations\nmy_nsim &lt;- 10\n# Set up matrix for simulation results\nresults &lt;- matrix(0, nrow = my_nsim * 4, ncol = 6)\ncolnames(results) &lt;- c(\"Sim\", \"Analysis\", \"Events\", \"beta\", \"var\", \"logrank\")\n\n# Set the index for results row\nii &lt;- 1\nset.seed(123)\nfor (sim in 1:my_nsim) {\n  # Simulate a trial\n  ds &lt;- simPWSurv( # Generate my_N observations\n    n = my_N,\n    # Use the same enrollRates as that in AHR\n    enrollRates = my_enrollRates,\n    # Conversion of failRates from simfix() to simPWSurv() format\n    failRates = simfix2simPWSurv(my_failRates)$failRates,\n    # Conversion of dropoutRates from simfix() to simPWSurv() format\n    dropoutRates = simfix2simPWSurv(my_failRates)$dropoutRates\n  )\n  # For each generated my_N observations\n  # Go through pre-defined 4 looks\n  for (j in seq_along(my_analysisTimes)) {\n    # Cut data at specified analysis times\n    # Use cutDataAtCount to cut at event count\n    # des is a dataset ready for survival analysis\n    dsc &lt;- ds %&gt;% cutData(my_analysisTimes[j])\n\n    # 1st column of results records the index of the simulation\n    results[ii, 1] &lt;- sim\n    # 2nd column of results records the index of the look\n    results[ii, 2] &lt;- j\n    # 3rd column of results records the number of events\n    results[ii, 3] &lt;- sum(dsc$event)\n\n    # Apply Cox model\n    cox &lt;- coxph(Surv(tte, event) ~ Treatment, data = dsc)\n    # 4th column of results records the number log HR\n    results[ii, 4] &lt;- as.numeric(cox$coefficients)\n    # 5th column of results records the variance\n    results[ii, 5] &lt;- as.numeric(cox$var)\n\n    # Logrank test\n    Z &lt;- dsc %&gt;%\n      tensurv(txval = \"Experimental\") %&gt;%\n      tenFH(rg = tibble::tibble(rho = 0, gamma = 0))\n    # 5th column of results records the logrank\n    results[ii, 6] &lt;- as.numeric(Z$Z)\n\n    # Increate the row index\n    ii &lt;- ii + 1\n  }\n}\nsave(results, file = \"./data/simulation_AHR_simPRSurv.Rdata\")\n\n\n# Calculate the simulated AHR\nload(\"./data/simulation_AHR_simPRSurv.Rdata\")\n\n# Distribution of Cox coefficient\nggplot(\n  tibble::as_tibble(results),\n  aes(x = factor(Analysis), y = beta)\n) +\n  geom_violin() +\n  ggtitle(\"Distribution of Cox Coefficient by Analysis\") +\n  xlab(\"Analysis\") +\n  ylab(\"Cox coefficient\")\n\n\n\n\n\n\n\n# Variability of results\nggplot(\n  filter(tibble::as_tibble(results), Sim &lt; 10) %&gt;%\n    mutate(Sim = factor(Sim), Analysis = factor(Analysis)),\n  aes(x = Analysis, y = exp(beta), group = Sim, col = Sim)\n) +\n  geom_line(show.legend = FALSE) +\n  geom_point(aes(shape = Analysis), show.legend = FALSE) +\n  scale_y_log10(breaks = seq(.6, 1.1, .1)) +\n  ylab(\"HR\")\n\n\n\n\n\n\n\n# Comparison of asymptotic vs simulation\nAHR_simulated &lt;- tibble::as_tibble(results) %&gt;%\n  group_by(Analysis) %&gt;%\n  summarize(\n    AHR = exp(mean(beta)),\n    Events = mean(Events),\n    info = 1 / mean(var(beta)),\n    info0 = Events / 4\n  ) %&gt;%\n  select(AHR, Events, info, info0)\ncolnames(AHR_simulated) &lt;- c(\"AHR_sim\", \"Events_sim\", \"info_sim\", \"info0_sim\")\n\n# Calculate the AHR asymptotically\n# gsDesign2::AHR() uses asymptotic distribution\n# We will compare its outputs with the simulated outputs\n# The simulated outputs is calculated by simtrial::simPWSurv()\n\n# Set the sample size the same as simPWSurv()\nmy_N &lt;- 500\n# Set the analysis time the same as simPWSurv()\nmy_analysisTimes &lt;- c(12, 20, 28, 36)\n# Set the enrollment rates the same as simPWSurv()\nmy_enrollRates &lt;- tibble(\n  Stratum = \"All\",\n  duration = 12,\n  rate = my_N / 12\n)\n# Set the failure rates the same as simPWSurv()\nmy_failRates &lt;- tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15,\n  hr = c(1, 0.6),\n  dropoutRate = 0.001\n)\n\nAHR_asymptotics &lt;- gsDesign2::AHR(\n  enrollRates = my_enrollRates,\n  failRates = my_failRates,\n  totalDuration = my_analysisTimes,\n  ratio = 1\n)\ncolnames(AHR_asymptotics) &lt;- c(\"Time\", \"AHR_asy\", \"Events_asy\", \"info_asy\", \"info0_asy\")\n\n# Compare the results\ncbind(AHR_asymptotics, AHR_simulated) %&gt;%\n  gt::gt() %&gt;%\n  gt::fmt_number(columns = c(2, 4, 5, 6, 8, 9), decimals = 4) %&gt;%\n  gt::fmt_number(columns = c(3, 7), decimals = 2)\n\n\n\n\n\n\nTime\nAHR_asy\nEvents_asy\ninfo_asy\ninfo0_asy\nAHR_sim\nEvents_sim\ninfo_sim\ninfo0_sim\n\n\n\n12\n0.8395\n107.39\n26.3710\n26.8486\n0.8415\n107.25\n26.2593\n26.8130\n\n\n20\n0.7379\n207.90\n50.6695\n51.9741\n0.7397\n207.71\n50.8936\n51.9281\n\n\n28\n0.7000\n279.10\n68.2263\n69.7759\n0.7012\n278.93\n67.9878\n69.7321\n\n\n36\n0.6832\n331.29\n81.3779\n82.8227\n0.6839\n331.19\n80.6198\n82.7969\n\n\n\n\n\n\n\n\n4.4.3 Simulation 3: Compare \\(\\beta\\) by MLE and weighted summation\nThis section compares the AHR estimated by MLE (see theoretical details in ?sec-AhrMLE) and weighted summation (see theoretical details in ?sec-AhrWeightedSum).\n\nlibrary(dplyr)\nlibrary(gt)\n\nmy_nsim &lt;- 10\nmy_nNewtonRaphson &lt;- 10\ncompare_results &lt;- matrix(0, nrow = my_nsim, ncol = 2)\nset.seed(123)\n\nfor (sim in 1:my_nsim) {\n  # Generate the number of change points, i.e., total number of timeline piece - 1\n  sim_M &lt;- sample(3:10, size = 1)\n  sim_d0_start &lt;- sample(4:8, size = 1)\n  sim_d1_start &lt;- sim_d0_start - 3\n  sim_T0_start &lt;- sample(10:20, size = 1)\n  sim_T1_start &lt;- sim_T0_start + 1\n  # Generate simulated data\n  obs_data &lt;- data.frame( # m = 1:5,\n    m = sim_M,\n    # d0 = 4:8,\n    d0 = seq(from = sim_d0_start, length.out = sim_M),\n    # d1 = 1:5,\n    d1 = seq(from = sim_d1_start, length.out = sim_M),\n    # T0 = 10:14,\n    T0 = seq(from = sim_T0_start, length.out = sim_M),\n    # T1 = 11:15\n    T1 = seq(from = sim_T1_start, length.out = sim_M)\n  ) %&gt;%\n    mutate(\n      lambda0 = d0 / T0,\n      lambda1 = d1 / T1,\n      HR = lambda1 / lambda0,\n      # beta_m\n      gamma = log(HR),\n      # Var(beta_m)\n      vargamma = 1 / d0 + 1 / d1\n    )\n\n  # Estimate beta by weighted summation\n  # beta = variable `logAHR`\n  estimate_beta_WS &lt;- obs_data %&gt;%\n    summarise(\n      wsum = sum(1 / vargamma),\n      # estimation of beta_WS\n      beta_WS = sum(gamma / vargamma) / wsum,\n      # variance of the estimation of beta_WS\n      var = sum(vargamma^(-1))^(-1),\n      # standard derivation of the estimation of beta_WS\n      se = sqrt(var),\n      # AHR: average of lambda_{1,m}/lambda_{0,m}\n      AHR = exp(beta_WS)\n    )\n  compare_results[sim, 1] &lt;- estimate_beta_WS$beta_WS\n\n  # Estimate beta by MLE\n  beta_MLE &lt;- estimate_beta_WS$beta_WS\n  # beta_MLE_seq &lt;- beta_MLE    # ensure convergence\n\n  for (i in 1:my_nNewtonRaphson) {\n    ## Calculate the first order derivative at the value of beta_k\n    temp_beta_d1 &lt;- obs_data %&gt;%\n      summarise(beta_d1 = -sum((d0 + d1) * T1 * exp(beta_MLE) / (T0 + T1 * exp(beta_MLE)))\n      +\n        sum(d1))\n    beta_d1_curr &lt;- temp_beta_d1$beta_d1\n\n    ## Calculate the second order derivative at the value of beta_k\n    temp_beta_d2 &lt;- obs_data %&gt;%\n      summarise(beta_d2 = sum((T1 * exp(beta_MLE))^2 * (d0 + d1) / (T0 + T1 * exp(beta_MLE))^2)\n      -\n        sum(((d0 + d1) * T1 * exp(beta_MLE)) / (T0 + T1 * exp(beta_MLE))))\n    beta_d2_curr &lt;- temp_beta_d2$beta_d2\n\n    ## Update beta by Newton-Raphson method, i.e.,\n    ## beta_k+1 = beta_k - l'(beta_k)/l''(beta_k)\n    beta_MLE &lt;- beta_MLE - beta_d1_curr / beta_d2_curr\n    # beta_MLE_seq &lt;- c(beta_MLE_seq, beta_MLE)\n  }\n\n  compare_results[sim, 2] &lt;- beta_MLE\n}\n\ncolnames(compare_results) &lt;- c(\"Weighted Summation\", \"MLE\")\nsave(compare_results, file = \"./data/simulation_MLE_vs_WS.Rdata\")\n\n\nload(\"./data/simulation_MLE_vs_WS.Rdata\")\nmy_nsim &lt;- 1e+5\nhead(compare_results)\n#&gt;      Weighted Summation        MLE\n#&gt; [1,]         -0.4139139 -0.4158463\n#&gt; [2,]         -0.6729929 -0.6772665\n#&gt; [3,]         -0.4203335 -0.4207679\n#&gt; [4,]         -0.5409583 -0.5530679\n#&gt; [5,]         -0.4247562 -0.4252013\n#&gt; [6,]         -0.7252355 -0.7411065\ncat(\n  \"The MSE between MLE and weighted summation is \",\n  sum((compare_results[, 1] - compare_results[, 2])^2) / my_nsim, \"\\n\"\n)\n#&gt; The MSE between MLE and weighted summation is  4.464504e-05\n\n\n\n\n\nJennison, Christopher, and Bruce W. Turnbull. 2000. Group Sequential Methods with Applications to Clinical Trials. Boca Raton, FL: Chapman; Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to gsdmvn, gsDesign2, and simtrial</span>"
    ]
  },
  {
    "objectID": "other-tests.html",
    "href": "other-tests.html",
    "title": "Others tests",
    "section": "",
    "text": "We consider other alternative tests for group sequential design:\n\nWeighted logrank tests such as the Harrington and Fleming (1982) and Magirr and Burman (2019) tests.\nCombination tests such as the MaxCombo test by Lin et al. (2020), Roychoudhury et al. (2021).\n\nWe will illustrate the concepts based on an experimental R package:\n\ngsdmvn: analytically based group sequential design.\n\nSpecifically, we will provide theoretical justification and examples for:\n\nFixed design based on weighted logrank test and MaxCombo test.\nGroup sequential design with weighted logrank test.\nGroup sequential design with MaxCombo test.\n\n\n\n\n\nHarrington, David P, and Thomas R Fleming. 1982. “A Class of Rank Test Procedures for Censored Survival Data.” Biometrika 69 (3): 553–66.\n\n\nLin, Ray S, Ji Lin, Satrajit Roychoudhury, Keaven M Anderson, Tianle Hu, Bo Huang, Larry F Leon, et al. 2020. “Alternative Analysis Methods for Time to Event Endpoints Under Nonproportional Hazards: A Comparative Analysis.” Statistics in Biopharmaceutical Research 12 (2): 187–98.\n\n\nMagirr, Dominic, and Carl-Fredrik Burman. 2019. “Modestly Weighted Logrank Tests.” Statistics in Medicine 38 (20): 3782–90.\n\n\nRoychoudhury, Satrajit, Keaven M Anderson, Jiabu Ye, and Pralay Mukhopadhyay. 2021. “Robust Design and Analysis of Clinical Trials with Non-Proportional Hazards: A Straw Man Guidance from a Cross-Pharma Working Group.” Statistics in Biopharmaceutical Research, 1–37.",
    "crumbs": [
      "Others tests"
    ]
  },
  {
    "objectID": "fixed-design.html",
    "href": "fixed-design.html",
    "title": "5  Fixed design",
    "section": "",
    "text": "5.1 Summary of assumptions\nFor simplicity, we made a few key assumptions.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#summary-of-assumptions",
    "href": "fixed-design.html#summary-of-assumptions",
    "title": "5  Fixed design",
    "section": "",
    "text": "Balanced design (1:1 randomization ratio).\n1-sided test.\nLocal alternative: variance under null and alternative are approximately equal.\nAccrual distribution:\n\nPiecewise uniform using npsurvSS::create_arm()\n\nPoisson process with piecewise uniform enrollment using gsDesign::nSurv() Lachin and Foulkes (1986) and gsdmvn::gs_design_ahr()\n\n\n\nSurvival distribution: piecewise exponential\nLoss to follow-up: exponential\nNo stratification\nNo cure fraction\n\n\n\n\n\n\n\nNote\n\n\n\nSome of these assumptions have been generalized in the literature and the R package gsdmvn. These assumptions will be used unless another clarification is made in specific sections.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#notation",
    "href": "fixed-design.html#notation",
    "title": "5  Fixed design",
    "section": "\n5.2 Notation",
    "text": "5.2 Notation\nWe also define some commonly used notations as below.\n\n\n\\(\\alpha\\): Type I error\n\n\\(\\beta\\): Type II error or power (1 - \\(\\beta\\))\n\n\\(z_\\alpha\\): upper \\(\\alpha\\) percentile of standard normal distribution\n\n\\(z_\\beta\\): upper \\(\\beta\\) percentile of standard normal distribution\n\nFor illustration purpose, we considered a 1-sided test with type I error at \\(\\alpha=0.025\\) and \\(1-\\beta=80\\%\\) power. In R, it is easy to calculate \\(z_\\alpha\\) and \\(z_\\beta\\) as below.\n\nz_alpha &lt;- abs(qnorm(0.025))\nz_alpha\n#&gt; [1] 1.959964\n\n\nz_beta &lt;- abs(qnorm(0.2))\nz_beta\n#&gt; [1] 0.8416212",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#sample-size-calculation",
    "href": "fixed-design.html#sample-size-calculation",
    "title": "5  Fixed design",
    "section": "\n5.3 Sample size calculation",
    "text": "5.3 Sample size calculation\n\n\n\\(\\theta\\): effect size.\n\nTo calculate sample size, a key step is to define the effect size. For example, the effect size in two-sample t-test is \\((\\mu_1 - \\mu_2)/\\sigma\\), where \\(\\mu_1\\) and \\(\\mu_2\\) are group mean and \\(\\sigma\\) is pooled standard deviation.\n\n\\(n\\): total sample size.\n\n\\(Z\\): test statistics is asymptotic normal.\n\nUnder null hypothesis: \\(Z \\sim \\mathcal{N}(0, \\sigma_0^2)\\)\n\nUnder alternative hypothesis: \\(Z \\sim \\mathcal{N}(\\sqrt{n}\\theta, \\sigma_1^2)\\)\n\n\n\n\nBy assuming local alternative, we have\n\\[\\sigma_0^2 \\approx \\sigma_1^2 = \\sigma^2\\] In this simplified case, the sample size can be calculated as\n\\[ n = \\frac{4 (z_{\\alpha}+z_{\\beta})^{2}}{\\theta^2} \\] Here \\(\\theta\\) is standardized treatment effect.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#two-sample-t-test",
    "href": "fixed-design.html#two-sample-t-test",
    "title": "5  Fixed design",
    "section": "\n5.4 Two-sample t-test",
    "text": "5.4 Two-sample t-test\nLet’s revisit the two-sample t-test to make a connection between the math formula and R code.\nIn two-sample t-test, we have\n\\[\\theta = \\frac{\\Delta}{\\sigma}\\], where \\(\\theta\\) is difference of mean in two groups and \\(\\sigma\\) is standard deviation of \\(\\theta\\).\nIf we consider a scenarios with treatment effect at 0.5 with pooled standard deviation at 2.\nLet’s calculate the sample size using the formula above.\n\n# Effect size\ntheta &lt;- 0.5 / 2\n\n\n# Sample size formula\n4 * (z_alpha + z_beta)^2 / theta^2\n#&gt; [1] 502.3283\n\nThe same assumption is used in gsDesign::nNormal().\n\ngsDesign::nNormal(delta1 = 0.5, sd = 2, alpha = 0.025, beta = 0.2)\n#&gt; [1] 502.3283\n\nstats::power.t.test() uses the t-distribution for test statistics that is recommended in practice. It provides a slightly larger sample size under the same study design scenario.\n\nstats::power.t.test(delta = 0.5, sd = 2, sig.level = 0.05, power = 0.8)$n * 2\n#&gt; [1] 504.2562",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#logrank-test",
    "href": "fixed-design.html#logrank-test",
    "title": "5  Fixed design",
    "section": "\n5.5 Logrank test",
    "text": "5.5 Logrank test\nLet’s also explore the number of events required for logrank test under the proportional hazards assumption. It is well known that the sample size calculation is “event-driven” in this special case.\nThis is a nice feature under the proportional hazards assumption. because the effect size for number of events is only depends on the hazard ratio.\n\n\n\n\n\n\nNote\n\n\n\nNotation: \\(\\text{HR}\\) is the hazard ratio. \\(d\\) is the number of events.\n\n\nThe effect size is: \\[\\theta = \\log{\\text{HR}} / 2\\]\n\n\n\n\n\n\nNote\n\n\n\nAs an exercise, the readers can derive the effect size following Section 9.5 of the NCSU ST520 course notes. The effect size is in formula (9.3) of the course notes.\n\n\nWith the effect size, we can use the same formula to calculate the number of events (\\(d\\)) as below.\n\\[ d = \\frac{4 (z_{\\alpha}+z_{\\beta})^{2}}{(\\log{\\text{HR}/2})^2} \\]\nAfter the total number of events is defined, the sample size (\\(n\\)) can be determined based on accrual distribution, survival distribution loss to follow-up distribution and study duration.\nThe sample size calculation has been implemented in\n\ngsDesign::nSurv()\nnpsurvSS::size_two_arm()\n\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, we skip the details of sample size calculation discussed in Lachin and Foulkes (1986).\n\n\nLet’s make connection between math formula and R code by considering a scenario with hazard ratio at 0.6.\n\n# Effect size\ntheta &lt;- log(0.6) / 2\n\n\n# Number of Events\n(z_alpha + z_beta)^2 / theta^2\n#&gt; [1] 120.3157\n\nWe compare the results using npsurvSS. The key argument in npsurvSS::create_arm() is surv_scale that defines the hazard rates in each arm.\n\n# Define study design assumption for each arm\narm0 &lt;- npsurvSS::create_arm(\n  size = 1, accr_time = 6, surv_scale = 1,\n  loss_scale = 0.1, follow_time = 12\n)\n\narm1 &lt;- npsurvSS::create_arm(\n  size = 1, accr_time = 6, surv_scale = 0.6,\n  loss_scale = 0.1, follow_time = 12\n)\n\nThen we can use npsurvSS::size_two_arm() to calculate number of events and sample size.\n\n# Sample size for logrank test\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(\n    test = \"weighted logrank\", weight = \"1\",\n    mean.approx = \"event driven\"\n  )\n)\n#&gt;        n0        n1         n        d0        d1         d \n#&gt;  68.12167  68.12167 136.24335  61.92878  58.38693 120.31570\n\nThe number of events from gsDesign::nSurv() is slightly smaller because gsDesign::nSurv() follows Lachin and Foulkes (1986) that relax the local alternative assumption and is recommended in practice.\n\ngsDesign::nSurv(\n  lambdaC = 1,\n  hr = 0.6,\n  eta = 0.1,\n  alpha = 0.025,\n  beta = 0.2,\n  T = 18,\n  minfup = 12\n)\n#&gt; Fixed design, two-arm trial with time-to-event\n#&gt; outcome (Lachin and Foulkes, 1986).\n#&gt; Solving for:  Accrual rate \n#&gt; Hazard ratio                  H1/H0=0.6/1\n#&gt; Study duration:                   T=18\n#&gt; Accrual duration:                   6\n#&gt; Min. end-of-study follow-up: minfup=12\n#&gt; Expected events (total, H1):        119.7983\n#&gt; Expected sample size (total):       135.6574\n#&gt; Accrual rates:\n#&gt;     Stratum 1\n#&gt; 0-6   22.6096\n#&gt; Control event rates (H1):\n#&gt;       Stratum 1\n#&gt; 0-Inf         1\n#&gt; Censoring rates:\n#&gt;       Stratum 1\n#&gt; 0-Inf       0.1\n#&gt; Power:                 100*(1-beta)=80%\n#&gt; Type I error (1-sided):   100*alpha=2.5%\n#&gt; Equal randomization:          ratio=1",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#non-proportional-hazards",
    "href": "fixed-design.html#non-proportional-hazards",
    "title": "5  Fixed design",
    "section": "\n5.6 Non-proportional hazards",
    "text": "5.6 Non-proportional hazards\nUnder proportional hazard assumption, we commonly used an event-driven approach for sample size calculation. (i.e., calculate the number of events, then derive the sample size.)\n\nEvent (\\(d\\)) to Sample size (\\(n\\)) or d-&gt;n\n\n\n\n\n\n\n\n\n\n\nUnder non-proportional hazards, the event-driven approach is not applicable. We need to derive the sample size first.\n\nSample size (\\(n\\)) to Event (\\(d\\)) or n-&gt;d\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau_a\\): accrual time\n\n\\(\\tau_f\\): follow-up time\n\n\n\n\n\n\n\nNote\n\n\n\nThe two figures above are copied from Yung and Liu (2019).\n\n\nLet’s consider a delayed effect scenario below to illustrate NPH and sample size calculation.\n\nDuration of enrollment: 12 months\nEnrollment rate: 500/12 per month\n\n\nenrollRates &lt;- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500 / 12)\nenrollRates\n#&gt; # A tibble: 1 × 3\n#&gt;   Stratum duration  rate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 All           12  41.7\n\n\nFailure rate in control group: log(2) / 15\n\nMedian survival time: 15 months.\n\n\nHazard ratio:\n\nFirst 4 months: 1\nAfter 4 months: 0.6\n\n\nDropout Rate: 0.001\n\n\nfailRates &lt;- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15, # Median survival 15 months\n  hr = c(1, .6), # Delay effect after 4 months\n  dropoutRate = 0.001\n)\nfailRates\n#&gt; # A tibble: 2 × 5\n#&gt;   Stratum duration failRate    hr dropoutRate\n#&gt;   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 All            4   0.0462   1         0.001\n#&gt; 2 All          100   0.0462   0.6       0.001\n\nThe figure below illustrates the survival probability over time in two treatment groups.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#weighted-logrank-test",
    "href": "fixed-design.html#weighted-logrank-test",
    "title": "5  Fixed design",
    "section": "\n5.7 Weighted logrank test",
    "text": "5.7 Weighted logrank test\nFor the weighted logrank test, we first illustrate it by using the Fleming-Harrington weight.\n\\[FH^{\\rho, \\gamma}(t) = S(t)^\\rho(1 - S(t))^\\gamma\\]\nTo demonstrate the weight function, we covert the input of enrollRates and failRates as required by npsurvSS.\n\n# Define study design object in each arm\ngs_arm &lt;- gsdmvn:::gs_create_arm(\n  enrollRates,\n  failRates,\n  ratio = 1, # Randomization ratio\n  total_time = 36 # Total study duration\n)\narm0 &lt;- gs_arm[[\"arm0\"]]\narm1 &lt;- gs_arm[[\"arm1\"]]\n\n\n\n\n\n\n\nNote\n\n\n\nNote: in npsurvSS, p1_q0 is for \\(\\rho=q=0\\) and \\(\\gamma=p=1\\)\n\n\n\nFH(0,1): Place more weights on later time points\n\n\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p1_q0\")\n)\n#&gt;        n0        n1         n        d0        d1         d \n#&gt; 138.39353 138.39353 276.78707 102.15617  81.23792 183.39408\n\n\nFH(1,1): Place more weights on middle time points\n\n\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p1_q1\")\n)\n#&gt;        n0        n1         n        d0        d1         d \n#&gt; 130.75651 130.75651 261.51302  96.51884  76.75493 173.27377\n\n\nFH(1,0): Place more weights on earlier time points\n\n\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p0_q1\")\n)\n#&gt;       n0       n1        n       d0       d1        d \n#&gt; 237.5982 237.5982 475.1964 175.3848 139.4717 314.8565\n\n\nFH(0,0) or logrank test\n\n\n# Sample size for logrank test\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p0_q0\")\n)\n#&gt;        n0        n1         n        d0        d1         d \n#&gt; 164.97626 164.97626 329.95252 121.77839  96.84215 218.62055\n\n\n5.7.1 Effect size\nIn Yung and Liu (2019), section 2.3.3, it is shown that the test statistics \\(Z\\rightarrow_d\\mathcal{N}(\\sqrt{n}\\theta, 1)\\) approximately, where \\(\\theta = \\Delta/\\sigma\\) is the effect size. Here the test statistics for weighted logrank test is:\n\\[ Z=\\sqrt{\\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\\int_{0}^{\\tau}w(t)\\frac{\\overline{Y}_{0}(t)\\overline{Y}_{1}(t)}{\\overline{Y}_{0}(t)+\\overline{Y}_{0}(t)}\\left\\{ \\frac{d\\overline{N}_{1}(t)}{\\overline{Y}_{1}(t)}-\\frac{d\\overline{N}_{0}(t)}{\\overline{Y}_{0}(t)}\\right\\} \\]\n\nWeight \\(w(t)\\): implemented in gsdmvn.\nIntegration up to total follow-up time \\(\\tau\\).\n\n\nweight &lt;- function(x, arm0, arm1) {\n  gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n}\n\n\\[\\Delta=\\int_{0}^{\\tau}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)}\\{\\lambda_{1}(s)-\\lambda_{0}(s)\\}ds\\],\n\ndelta &lt;- abs(gsdmvn:::gs_delta_wlr(\n  arm0, arm1,\n  tmax = arm0$total_time,\n  weight = weight\n))\ndelta\n#&gt; [1] 0.02623776\n\n\\[\\sigma^{2}=\\int_{0}^{\\tau}w(s)^{2}\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^{2}}dv(s)\\]\n\nsigma2 &lt;- gsdmvn:::gs_sigma2_wlr(\n  arm0, arm1,\n  tmax = arm0$total_time,\n  weight = weight\n)\nsigma2\n#&gt; [1] 0.0242674\n\nBelow are definitions of each component.\n\n\n\\(n = n_0 + n_1\\): Total sample size\n\n\\(p_0 = n_0/n\\), \\(p_1 = n_1/n\\): Randomization probability inferred by randomization ratio\n\n\\(\\pi_0(t) = E\\{N_0(t)\\}\\), \\(\\pi_1(t) = E\\{N_1(t)\\}\\)\n\n\n\\(\\pi(t) = p_0\\pi_0(t)+p_1\\pi_1(t)\\): Probability of events\n\n\\(v(t) = p_0E\\{Y_0(t)\\}+p_1E\\{Y_1(t)\\}\\): Probability of people at risk\n\n5.7.2 Sample size and number of events\nWe can calculate sample size after deriving the effect size.\n\\[ n = \\frac{\\sigma^{2}(z_{\\alpha}+z_{\\beta})^{2}}{\\Delta^2} = \\frac{(z_{\\alpha}+z_{\\beta})^{2}}{\\theta^2} \\]\n\nz_alpha &lt;- qnorm(1 - 0.025)\nz_beta &lt;- qnorm(1 - 0.2)\nn &lt;- sigma2 * (z_alpha + z_beta)^2 / delta^2\nn\n#&gt; [1] 276.6798\n\n\n# Sample size for FH(0,1)\nnpsurvSS::size_two_arm(arm0, arm1,\n  power = 0.8, alpha = 0.025,\n  test = list(test = \"weighted logrank\", weight = \"FH_p1_q0\")\n)\n#&gt;        n0        n1         n        d0        d1         d \n#&gt; 138.39353 138.39353 276.78707 102.15617  81.23792 183.39408\n\nThe number of events can also be calculated as below. More details can be found in the technical details.\n\nn0 &lt;- n1 &lt;- n / 2\ngsdmvn:::prob_event.arm(arm0, tmax = arm0$total_time) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = arm0$total_time) * n1\n#&gt; [1] 183.323",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#technical-details",
    "href": "fixed-design.html#technical-details",
    "title": "5  Fixed design",
    "section": "\n5.8 Technical details",
    "text": "5.8 Technical details\n\n5.8.1 Accrual and follow-up time\n\n\n\\(R\\): time of study entry\n\n\\(F_R(\\cdot)\\) CDF of \\(R\\)\n\n\n\nx &lt;- seq(0, arm0$total_time, 0.1)\n# Piecewise Uniform Distribution\nplot(x, npsurvSS::daccr(x, arm = arm0),\n  type = \"s\",\n  xlab = \"Calendar time (month)\",\n  ylab = \"Accrual Probability\"\n)\n\n\n\n\n\n\n\n\n\n\\(\\tau_a\\): accrual time\n\n\narm0$accr_time\n#&gt; [1] 12\n\n\n\n\\(\\tau_f\\): follow-up time\n\n\narm0$follow_time\n#&gt; [1] 24\n\n\n\n\\(\\tau = \\tau_a + \\tau_f\\): total study duration\n\n\narm0$total_time\n#&gt; [1] 36\n\n\n5.8.2 Event time\n\n\n\\(T\\): time to event from study entry.\n\n\\(S_T(\\cdot)\\): Survival function\n\n\\(F_T(\\cdot)\\): CDF of \\(T\\)\n\n\n\\(f_T(\\cdot)\\): Density function\n\n\n# Survival function of time to event from study entry\n# Piecewise Exponential distribution\nplot(x, 1 - npsurvSS::psurv(x, arm0),\n  type = \"l\",\n  xlab = \"Calendar time (month)\",\n  ylab = \"Survival Probability\",\n  ylim = c(0, 1)\n)\nlines(x, 1 - npsurvSS::psurv(x, arm1), lty = 2)\nlegend(\"topright\", lty = c(1, 2), legend = c(\"control\", \"experiment\"))\n\n\n\n\n\n\n\n\n5.8.3 Censoring time\n\n\n\\(L\\): time to loss of follow-up from study entry\n\n\\(S_L(\\cdot)\\): Survival function of \\(L\\)\n\n\n\n# PDF of the time to loss of follow-up from study entry\n# Exponential Distribution\nplot(x, 1 - pexp(x),\n  type = \"l\",\n  xlab = \"Calendar time (month)\",\n  ylab = \"Loss to Follow-up Probability\"\n)\n\n\n\n\n\n\n\n\n\n\\(C = \\min(L, \\tau - R)\\): time to censoring from study entry.\n\n5.8.4 Observed time\n\n\n\\(U = \\min(T,C)\\): observed time.\n\n\\(\\delta = I(T&lt;C)\\): event indicator\n\n5.8.5 Expected events\n\n\n\\(N(t) = I(U \\le t, \\delta = 1)\\): counting process\n\n\\(E\\{N(t)\\}\\): expected probability of events\n\n\\[E\\{N(t)\\} = \\int_{0}^{t}f_{T}(s)S_{L}(s)F_{R}(\\tau_{a}\\land(\\tau-s))ds\\]\n\n# Probability of Events\nx_int &lt;- 0:arm0$total_time\nplot(x_int, gsdmvn:::prob_event.arm(arm0, tmax = x_int),\n  type = \"l\",\n  xlab = \"Calendar time (month)\",\n  ylab = \"Event Probability\",\n  ylim = c(0, 1)\n)\n\n\n\n\n\n\n\n\n\n\\(Y(t) = I(U\\ge t)\\): at-risk process\n\n\\(E\\{Y(t)\\}\\): expected probability at risk\n\n\\[E\\{Y(t)\\} = S_{T}(s)S_{L}(s)F_{R}(\\tau_{a}\\land(\\tau-s))\\]\n\n# Probability of People at risk\nplot(x_int, gsdmvn:::prob_risk(arm0, x_int, arm0$total_time),\n  type = \"l\",\n  xlab = \"Calendar time (month)\",\n  ylab = \"Probability at Risk\",\n  ylim = c(0, 1)\n)\n\n\n\n\n\n\n\n\n5.8.6 Weight function in weighted logrank test\n\nDefine different weight functions in gsdmvn\nWeight function: \\(w(t)\\)\n\nConstant weight (logrank test): \\(1\\)\n\n\n\ngsdmvn::wlr_weight_1(x = c(12, 24, 36), arm0, arm1)\n#&gt; [1] 1\n\n\nFleming-Harrington weight: \\(w(t) = FH^{\\rho, \\gamma}(t) = S(t)^\\rho(1 - S(t))^\\gamma\\)\n\n\n\nweight_legend &lt;- apply(\n  expand.grid(\n    c(\"rho=0\", \"rho=1\"),\n    c(\"gamma=0\", \"gamma=1\")\n  ),\n  1,\n  paste0,\n  collapse = \"; \"\n)\nplot(\n  1:36,\n  gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 0, gamma = 0),\n  xlab = \"Calendar time (month)\", col = 1,\n  ylab = \"Weight\", type = \"l\", ylim = c(-0.5, 1.2)\n)\nlines(\n  1:36,\n  gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 1, gamma = 0),\n  col = 2\n)\nlines(\n  1:36,\n  gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 0, gamma = 1),\n  col = 3\n)\nlines(\n  1:36,\n  gsdmvn::wlr_weight_fh(x = 1:36, arm0, arm1, rho = 1, gamma = 1),\n  col = 4\n)\nlegend(\"bottomright\", legend = weight_legend, lty = 1, col = 1:4)\n\n\n\n\n\n\n\n\nModestly WLR: \\(S^{-1}(t\\land\\tau_m)\\) Magirr and Burman (2019).\nChoose \\(\\tau_m\\) around the change point in delay effect scenario.\nOnly down-weight early event. Constant weight after change point.\n\n\nplot(\n  1:36,\n  gsdmvn::wlr_weight_fh(\n    x = 1:36, arm0, arm1, rho = -1, gamma = 0, tau = 4\n  ),\n  xlab = \"Calendar time (month)\",\n  ylab = \"Weight\",\n  type = \"l\"\n)\n\n\n\n\n\n\n\n\n5.8.7 Average hazard ratio\n\n\n\\(\\Delta\\) is a weighted average of hazard function difference \\(\\lambda_{1}(\\cdot)-\\lambda_{0}(\\cdot)\\).\n\n\\[\\Delta=\\int_{0}^{\\tau}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)}\\{\\lambda_{1}(s)-\\lambda_{0}(s)\\}ds\\]\n\nTaylor expansion builds a connection between \\(\\Delta\\) and weighted average of log hazard ratio (AHR). It is a generalization of the Schoenfeld (1981) asymptotic expansion.\n\n\\[\\Delta\\approx\\int_{0}^{\\tau}w(s)\\log\\left(\\frac{\\lambda_1(s)}{\\lambda_0(s)}\\right)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^2}v'(s)ds\\]\n\nLog of AHR can be estimated after normalizing the weights in \\(\\Delta\\)\n\n\n\\[ \\log{AHR} = \\frac{\\Delta}{\\int_{0}^{\\tau}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^2}v'(s)ds} \\]\n\nt &lt;- 1:36\nlog_ahr &lt;- sapply(t, function(t) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t, weight = weight) /\n    gsdmvn:::gs_delta_wlr(arm0, arm1,\n      tmax = t, weight = weight,\n      approx = \"generalized schoenfeld\", normalization = TRUE\n    )\n})\nplot(t, exp(log_ahr),\n  ylim = c(0.6, 1),\n  xlab = \"Calendar time (month)\",\n  ylab = \"Average Hazard Ratio\",\n  type = \"l\"\n)\n\n\n\n\n\n\n\n\nNote the AHR depends on the choice of weight function \\(w(\\cdot)\\).\nUnder logrank test with piecewise exponential distribution and 1:1 randomization ratio, it can be simplified to\n\n\\[\\log(AHR) = \\frac{\\sum d_i \\log{HR_i}}{\\sum d_i}\\]\n\ngsDesign2::AHR(enrollRates, failRates, totalDuration = arm0$total_time)\n#&gt; # A tibble: 1 × 5\n#&gt;    Time   AHR Events  info info0\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    36 0.683   331.  81.4  82.8\n\n\nlog_ahr &lt;- gsdmvn:::gs_delta_wlr(\n  arm0, arm1,\n  tmax = arm0$total_time,\n  weight = gsdmvn:::wlr_weight_1\n) /\n  gsdmvn:::gs_delta_wlr(\n    arm0, arm1,\n    tmax = arm0$total_time,\n    weight = gsdmvn:::wlr_weight_1,\n    approx = \"generalized schoenfeld\",\n    normalization = TRUE\n  )\nexp(log_ahr)\n#&gt; [1] 0.6831735\n\nAlso computed by gsDesign2::AHR().",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "fixed-design.html#maxcombo-test",
    "href": "fixed-design.html#maxcombo-test",
    "title": "5  Fixed design",
    "section": "\n5.9 MaxCombo test",
    "text": "5.9 MaxCombo test\nMaxCombo test statistics is the maximum of multiple WLR test statistics using different weight functions. We focus on Fleming and Harrington weight function \\(FH(\\rho, \\gamma)\\) defined in gsdmvn::wlr_weight_fh().\nTo calculate the sample size for the MaxCombo test, we will need to know the variance-covariance of multiple WLR test statistics.\nFor Fleming and Harrington weight function, the covariance of different WLR test statistics can be calculated. Based on Karrison (2016) and Wang, Luo, and Zheng (2019), the covariance of two test statistics \\(Z_1 = Z(\\rho_1, \\gamma_1), Z_2 = Z(\\rho_2, \\gamma_2)\\) is\n\\[\\text{Cov}(Z_1, Z_2) = \\text{Var}(Z(\\frac{\\rho_1 + \\rho_2}{2}, \\frac{\\gamma_1 + \\gamma_2}{2}))\\]\n\nWe illustrate the idea based on \\(FH(0, 0.5)\\) and \\(FH(0.5, 0.5)\\)\n\nAll weight functions\n\n\nweight_combo &lt;- list(\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0, gamma = 0.5\n    )\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0.25, gamma = 0.5\n    )\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0.25, gamma = 0.5\n    )\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(\n      x, arm0, arm1,\n      rho = 0.5, gamma = 0.5\n    )\n  }\n)\n\n\n\\(\\Delta\\)\n\n\ndelta_combo &lt;- sapply(weight_combo[c(1, 4)], function(x) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = arm0$total_time, weight = x)\n})\ndelta_combo\n#&gt; [1] -0.03980774 -0.02933963\n\n\nCovariance\n\n\nsigma2_combo &lt;- sapply(weight_combo, function(x) {\n  gsdmvn:::gs_sigma2_wlr(arm0, arm1, tmax = arm0$total_time, weight = x)\n})\nsigma2_combo &lt;- matrix(sigma2_combo, nrow = 2)\nsigma2_combo\n#&gt;            [,1]       [,2]\n#&gt; [1,] 0.05441018 0.04007109\n#&gt; [2,] 0.04007109 0.03014093\n\n\nCorrelation\n\n\ncorr_combo &lt;- cov2cor(sigma2_combo)\ncorr_combo\n#&gt;          [,1]     [,2]\n#&gt; [1,] 1.000000 0.989493\n#&gt; [2,] 0.989493 1.000000\n\n\n5.9.1 Type I error and power\nThe formula to calculate sample size based on effect size does not directly apply, because the test statistic for MaxCombo is not asymptotically normal.\nThe type I Error should be:\n\\[\n\\alpha = \\text{Pr}(\\max(Z_1, Z_2) &gt; z_{\\alpha} \\, | \\, H_0) = 1 - \\text{Pr}(Z_1 &lt; z_{\\alpha}, Z_2 &lt; z_{\\alpha} \\, | \\, H_0)\n\\]\n\nz_alpha &lt;- qmvnorm(p = 1 - 0.025, corr = corr_combo)\nz_alpha$quantile\n#&gt; [1] 2.014555\n\nThe power should be\n\\[\n\\beta = \\text{Pr}(\\max(Z_1, Z_2) &gt; z_{\\alpha} \\, | \\, H_1) = 1 - \\text{Pr}(Z_1 &lt; z_{\\alpha}, Z_2 &lt; z_{\\alpha} \\, | \\, H_1)\n\\]\n\n#' @param n Sample size\n#' @param power Target power. Use power=0 to calculate the actual power.\npower_combo &lt;- function(n, power = 0) {\n  theta &lt;- abs(delta_combo) / sqrt(diag(sigma2_combo))\n  power_combo &lt;- 1 - pmvnorm(\n    upper = z_alpha$quantile,\n    mean = sqrt(n) * theta, corr = corr_combo\n  )\n  as.numeric(power_combo) - power\n}\npower_combo(n = 150)\n#&gt; [1] 0.5493368\n\n\n5.9.2 Sample size\nTo calculate the sample size, we can solve the power function with a giving Type I error and power.\n\nn_combo &lt;- uniroot(power_combo, interval = c(0, 500), power = 0.8)$root\nn_combo\n#&gt; [1] 271.0453\n\n\nc(z = z_alpha$quantile, n = n_combo, power = power_combo(n_combo))\n#&gt;          z          n      power \n#&gt;   2.014555 271.045320   0.800000\n\n\n5.9.3 Number of events\nThe number of events required can be calculated accordingly as in weighted logrank test.\n\nn0 &lt;- n1 &lt;- n_combo / 2\ngsdmvn:::prob_event.arm(arm0, tmax = arm0$total_time) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = arm0$total_time) * n1\n#&gt; [1] 179.5897\n\n\n\n\n\nKarrison, Theodore G. 2016. “Versatile Tests for Comparing Survival Curves Based on Weighted Log-Rank Statistics.” The Stata Journal 16 (3): 678–90.\n\n\nLachin, John M, and Mary A Foulkes. 1986. “Evaluation of Sample Size and Power for Analyses of Survival with Allowance for Nonuniform Patient Entry, Losses to Follow-up, Noncompliance, and Stratification.” Biometrics, 507–19.\n\n\nMagirr, Dominic, and Carl-Fredrik Burman. 2019. “Modestly Weighted Logrank Tests.” Statistics in Medicine 38 (20): 3782–90.\n\n\nSchoenfeld, David. 1981. “The Asymptotic Properties of Nonparametric Tests for Comparing Survival Distributions.” Biometrika 68 (1): 316–19.\n\n\nWang, Lili, Xiaodong Luo, and Cheng Zheng. 2019. “A Simulation-Free Group Sequential Design with Max-Combo Tests in the Presence of Non-Proportional Hazards.” arXiv Preprint arXiv:1911.05684.\n\n\nYung, Godwin, and Yi Liu. 2019. “Sample Size and Power for the Weighted Log-Rank Test and Kaplan-Meier Based Tests with Allowance for Nonproportional Hazards.” Biometrics.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fixed design</span>"
    ]
  },
  {
    "objectID": "wlr.html",
    "href": "wlr.html",
    "title": "6  Weighted logrank test",
    "section": "",
    "text": "6.1 Assumptions\nFor a group sequential design, we assume there are total \\(K\\) analyses in a trial. The calendar time of the analyses are \\(t_k, k =1,\\dots, K\\).\nWe define the test statistic at analysis \\(k=1,2,\\dots,K\\) as\n\\[\nZ_k =  \\frac{\\widehat{\\Delta}_k}{\\sigma(\\widehat{\\Delta}_k)}\n\\]\nwhere \\(\\widehat{\\Delta}_k\\) is a statistics to characterize group difference and \\(\\sigma(\\widehat{\\Delta}_k)\\) is the standard deviation of \\(\\widehat{\\Delta}_k\\).",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#assumptions",
    "href": "wlr.html#assumptions",
    "title": "6  Weighted logrank test",
    "section": "",
    "text": "6.1.1 Asymptotic normality\nIn group sequential design, we consider the test statistics are asymptotically normal.\n\nUnder the null hypothesis\n\n\\[\nZ_k \\sim \\mathcal{N}(0,1)\n\\]\n\nUnder a (local) alternative hypothesis\n\n\\[\nZ_k \\sim \\mathcal{N}(\\Delta_k I_k^{1/2}, 1),\n\\]\nwhere \\(I_k\\) is the Fisher information \\(I_k \\approx 1/{\\sigma^2(\\widehat{\\Delta}_k)}\\)\nThen we can define the effect size as\n\\[\n\\theta = \\Delta_k / \\sigma_k = \\Delta_k I_k^{1/2}$ with $\\sigma_k = \\sigma(\\widehat{\\Delta}_k)\n\\].\nSo, it is equivalent to claim:\n\\[\nZ_k \\sim \\mathcal{N}(\\theta_k, 1)\n\\]\n\n6.1.2 Independent increments process\nFor group sequential design, we need to demonstrate the test statistics \\(Z = (Z_1, \\dots, Z_K)\\) is an independent increments process. The property for WLR has been proved by Scharfstein, Tsiatis, and Robins (1997) as a corollary of martingale representation.\n\nUnder the null \\(Z ~ \\sim MVN(0,\\Sigma)\\)\nUnder a local alternative \\(Z ~ \\sim MVN(\\theta, \\Sigma)\\)\nHere, \\(\\Sigma={\\Sigma_{ij}}\\) is a correlation matrix with\n\n\\[\n\\Sigma_{ij} = \\min(I_i, I_j) / \\max(I_i, I_j) \\approx \\min(\\sigma_i, \\sigma_j) / \\max(\\sigma_i, \\sigma_j)\n\\]",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#type-i-error-alpha-spending-function-and-group-sequential-design-boundary",
    "href": "wlr.html#type-i-error-alpha-spending-function-and-group-sequential-design-boundary",
    "title": "6  Weighted logrank test",
    "section": "\n6.2 Type I error, \\(\\alpha\\)-spending function, and group sequential design boundary",
    "text": "6.2 Type I error, \\(\\alpha\\)-spending function, and group sequential design boundary\n\nDefine test boundary for each interim analysis\n\n\n\\(\\alpha\\)-spending function Lan and DeMets (1983)\n\n\n\nBounds \\(-\\infty \\le a_k \\le b_k \\le \\infty\\) for \\(k=1,\\dots,K\\)\n\nnon-binding futility bound \\(a_k = -\\infty\\) for all \\(k\\)\n\n\n\nUpper boundary crossing probabilities\n\n\\(u_k = \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j &lt; b_j\\})\\)\n\n\nLower boundary crossing probabilities\n\n\\(l_k = \\text{Pr}(\\{Z_k &lt; a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j &lt; b_j\\})\\)\n\n\nUnder the null hypothesis, the probability to reject the null hypothesis.\n\n\\(\\alpha = \\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\} \\mid H_0)\\)\nWith a spending function family \\(f(t,\\gamma)\\),\n\ngsDesign::sfLDOF() (O’Brien-Fleming bound approximation) or other functions starts with sf in gsDesign\n\n\n\nFor simplicity, we directly use gsDesign boundary for this chapter. It will inflate Type I Error for WLR tests. We will discuss how to fix the issue in the next chapter.\n\nx &lt;- gsDesign::gsSurv(\n  k = 3, test.type = 4, alpha = 0.025,\n  beta = 0.1, astar = 0, timing = c(1),\n  sfu = gsDesign::sfLDOF, sfupar = c(0),\n  sfl = gsDesign::sfLDOF, sflpar = c(0),\n  lambdaC = c(0.1),\n  hr = 0.6, hr0 = 1, eta = 0.01,\n  gamma = c(10),\n  R = c(12), S = NULL,\n  T = 36, minfup = 24, ratio = 1\n)\n\n\nLower bound\n\n\nx$lower$bound\n#&gt; [1] -0.6945842  1.0023997  1.9929702\n\n\nUpper bound\n\n\nx$upper$bound\n#&gt; [1] 3.710303 2.511407 1.992970",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#power",
    "href": "wlr.html#power",
    "title": "6  Weighted logrank test",
    "section": "\n6.3 Power",
    "text": "6.3 Power\nGiven the lower and upper bound of a group sequential design, we can calculate the overall power of the design.\n\nPower: under a (local) alternative hypothesis, the probability to reject the null hypothesis.\n\n\\[\n1 - \\beta =\\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\} \\mid H_1)\n\\]\nIf there is no lower bound, the formula can be simplified as\n\\[\n\\beta = \\text{Pr}(\\cap_{j=1}^{K} \\{Z_j \\le b_j\\} \\mid H_1)\n\\]\nWe can calculate the sample size required for a group sequential design by solving the power equation with a given lower and upper bound and power, type I error and power.\nAs a note, we can calculate futility probability similarly:\n\\[\n\\sum_{k=1}^{K} l_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k &lt; a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\}\\mid H_1)\n\\]",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#weighted-logrank-test",
    "href": "wlr.html#weighted-logrank-test",
    "title": "6  Weighted logrank test",
    "section": "\n6.4 Weighted logrank test",
    "text": "6.4 Weighted logrank test\nSimilar to the fixed design, we can define the test statistics for weighted logrank test using the counting process formula as\n\\[\nZ_k=\\sqrt{\\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\\int_{0}^{t_k}w(t)\\frac{\\overline{Y}_{0}(t)\\overline{Y}_{1}(t)}{\\overline{Y}_{0}(t)+\\overline{Y}_{0}(t)}\\left\\{ \\frac{d\\overline{N}_{1}(t)}{\\overline{Y}_{1}(t)}-\\frac{d\\overline{N}_{0}(t)}{\\overline{Y}_{0}(t)}\\right\\}\n\\]\nHere \\(n_i\\) are the number of subjects in group \\(i\\). \\(\\overline{Y}_{i}(t)\\) are the number of subjects in group \\(i\\) at risk at time \\(t\\). \\(\\overline{N}_{i}(t)\\) are the number of events in group \\(i\\) up to and including time \\(t\\).\nNote, the only difference is that the test statistics fixed analysis up to \\(t_k\\) at \\(k\\)th interim analysis\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, we illustrate the sample size and power calculation based on the boundary from the logrank test. The same boundary for different types of analysis for a fair comparison. However, different WLR can have different information fraction for the same interim analysis time. Further discussion of the spending function based on actual information fraction will be provided later.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#example-scenario",
    "href": "wlr.html#example-scenario",
    "title": "6  Weighted logrank test",
    "section": "\n6.5 Example scenario",
    "text": "6.5 Example scenario\nWe considered an example scenario that is similar to the fixed design. The key difference is that we considered 2 interim analysis at 12 and 24 months before the final analysis at 36 months.\n\nenrollRates &lt;- tibble::tibble(Stratum = \"All\", duration = 12, rate = 500 / 12)\n\nfailRates &lt;- tibble::tibble(\n  Stratum = \"All\",\n  duration = c(4, 100),\n  failRate = log(2) / 15, # Median survival 15 month\n  hr = c(1, 0.6),\n  dropoutRate = 0.001\n)\n\n\n# Randomization Ratio is 1:1\nratio &lt;- 1\n\n# Type I error (one-sided)\nalpha &lt;- 0.025\n\n# Power (1 - beta)\nbeta &lt;- 0.2\npower &lt;- 1 - beta\n\n# Interim Analysis Time\nanalysisTimes &lt;- c(12, 24, 36)",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#sample-size-under-logrank-test-or-fh0-0",
    "href": "wlr.html#sample-size-under-logrank-test-or-fh0-0",
    "title": "6  Weighted logrank test",
    "section": "\n6.6 Sample size under logrank test or \\(FH(0, 0)\\)\n",
    "text": "6.6 Sample size under logrank test or \\(FH(0, 0)\\)\n\nIn gsdmvn, the sample size can be calculated using gsdmvn::gs_design_wlr() for the WLR test. For comparison purposes, we also provided the sample size calculation using gsdmvn::gs_design_ahr() for the logrank test. In each calculation, we compare the analytical results with the simulation results with 10,000 replications.\n\n\n\n\n\n\nNote\n\n\n\nWe described the theoretical details for sample size calculation in the technical details section.\n\n\n\nAHR with logrank test\n\n\ngsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2) %&gt;%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %&gt;%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)\n#&gt; # A tibble: 3 × 7\n#&gt;   Analysis  Time     N Events   AHR Upper Lower\n#&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1    12  386.   82.9  0.84  0     0.07\n#&gt; 2        2    24  386.  190.   0.71  0.41  0.13\n#&gt; 3        3    36  386.  256.   0.68  0.8   0.2\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;      n  t events  ahr lower upper\n#&gt; 10 386 12  82.77 0.87  0.07  0.00\n#&gt; 11 386 24 190.05 0.72  0.14  0.41\n#&gt; 12 386 36 255.61 0.69  0.20  0.80\n\n\n\\(FH(0,0)\\)\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = c(12, 24, 36)\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2) %&gt;%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %&gt;%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)\n#&gt; # A tibble: 3 × 7\n#&gt;   Analysis  Time     N Events   AHR Upper Lower\n#&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1    12  383.   82.3  0.84  0     0.07\n#&gt; 2        2    24  383.  189.   0.72  0.41  0.14\n#&gt; 3        3    36  383.  254.   0.68  0.8   0.2\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;     n  t events  ahr lower upper\n#&gt; 4 384 12  82.36 0.86  0.07  0.00\n#&gt; 5 384 24 189.05 0.72  0.14  0.41\n#&gt; 6 384 36 254.30 0.69  0.20  0.80",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#sample-size-under-modestly-wlr-cut-at-4-months",
    "href": "wlr.html#sample-size-under-modestly-wlr-cut-at-4-months",
    "title": "6  Weighted logrank test",
    "section": "\n6.7 Sample size under modestly WLR cut at 4 months",
    "text": "6.7 Sample size under modestly WLR cut at 4 months\n\nModestly WLR: \\(S^{-1}(t\\land\\tau_m)\\) Magirr and Burman (2019)\n\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = -1, gamma = 0, tau = 4)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  365.   78.5  3.71        0     0.83  0.16\n#&gt; 2        2 Upper    24  365.  180.   2.51        0.41  0.71  0.29\n#&gt; 3        3 Upper    36  365.  242.   1.99        0.8   0.68  0.33\n#&gt; 4        1 Lower    12  365.   78.5 -0.69        0.07  0.83  0.16\n#&gt; 5        2 Lower    24  365.  180.   1           0.13  0.71  0.29\n#&gt; 6        3 Lower    36  365.  242.   1.99        0.2   0.68  0.33\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#sample-size-under-fh0-1",
    "href": "wlr.html#sample-size-under-fh0-1",
    "title": "6  Weighted logrank test",
    "section": "\n6.8 Sample size under \\(FH(0, 1)\\)\n",
    "text": "6.8 Sample size under \\(FH(0, 1)\\)\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2) %&gt;%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %&gt;%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)\n#&gt; # A tibble: 3 × 7\n#&gt;   Analysis  Time     N Events   AHR Upper Lower\n#&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1    12  316.   68.0  0.73  0     0.04\n#&gt; 2        2    24  316.  156.   0.64  0.45  0.11\n#&gt; 3        3    36  316.  210.   0.62  0.8   0.2\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;      n  t events  ahr lower upper\n#&gt; 16 317 12  68.01 0.76  0.04  0.00\n#&gt; 17 317 24 156.13 0.65  0.12  0.45\n#&gt; 18 317 36 210.06 0.63  0.21  0.79",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#sample-size-under-fh0-0.5",
    "href": "wlr.html#sample-size-under-fh0-0.5",
    "title": "6  Weighted logrank test",
    "section": "\n6.9 Sample size under \\(FH(0, 0.5)\\)\n",
    "text": "6.9 Sample size under \\(FH(0, 0.5)\\)\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2) %&gt;%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %&gt;%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)\n#&gt; # A tibble: 3 × 7\n#&gt;   Analysis  Time     N Events   AHR Upper Lower\n#&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1    12  314.   67.4  0.78  0     0.05\n#&gt; 2        2    24  314.  155.   0.67  0.44  0.12\n#&gt; 3        3    36  314.  208.   0.64  0.8   0.2\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;      n  t events  ahr lower upper\n#&gt; 22 314 12  67.21 0.81  0.05  0.00\n#&gt; 23 314 24 154.43 0.67  0.12  0.44\n#&gt; 24 314 36 207.92 0.65  0.21  0.79",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#sample-size-under-fh0.5-0.5",
    "href": "wlr.html#sample-size-under-fh0.5-0.5",
    "title": "6  Weighted logrank test",
    "section": "\n6.10 Sample size under \\(FH(0.5, 0.5)\\)\n",
    "text": "6.10 Sample size under \\(FH(0.5, 0.5)\\)\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2) %&gt;%\n  select(Analysis, Bound, Time, N, Events, AHR, Probability) %&gt;%\n  tidyr::pivot_wider(names_from = Bound, values_from = Probability)\n#&gt; # A tibble: 3 × 7\n#&gt;   Analysis  Time     N Events   AHR Upper Lower\n#&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1    12  317.   68.0  0.79  0     0.05\n#&gt; 2        2    24  317.  156    0.68  0.43  0.12\n#&gt; 3        3    36  317.  210.   0.65  0.8   0.2\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;      n  t events  ahr lower upper\n#&gt; 28 317 12  67.87 0.81  0.06  0.00\n#&gt; 29 317 24 155.86 0.68  0.12  0.43\n#&gt; 30 317 36 209.82 0.66  0.20  0.80",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#average-hazard-ratio-comparison",
    "href": "wlr.html#average-hazard-ratio-comparison",
    "title": "6  Weighted logrank test",
    "section": "\n6.11 Average hazard ratio comparison",
    "text": "6.11 Average hazard ratio comparison\nThe average hazard ratio depends on the weight used in the WLR. We illustrate the difference in the figure below.\n\n\n\n\n\n\n\n\nNote that the average hazard ratio is weighted by corresponding weighted logrank weights.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#standardized-effect-size-comparison",
    "href": "wlr.html#standardized-effect-size-comparison",
    "title": "6  Weighted logrank test",
    "section": "\n6.12 Standardized effect size comparison",
    "text": "6.12 Standardized effect size comparison\nStandardized effect size of \\(FH(0, 0.5)\\) is larger than other weight function at month 36. The reason \\(FH(0.5, 0.5)\\) provides slightly smaller sample size compared with \\(FH(0, 1)\\) and \\(FH(0, 0.5)\\) is mainly due to smaller weight when variance becomes larger than \\(FH(0, 1)\\) with later events.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#information-fraction-comparison",
    "href": "wlr.html#information-fraction-comparison",
    "title": "6  Weighted logrank test",
    "section": "\n6.13 Information fraction comparison",
    "text": "6.13 Information fraction comparison\nThe logrank test information fraction curve is close to a linear function of time (the boundary calculation approach we used). Other WLR test information fraction curves are convex functions. Information fraction under the null hypothesis is smaller than information fraction under alternative at the same time.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr.html#technical-details",
    "href": "wlr.html#technical-details",
    "title": "6  Weighted logrank test",
    "section": "\n6.14 Technical details",
    "text": "6.14 Technical details\nThis section describes the details of calculating the sample size and events required for WLR under group sequential design. It can be skipped for the first read of this training material.\nWe illustrate the idea using \\(FH(0, 1)\\).\n\nweight &lt;- function(x, arm0, arm1) {\n  gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n}\n\n\n# Define study design object in each arm\ngs_arm &lt;- gsdmvn:::gs_create_arm(\n  enrollRates, failRates,\n  ratio = 1, # Randomization ratio\n  total_time = 36 # Total study duration\n)\narm0 &lt;- gs_arm[[\"arm0\"]]\narm1 &lt;- gs_arm[[\"arm1\"]]\n\nThe calculation of power is essentially the multiple integration of multivariate normal distribution, and we implement it into a function gs_power() where mvtnorm::pmvnorm() is used to take care of the multiple integration.\n\n#' Power and futility of group sequential design\n#'\n#' @param z Numerical vector of Z statistics\n#' @param corr Correlation matrix of Z statistics\n#' @param futility_bound Numerical vector of futility bound.\n#'                       -Inf indicates non-binding futility bound.\n#' @param efficacy_bound Numerical vector of efficacy bound.\n#'                       Inf indicates no early stop to declare superiority.\ngs_power &lt;- function(z, corr, futility_bound, efficacy_bound) {\n  p &lt;- c()\n  p[1] &lt;- 1 - pnorm(efficacy_bound[1], mean = z[1], sd = 1)\n\n  for (k in 2:length(z)) {\n    lower_k &lt;- c(futility_bound[1:(k - 1)], efficacy_bound[k])\n    upper_k &lt;- c(efficacy_bound[1:(k - 1)], Inf)\n    p[k] &lt;- mvtnorm::pmvnorm(\n      lower = lower_k, upper = upper_k,\n      mean = z[1:k], corr = corr[1:k, 1:k]\n    )\n  }\n\n  p\n}\n\nTo utilize gs_power(), four important blocks are required.\n\nThe expectation mean of Z statistics. To derive it, one needs \\(\\Delta_k\\), \\(\\sigma_k^{2}\\) and sample size ratio at each interim analysis.\nThe correlation matrix of Z statistics (denoted as \\(\\Sigma\\)).\nThe numerical vector of futility bound for multiple integration.\nThe numerical vector of efficacy bound for multiple integration.\n\nFirst, we calculate\n\\[\n  \\Delta_k\n  =\n  \\int_{0}^{t_k}w(s)\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)}\\{\\lambda_{1}(s)-\\lambda_{0}(s)\\}ds,\n\\]\nwhere \\(p_0 = n_0/(n_0 + n_1), p_1 = n_1/(n_0 + n_1)\\) are the randomization ratio of the control and treatment arm, respectively. \\(\\pi_0(t) = E(N_0(t)), \\pi_1(t) = E(N_1(t))\\) are the expected number of failures of the control and treatment arm, respectively. \\(\\pi(t) = p_0 \\pi_0(t) + p_1 \\pi_1(t)\\) is the probability of events. \\(\\lambda_0(t), \\lambda_1(t)\\) are hazard functions. The detailed calculation of \\(\\Delta_k\\) is implemented in gsdmvn:::gs_delta_wlr():\n\ndelta &lt;- abs(sapply(analysisTimes, function(x) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = x, weight = weight)\n}))\ndelta\n#&gt; [1] 0.002227119 0.013851909 0.026237755\n\nSecond, we calculate\n\\[\n\\sigma_k^{2}=\\int_{0}^{t_k}w(s)^{2}\\frac{p_{0}\\pi_{0}(s)p_{1}\\pi_{1}(s)}{\\pi(s)^{2}}dv(s),\n\\]\nwhere \\(v(t)= p_0 E(Y_0(t)) + p_1 E(Y_1(t))\\) is the probability of people at risk. The detailed calculation of \\(\\sigma_k^{2}\\) is implemented in gsdmvn:::gs_sigma2_wlr():\n\nsigma2 &lt;- abs(sapply(analysisTimes, function(x) {\n  gsdmvn:::gs_sigma2_wlr(arm0, arm1, tmax = x, weight = weight)\n}))\nsigma2\n#&gt; [1] 0.001411557 0.010443360 0.024267396\n\nThird, the sample size ratio at each interim analysis can be calculated by\n\n# Group sequential sample size ratio over time\ngs_n_ratio &lt;- (npsurvSS::paccr(analysisTimes, arm0) +\n  npsurvSS::paccr(analysisTimes, arm1)) / 2\ngs_n_ratio\n#&gt; [1] 1 1 1\n\nAfter getting \\(\\Delta_k\\), \\(\\sigma_k^{2}\\), and the sample size ratio, one can calculate the expectation mean of Z statistics as\n\nn &lt;- 500 # Assume a sample size to get power\ndelta / sqrt(sigma2) * sqrt(gs_n_ratio * n)\n#&gt; [1] 1.325499 3.030920 3.766172\n\nThen, we calculate the correlation matrix \\(\\Sigma\\) as\n\ncorr &lt;- outer(sqrt(sigma2), sqrt(sigma2), function(x, y) pmin(x, y) / pmax(x, y))\ncorr\n#&gt;           [,1]      [,2]      [,3]\n#&gt; [1,] 1.0000000 0.3676453 0.2411779\n#&gt; [2,] 0.3676453 1.0000000 0.6560071\n#&gt; [3,] 0.2411779 0.6560071 1.0000000\n\nFinally, for numerical vector of futility and bound, they are simply\n\nx$lower$bound\n#&gt; [1] -0.6945842  1.0023997  1.9929702\nx$upper$bound\n#&gt; [1] 3.710303 2.511407 1.992970\n\n\n6.14.1 Power and sample size calculation\n\nPower\n\nWith all blocks prepared, one can calculate the power\n\\[\n  1 -\\beta\n  =\n  \\sum_{k=1}^{K} u_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k \\ge b_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\})\n\\]\nby\n\ncumsum(gs_power(\n  delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),\n  corr,\n  x$lower$bound,\n  x$upper$bound\n))\n#&gt; [1] 0.00854411 0.69113520 0.93587653\n\n\nType I error\n\nOne can also calculate the type I error by\n\ncumsum(gs_power(\n  rep(0, length(delta)),\n  corr,\n  rep(-Inf, length(delta)),\n  x$upper$bound\n))\n#&gt; [1] 0.0001035057 0.0061027684 0.0266370410\n\n\nFutility probability\n\nSimilarly, we can calculate the futility probability\n\\[\n\\sum_{k=1}^{K} l_k = \\sum_{k=1}^{K} \\text{Pr}(\\{Z_k &lt; a_k\\} \\cap_{j=1}^{k-1} \\{a_j \\le Z_j \\le b_j\\})\n\\]\nby\n\n#' @rdname power_futility\ngs_futility &lt;- function(z, corr, futility_bound, efficacy_bound) {\n  p &lt;- c()\n  p[1] &lt;- pnorm(futility_bound[1], mean = z[1], sd = 1)\n\n  for (k in 2:length(z)) {\n    lower_k &lt;- c(futility_bound[1:(k - 1)], -Inf)\n    upper_k &lt;- c(efficacy_bound[1:(k - 1)], futility_bound[k])\n    p[k] &lt;- mvtnorm::pmvnorm(\n      lower = lower_k, upper = upper_k,\n      mean = z[1:k], corr = corr[1:k, 1:k]\n    )\n  }\n\n  p\n}\n\n\ncumsum(gs_futility(\n  delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),\n  corr, x$lower$bound, x$upper$bound\n))\n#&gt; [1] 0.02168739 0.04054411 0.06412435\n\n\nSample size\n\nIn addition to power and futility probability, one can also calculate the sample size by\n\nefficacy_bound &lt;- x$upper$bound\nfutility_bound &lt;- x$lower$bound\n\n# Sample size to event\nn &lt;- uniroot(\n  function(n, power) {\n    power - sum(gs_power(\n      delta / sqrt(sigma2) * sqrt(gs_n_ratio * n),\n      corr, futility_bound, efficacy_bound\n    ))\n  },\n  interval = c(1, 1e5), power = 1 - beta\n)$root\n\n\nn_subject &lt;- n * ratio / sum(ratio)\nn_subject * gs_n_ratio\n#&gt; [1] 316.4481 316.4481 316.4481\n\n\nNumber of events\n\nWith the sample size available, one can calculate the number of events by\n\nn0 &lt;- n1 &lt;- n_subject / 2\ngsdmvn:::prob_event.arm(arm0, tmax = analysisTimes) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = analysisTimes) * n1\n#&gt; [1]  67.96942 155.87183 209.67276\n\n\nAverage hazard ratio\n\nThe function below implements the connection between \\(\\Delta\\) and average hazard ratio using Taylor series expansion.\n\nlog_ahr &lt;- sapply(analysisTimes, function(t_k) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t_k, weight = weight) /\n    gsdmvn:::gs_delta_wlr(arm0, arm1,\n      tmax = t_k, weight = weight,\n      approx = \"generalized schoenfeld\",\n      normalization = TRUE\n    )\n})\nexp(log_ahr)\n#&gt; [1] 0.7342540 0.6372506 0.6174103\n\n\nThe info column is \\(\\sigma^2_k\\) times \\(N\\)\n\n\n\nn_subject * sigma2\n#&gt; [1] 0.4466845 3.3047813 7.6793709\n\n\nThe info0 column is \\(\\sigma^2_k\\) times \\(N\\) under the null where we both use arm0 for calculation in active and control group.\n\n6.14.2 Cross comparison with gs_design_wlr()\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  316.   68.0  3.71        0     0.73  1.58\n#&gt; 2        2 Upper    24  316.  156.   2.51        0.45  0.64  1.33\n#&gt; 3        3 Upper    36  316.  210.   1.99        0.8   0.62  1.08\n#&gt; 4        1 Lower    12  316.   68.0 -0.69        0.04  0.73  1.58\n#&gt; 5        2 Lower    24  316.  156.   1           0.11  0.64  1.33\n#&gt; 6        3 Lower    36  316.  210.   1.99        0.2   0.62  1.08\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\n6.14.3 Illustration of gs_info_wlr()\n\nThe necessary information has also been summarized in a data frame using gsdmvn::gs_info_wlr().\n\ngs_info &lt;- gsdmvn::gs_info_wlr(\n  enrollRates, failRates, ratio,\n  analysisTimes = analysisTimes,\n  weight = weight\n)\n\ngs_info %&gt;% mutate_if(is.numeric, round, digits = 2)\n#&gt;   Analysis Time   N Events  AHR delta sigma2 theta  info info0\n#&gt; 1        1   12 500 107.39 0.73  0.00   0.00  1.58  0.71  0.71\n#&gt; 2        2   24 500 246.28 0.64 -0.01   0.01  1.33  5.22  5.41\n#&gt; 3        3   36 500 331.29 0.62 -0.03   0.02  1.08 12.13 12.96\n\n\n\nN is based on the information in enrollRates\n\n\n\nN &lt;- sum(enrollRates$rate * enrollRates$duration)\nN\n#&gt; [1] 500\n\n\n\nEvents is the probability of events times \\(N\\)\n\n\n\nn0 &lt;- n1 &lt;- N / 2\ngsdmvn:::prob_event.arm(arm0, tmax = analysisTimes) * n0 +\n  gsdmvn:::prob_event.arm(arm1, tmax = analysisTimes) * n1\n#&gt; [1] 107.3943 246.2834 331.2909\n\n\n\nAHR is the average hazard ratio\n\n\nlog_ahr &lt;- sapply(analysisTimes, function(t_k) {\n  gsdmvn:::gs_delta_wlr(arm0, arm1, tmax = t_k, weight = weight) /\n    gsdmvn:::gs_delta_wlr(\n      arm0, arm1,\n      tmax = t_k,\n      weight = weight,\n      approx = \"generalized schoenfeld\",\n      normalization = TRUE\n    )\n})\nexp(log_ahr)\n#&gt; [1] 0.7342540 0.6372506 0.6174103\n\n\n\ndelta, sigma2, and theta are defined as above\n\n\ndelta / sigma2\n#&gt; [1] 1.577775 1.326384 1.081194\n\n\nThe info column is \\(\\sigma^2_k\\) times \\(N\\)\n\n\n\nN * sigma2\n#&gt; [1]  0.7057784  5.2216800 12.1336979\n\n\nThe info0 column is \\(\\sigma^2_k\\) times \\(N\\) under the null.\n\n\n\n\n\nLan, K. K. G., and David L. DeMets. 1983. “Discrete Sequential Boundaries for Clinical Trials.” Biometrika 70: 659–63.\n\n\nMagirr, Dominic, and Carl-Fredrik Burman. 2019. “Modestly Weighted Logrank Tests.” Statistics in Medicine 38 (20): 3782–90.\n\n\nScharfstein, Daniel O, Anastasios A Tsiatis, and James M Robins. 1997. “Semiparametric Efficiency and Its Implication on the Design and Analysis of Group-Sequential Studies.” Journal of the American Statistical Association 92 (440): 1342–50.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html",
    "href": "wlr-boundary.html",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "",
    "text": "7.1 Group sequential design boundary calculation strategy\nIn the last chapter, we pre-specified boundary derived from gsDesign. Therefore, the Type I error may be inflated because the information fraction is different for different WLR.\nIn this chapter, we calculate boundaries based on the error spending approach following Gordon Lan and DeMets (1983).\nThe spending function has been implemented in gsDesign.\nThere are other ways to derive boundaries but will not be covered:",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#group-sequential-design-boundary-calculation-strategy",
    "href": "wlr-boundary.html#group-sequential-design-boundary-calculation-strategy",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "",
    "text": "gsDesign::sfLDOF() (O’Brien-Fleming bound approximation) or other functions starting with sf in gsDesign.\nThe boundary family approach (Pocock 1977; O’Brien and Fleming 1979; Wang and Tsiatis 1987).\n\n\n\nConditional power (Lachin 2005).",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#types-of-error-probability",
    "href": "wlr-boundary.html#types-of-error-probability",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.2 Types of error probability",
    "text": "7.2 Types of error probability\nThere are 6 different types of error probability that have been implemented in gsdmvn. In this training material, we focus on test.type = 4.\n\n\ntest.type argument in gsDesign\nUpper bound:\n\n\\(\\alpha_k(0) = \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt; Z_i &lt; b_i, Z_k &gt; b_k \\mid H_0)\\)\n\n\\(\\alpha_k^{+}(0) = \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt; Z_i &lt; b_i, Z_k &gt; b_k \\mid H_0)\\) (ignore lower bound)\n\n\nLower bound:\n\n\n\\(\\beta_k(0) = \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt; Z_i &lt; b_i, Z_k &lt; a_k \\mid H_0)\\) (under null)\n\n\\(\\beta_k(\\delta) = \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt; Z_i &lt; b_i, Z_k &lt; a_k \\mid H_1)\\) (under alternative)\n\n\n\ntest.type\nUpper bound\nLower bound\n\n\n\n1\n\\(\\alpha_k^{+}(0)\\)\nNone\n\n\n2\n\\(\\alpha(0)\\)\n\\(\\beta_k(0)\\)\n\n\n3\n\\(\\alpha_k(0)\\)\n\\(\\beta_k(\\delta)\\)\n\n\n4\n\\(\\alpha_k^{+}(0)\\)\n\\(\\beta_k(\\delta)\\)\n\n\n5\n\\(\\alpha(0)\\)\n\\(\\beta_k(0)\\)\n\n\n6\n\\(\\alpha^{+}(0)\\)\n\\(\\beta_k(0)\\)\n\n\n\n\n\ntest.type = 1, 2, 5, 6: sample size boundaries can be computed in a single step.\n\ntest.type = 3 and test.type = 4: sample size and boundaries are set simultaneously using an iterative algorithm.\nThis section and last section focus on test.type = 4.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#information-fraction",
    "href": "wlr-boundary.html#information-fraction",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.3 Information fraction",
    "text": "7.3 Information fraction\nUnder the same design assumption, information fraction is different from different weight parameters in WLR.\nWe continue using the same example scenario in the last chapter.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#spending-function-based-on-information-fraction",
    "href": "wlr-boundary.html#spending-function-based-on-information-fraction",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.4 Spending function based on information fraction",
    "text": "7.4 Spending function based on information fraction\nThe spending function is based on information fraction. We considered the Lan-DeMets spending function to approximate an O’Brien-Fleming bound Gordon Lan and DeMets (1983). (gsDesign::sfLDOF()).\nHere, \\(t\\) is information fraction in the formula below.\n\\[f(t; \\alpha)=2-2\\Phi\\left(\\Phi^{-1}\\left(\\frac{1-\\alpha/2}{t^{\\rho/2}}\\right)\\right)\\]\n\n7.4.1 Spending function in gsDesign\nAfter the spending function is selected, we can calculate the lower and upper bound of a group sequential design.\nIn test type 4, the lower bound is non-binding. So we set lower bound are all -Inf when we calculate the probability to cross upper bound.\nWe first use the alpha spending function to determine the upper bound of a group sequential design\n\nLet \\((a_k, b_k), k=1,\\dots, K\\) denotes the lower and upper bound.\n\nFor gsDesign with logrank test, we considered equal increments of information fraction at t = 1:3 / 3 is.\nThe upper bound and lower bound based on the Lan-DeMets spending function can be calculated using gsDesign::sfLDOF().\n\nUpper bound:\n\n\nalpha_spend &lt;- gsDesign::sfLDOF(alpha = 0.025, t = 1:3 / 3)$spend\nalpha_spend\n#&gt; [1] 0.0001035057 0.0060483891 0.0250000000\n\n\nLower bound:\n\n\nbeta_spend &lt;- gsDesign::sfLDOF(alpha = 0.2, t = 1:3 / 3)$spend\nbeta_spend\n#&gt; [1] 0.02643829 0.11651432 0.20000000\n\nWe considered different WLR tests with weight functions: \\(FH(0, 0)\\), \\(FH(0.5, 0.5)\\), \\(FH(0, 0.5)\\), \\(FH(0, 1)\\)\n\nweight_fun &lt;- list(\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  }\n)\n\n# Weight name\nweight_name &lt;- data.frame(rho = c(0, 0.5, 0, 0), gamma = c(0, 0.5, 0.5, 1))\nweight_name &lt;- with(weight_name, paste0(\"rho = \", rho, \"; gamma = \", gamma))\n\nFor test type 4, the alpha and beta is defined as below.\n\n\n\\(\\alpha_k^{+}(0) = \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt; Z_i &lt; b_i, Z_k &gt; b_k \\mid H_0)\\) (ignore lower bound)\n\n\n\\(\\alpha_k^{+}(0)\\) is calculated under the null. It is the same for different sample size.\n\n\n\n\\(\\beta_k(\\delta) =  \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt; Z_i &lt; b_i, Z_k &lt; a_k \\mid H_1)\\) (under alternative)\n\n\n\\(\\beta_k(\\delta)\\) is calculated under the alternative. It depends on the sample size.\nIteration is required to find proper sample size and boundary.\n\n\n\nThe table below provide the cumulative alpha and beta at different analysis time for each WLR test.\nwe draw the Alpha spending (\\(\\alpha=0.025\\)) function based on information fraction at 12, 24 and 36 months\n\n\n\n\n\n\n\n\nSimilarly, we draw the Beta spending (\\(\\beta=0.2\\)) function based on information fraction at 12, 24 and 36 months\n\n\n\n\n\n\n\n\n\nanalysisTimes &lt;- c(12, 24, 36)\ngs_spend &lt;- lapply(weight_fun, function(weight) {\n  tmp &lt;- gsdmvn::gs_info_wlr(\n    enrollRates, failRates,\n    analysisTimes = analysisTimes,\n    weight = weight\n  )\n\n  tmp %&gt;% mutate(\n    theta = abs(delta) / sqrt(sigma2),\n    info = info / max(info),\n    info0 = info0 / max(info0),\n    alpha = gsDesign::sfLDOF(alpha = 0.025, t = info0)$spend,\n    beta = gsDesign::sfLDOF(alpha = 0.20, t = info)$spend\n  )\n})\n\n\nnames(gs_spend) &lt;- weight_name\nbind_rows(gs_spend, .id = \"weight\") %&gt;%\n  select(weight, Time, alpha, beta) %&gt;%\n  mutate_if(is.numeric, round, digits = 3)\n#&gt;                    weight Time alpha  beta\n#&gt; 1      rho = 0; gamma = 0   12 0.000 0.025\n#&gt; 2      rho = 0; gamma = 0   24 0.009 0.139\n#&gt; 3      rho = 0; gamma = 0   36 0.025 0.200\n#&gt; 4  rho = 0.5; gamma = 0.5   12 0.000 0.003\n#&gt; 5  rho = 0.5; gamma = 0.5   24 0.006 0.118\n#&gt; 6  rho = 0.5; gamma = 0.5   36 0.025 0.200\n#&gt; 7    rho = 0; gamma = 0.5   12 0.000 0.000\n#&gt; 8    rho = 0; gamma = 0.5   24 0.003 0.088\n#&gt; 9    rho = 0; gamma = 0.5   36 0.025 0.200\n#&gt; 10     rho = 0; gamma = 1   12 0.000 0.000\n#&gt; 11     rho = 0; gamma = 1   24 0.001 0.051\n#&gt; 12     rho = 0; gamma = 1   36 0.025 0.200",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#lower-and-upper-bound",
    "href": "wlr-boundary.html#lower-and-upper-bound",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.5 Lower and upper bound",
    "text": "7.5 Lower and upper bound\nLet’s calculate the lower and upper bound of the first interim analysis.\n\nFirst interim analysis upper bound: \\(\\text{Pr}(Z_1 &gt; b_1 \\mid H_0)\\)\n\n\n\n-qnorm(gs_spend[[1]]$alpha[1])\n#&gt; [1] 3.790778\n\n\nFirst interim analysis lower bound \\(\\text{Pr}(Z_1 &lt; b_1 \\mid H_1)\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe lower bound is calculated under alternative hypothesis and depends on sample size.\n\n\n\nn &lt;- 400\nmean &lt;- gs_spend[[1]]$theta[1] * sqrt(n)\nqnorm(gs_spend[[1]]$beta[1], mean = mean, sd = 1)\n#&gt; [1] -1.159606\n\n\nn &lt;- 500\nmean &lt;- gs_spend[[1]]$theta[1] * sqrt(n)\nqnorm(gs_spend[[1]]$beta[1], mean = mean, sd = 1)\n#&gt; [1] -1.065469\n\nThe figure below illustrates the lower and upper bound in different sample size\n\nA larger sample size has a larger lower bound (solid line compared with dashed line)\nIteration is required to find proper sample size and boundary.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#sample-size-calculation-logrank-test-based-on-ahr",
    "href": "wlr-boundary.html#sample-size-calculation-logrank-test-based-on-ahr",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.6 Sample size calculation logrank test based on AHR",
    "text": "7.6 Sample size calculation logrank test based on AHR\n\nSample size\n\n\nx &lt;- gsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  379.   81.4  3.77        0     0.84  0.17\n#&gt; 2        2 Upper    24  379.  187.   2.35        0.47  0.71  0.34\n#&gt; 3        3 Upper    36  379.  251.   2.01        0.8   0.68  0.38\n#&gt; 4        1 Lower    12  379.   81.4 -1.19        0.02  0.84  0.17\n#&gt; 5        2 Lower    24  379.  187.   1.13        0.14  0.71  0.34\n#&gt; 6        3 Lower    36  379.  251.   2.01        0.2   0.68  0.38\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;     n  t events  ahr lower upper\n#&gt; 1 379 12  81.23 0.86  0.02  0.00\n#&gt; 2 379 24 186.50 0.72  0.13  0.48\n#&gt; 3 379 36 251.05 0.69  0.19  0.81\n\n\nType I error\n\n\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]\n#&gt; [1] 8.162377e-05 9.415389e-03 2.505418e-02\n\n\nCompared with fixed design\n\n\ngsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds\n#&gt; # A tibble: 1 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    36  328.   217.  1.96         0.8 0.683 0.381\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#sample-size-calculation-logrank-test-based-on-wlr-fh0-0",
    "href": "wlr-boundary.html#sample-size-calculation-logrank-test-based-on-wlr-fh0-0",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.7 Sample size calculation logrank test based on WLR \\(FH(0, 0)\\)\n",
    "text": "7.7 Sample size calculation logrank test based on WLR \\(FH(0, 0)\\)\n\n\nSample size\n\n\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  377.    81   3.79        0     0.84  0.17\n#&gt; 2        2 Upper    24  377.   186.  2.36        0.46  0.72  0.33\n#&gt; 3        3 Upper    36  377.   250.  2.01        0.8   0.68  0.38\n#&gt; 4        1 Lower    12  377.    81  -1.18        0.03  0.84  0.17\n#&gt; 5        2 Lower    24  377.   186.  1.15        0.14  0.72  0.33\n#&gt; 6        3 Lower    36  377.   250.  2.01        0.2   0.68  0.38\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;     n  t events  ahr lower upper\n#&gt; 1 378 12  80.98 0.86  0.03  0.00\n#&gt; 2 378 24 186.10 0.72  0.14  0.47\n#&gt; 3 378 36 250.33 0.69  0.20  0.80\n\n\nType I error\n\n\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]\n#&gt; [1] 7.532364e-05 9.164191e-03 2.497474e-02\n\n\nCompared with fixed design\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds\n#&gt; # A tibble: 1 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    36  324.   215.  1.96         0.8 0.683 0.381\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#sample-size-calculation-fh0-1",
    "href": "wlr-boundary.html#sample-size-calculation-fh0-1",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.8 Sample size calculation \\(FH(0, 1)\\)\n",
    "text": "7.8 Sample size calculation \\(FH(0, 1)\\)\n\n\nSample size\n\n\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events      Z Probability   AHR\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  287.   61.6 Inf           0     0.73\n#&gt; 2        2 Upper    24  287.  141.    3.28        0.16  0.64\n#&gt; 3        3 Upper    36  287.  190.    1.96        0.8   0.62\n#&gt; 4        1 Lower    12  287.   61.6  -4.18        0     0.73\n#&gt; 5        2 Lower    24  287.  141.    0.66        0.05  0.64\n#&gt; 6        3 Lower    36  287.  190.    1.96        0.2   0.62\n#&gt; # ℹ 3 more variables: theta &lt;dbl&gt;, info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;      n  t events  ahr lower upper\n#&gt; 10 287 12  61.64 0.77  0.00  0.00\n#&gt; 11 287 24 141.41 0.65  0.05  0.17\n#&gt; 12 287 36 190.20 0.63  0.20  0.80\n\n\nType I error\n\n\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]\n#&gt; [1] 0.0000000000 0.0005191052 0.0251730067\n\n\nCompared with fixed design\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 1)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds\n#&gt; # A tibble: 1 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    36  264.   175.  1.96         0.8 0.617  1.08\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#sample-size-calculation-fh0-0.5",
    "href": "wlr-boundary.html#sample-size-calculation-fh0-0.5",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.9 Sample size calculation \\(FH(0, 0.5)\\)\n",
    "text": "7.9 Sample size calculation \\(FH(0, 0.5)\\)\n\n\nSample size\n\n\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  288.   61.9  6.18        0     0.78  0.63\n#&gt; 2        2 Upper    24  288.  142    2.8         0.3   0.67  0.77\n#&gt; 3        3 Upper    36  288.  191.   1.97        0.8   0.64  0.73\n#&gt; 4        1 Lower    12  288.   61.9 -2.43        0     0.78  0.63\n#&gt; 5        2 Lower    24  288.  142    0.93        0.09  0.67  0.77\n#&gt; 6        3 Lower    36  288.  191.   1.97        0.2   0.64  0.73\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;     n  t events  ahr lower upper\n#&gt; 7 289 12  62.11 0.81  0.00   0.0\n#&gt; 8 289 24 142.28 0.68  0.09   0.3\n#&gt; 9 289 36 191.44 0.65  0.20   0.8\n\n\nType I error\n\n\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]\n#&gt; [1] 3.205080e-10 2.555246e-03 2.523428e-02\n\n\nCompared with fixed design\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds\n#&gt; # A tibble: 1 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    36  261.   173.  1.96         0.8 0.639 0.732\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "wlr-boundary.html#sample-size-calculation-fh0.5-0.5",
    "href": "wlr-boundary.html#sample-size-calculation-fh0.5-0.5",
    "title": "7  Group sequential design boundary with weighted logrank test",
    "section": "\n7.10 Sample size calculation \\(FH(0.5, 0.5)\\)\n",
    "text": "7.10 Sample size calculation \\(FH(0.5, 0.5)\\)\n\n\nSample size\n\n\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  304.   65.3  5.05        0     0.79  0.68\n#&gt; 2        2 Upper    24  304.  150.   2.51        0.42  0.68  0.93\n#&gt; 3        3 Upper    36  304.  201.   1.99        0.8   0.65  0.97\n#&gt; 4        1 Lower    12  304.   65.3 -1.8         0     0.79  0.68\n#&gt; 5        2 Lower    24  304.  150.   1.12        0.12  0.68  0.93\n#&gt; 6        3 Lower    36  304.  201.   1.99        0.2   0.65  0.97\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\nSimulation results based on 10,000 replications.\n\n\nload(\"simulation/simu_gsd_wlr_boundary.Rdata\")\nsimu_res %&gt;%\n  subset(rho == 0.5 & gamma == 0.5) %&gt;%\n  select(-scenario, -rho, -gamma) %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt;     n  t events  ahr lower upper\n#&gt; 4 304 12  65.10 0.81  0.00  0.00\n#&gt; 5 304 24 149.51 0.68  0.12  0.42\n#&gt; 6 304 36 201.23 0.66  0.20  0.80\n\n\nType I error\n\n\ngsdmvn::gs_power_npe(\n  theta = rep(0, length(analysisTimes)),\n  info = x$info0[x$Bound == \"Upper\"],\n  upar = x$Z[x$Bound == \"Upper\"],\n  lpar = rep(-Inf, 3)\n)$Probability[1:length(analysisTimes)]\n#&gt; [1] 2.209050e-07 6.036759e-03 2.515345e-02\n\n\nCompared with fixed design\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  ratio = 1, alpha = 0.025, beta = 0.2,\n  upar = -qnorm(0.025),\n  lpar = -qnorm(0.025),\n  analysisTimes = 36\n)$bounds\n#&gt; # A tibble: 1 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    36  269.   178.  1.96         0.8 0.650 0.973\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\n\n\n\nGordon Lan, KK, and David L DeMets. 1983. “Discrete Sequential Boundaries for Clinical Trials.” Biometrika 70 (3): 659–63.\n\n\nLachin, John M. 2005. “A Review of Methods for Futility Stopping Based on Conditional Power.” Statistics in Medicine 24 (18): 2747–64.\n\n\nO’Brien, Peter C, and Thomas R Fleming. 1979. “A Multiple Testing Procedure for Clinical Trials.” Biometrics, 549–56.\n\n\nPocock, Stuart J. 1977. “Group Sequential Methods in the Design and Analysis of Clinical Trials.” Biometrika 64 (2): 191–99.\n\n\nWang, Samuel K, and Anastasios A Tsiatis. 1987. “Approximately Optimal One-Parameter Boundaries for Group Sequential Trials.” Biometrics, 193–99.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Group sequential design boundary with weighted logrank test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html",
    "href": "maxcombo.html",
    "title": "8  MaxCombo test",
    "section": "",
    "text": "8.1 MaxCombo test with interim analysis\n\\[\nZ_{ik}=\\sqrt{\\frac{n_{0}+n_{1}}{n_{0}n_{1}}}\\int_{0}^{t_k}w_i(t)\\frac{\\overline{Y}_{0}(t)\\overline{Y}_{1}(t)}{\\overline{Y}_{0}(t)+\\overline{Y}_{0}(t)}\\left\\{ \\frac{d\\overline{N}_{1}(t)}{\\overline{Y}_{1}(t)}-\\frac{d\\overline{N}_{0}(t)}{\\overline{Y}_{0}(t)}\\right\\}\n\\]",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#maxcombo-test-with-interim-analysis",
    "href": "maxcombo.html#maxcombo-test-with-interim-analysis",
    "title": "8  MaxCombo test",
    "section": "",
    "text": "\\(G_k = \\max\\{Z_{1k}, Z_{2k}, \\ldots \\}\\)\nTest statistics: analysis at \\(t_k\\) for weight \\(w_i(t)\\)\n\n\n\nNot necessary to have the same number of tests in each interim analysis",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#examples",
    "href": "maxcombo.html#examples",
    "title": "8  MaxCombo test",
    "section": "\n8.2 Examples",
    "text": "8.2 Examples\nWe continue to use the same example scenario from the last chapter.\n\n8.2.1 Example 1\n\nUsing logrank test in all interim analyses and a MaxCombo test \\(Z_{1k}: FH(0,0)\\), \\(Z_{2k}: FH(0,0.5)\\), \\(Z_{3k}: FH(0.5,0.5)\\) in final analysis.\n\n\nfh_test1 &lt;- rbind(\n  data.frame(\n    rho = 0, gamma = 0, tau = -1,\n    test = 1,\n    Analysis = 1:3,\n    analysisTimes = c(12, 24, 36)\n  ),\n  data.frame(\n    rho = c(0, 0.5), gamma = 0.5, tau = -1,\n    test = 2:3,\n    Analysis = 3, analysisTimes = 36\n  )\n)\nfh_test1\n#&gt;   rho gamma tau test Analysis analysisTimes\n#&gt; 1 0.0   0.0  -1    1        1            12\n#&gt; 2 0.0   0.0  -1    1        2            24\n#&gt; 3 0.0   0.0  -1    1        3            36\n#&gt; 4 0.0   0.5  -1    2        3            36\n#&gt; 5 0.5   0.5  -1    3        3            36\n\n\n8.2.2 Example 2\n\nUsing \\(Z_{1k}: FH(0,0)\\) and \\(Z_{2k}: FH(0,0.5)\\).\n\n\nfh_test2 &lt;- data.frame(\n  rho = c(0, 0), gamma = c(0, 0.5), tau = -1,\n  analysisTimes = rep(c(12, 24, 36), each = 2),\n  Analysis = rep(1:3, each = 2),\n  test = rep(1:2, 3)\n)\nfh_test2\n#&gt;   rho gamma tau analysisTimes Analysis test\n#&gt; 1   0   0.0  -1            12        1    1\n#&gt; 2   0   0.5  -1            12        1    2\n#&gt; 3   0   0.0  -1            24        2    1\n#&gt; 4   0   0.5  -1            24        2    2\n#&gt; 5   0   0.0  -1            36        3    1\n#&gt; 6   0   0.5  -1            36        3    2",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#sample-size-calculation",
    "href": "maxcombo.html#sample-size-calculation",
    "title": "8  MaxCombo test",
    "section": "\n8.3 Sample size calculation",
    "text": "8.3 Sample size calculation\nWe first consider a user-defined lower and upper bound using gsDesign bound\nIn general, gsDesign bound cannot be directly used for MaxCombo test:\n\nMultiple test statistics are considered in interim analysis or final analysis.\n\nWe will explain the way to derive bound using spending function in the next chapter.\n\nx &lt;- gsDesign::gsSurv(\n  k = 3, test.type = 4, alpha = 0.025,\n  beta = 0.2, astar = 0, timing = c(1),\n  sfu = sfLDOF, sfupar = c(0), sfl = sfLDOF,\n  sflpar = c(0), lambdaC = c(0.1),\n  hr = 0.6, hr0 = 1, eta = 0.01,\n  gamma = c(10),\n  R = c(12), S = NULL,\n  T = 36, minfup = 24, ratio = 1\n)\n\n\nx$upper$bound\n#&gt; [1] 3.710303 2.511407 1.992970\n\n\nx$lower$bound\n#&gt; [1] -0.2361874  1.1703638  1.9929702\n\n\n8.3.1 Example 1\nSample size can be calculated using gsdmvn::gs_design_combo().\n\ngsdmvn::gs_design_combo(enrollRates,\n  failRates,\n  fh_test1,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE,\n  upar = x$upper$bound,\n  lpar = x$lower$bound\n) %&gt;%\n  mutate(`Probability_Null (%)` = Probability_Null * 100) %&gt;%\n  select(-Probability_Null) %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt;   Analysis Bound Time      N Events     Z Probability\n#&gt; 1        1 Upper   12 444.81  95.54  3.71        0.00\n#&gt; 3        2 Upper   24 444.81 219.10  2.51        0.47\n#&gt; 5        3 Upper   36 444.81 294.72  1.99        0.80\n#&gt; 2        1 Lower   12 444.81  95.54 -0.24        0.14\n#&gt; 4        2 Lower   24 444.81 219.10  1.17        0.19\n#&gt; 6        3 Lower   36 444.81 294.72  1.99        0.20\n#&gt;   Probability_Null (%)\n#&gt; 1                 0.01\n#&gt; 3                 0.61\n#&gt; 5                 3.26\n#&gt; 2                   NA\n#&gt; 4                   NA\n#&gt; 6                   NA\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;     n      d analysisTimes lower upper\n#&gt; 1 445  95.46            12  0.14  0.00\n#&gt; 2 445 219.07            24  0.19  0.47\n#&gt; 3 445 294.71            36  0.20  0.80\n\n\nCompared with group sequential design with logrank test (based on AHR).\n\n\ngsdmvn::gs_design_ahr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  469.   101.  3.71        0     0.84  0.17\n#&gt; 2        2 Upper    24  469.   231.  2.51        0.49  0.71  0.34\n#&gt; 3        3 Upper    36  469.   311.  1.99        0.8   0.68  0.38\n#&gt; 4        1 Lower    12  469.   101. -0.24        0.13  0.84  0.17\n#&gt; 5        2 Lower    24  469.   231.  1.17        0.17  0.71  0.34\n#&gt; 6        3 Lower    36  469.   311.  1.99        0.2   0.68  0.38\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\n8.3.2 Example 2\n\ngs_design_combo(enrollRates,\n  failRates,\n  fh_test2,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE,\n  upar = x$upper$bound,\n  lpar = x$lower$bound\n) %&gt;%\n  mutate(`Probability_Null (%)` = Probability_Null * 100) %&gt;%\n  select(-Probability_Null) %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt;   Analysis Bound Time      N Events     Z Probability\n#&gt; 1        1 Upper   12 348.21  74.79  3.71        0.00\n#&gt; 3        2 Upper   24 348.21 171.52  2.51        0.49\n#&gt; 5        3 Upper   36 348.21 230.72  1.99        0.80\n#&gt; 2        1 Lower   12 348.21  74.79 -0.24        0.10\n#&gt; 4        2 Lower   24 348.21 171.52  1.17        0.15\n#&gt; 6        3 Lower   36 348.21 230.72  1.99        0.20\n#&gt;   Probability_Null (%)\n#&gt; 1                 0.02\n#&gt; 3                 0.84\n#&gt; 5                 3.31\n#&gt; 2                   NA\n#&gt; 4                   NA\n#&gt; 6                   NA\n\n\nSimulation results based on 10,000 replications.\n\n\n#&gt;     n      d analysisTimes lower upper\n#&gt; 4 349  74.85            12  0.10  0.00\n#&gt; 5 349 171.79            24  0.16  0.48\n#&gt; 6 349 231.20            36  0.20  0.80\n\n\nCompared with group sequential design with FH(0, 0.5)\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  368.   79.0  3.71        0     0.78  0.63\n#&gt; 2        2 Upper    24  368.  181.   2.51        0.5   0.67  0.77\n#&gt; 3        3 Upper    36  368.  244.   1.99        0.8   0.64  0.73\n#&gt; 4        1 Lower    12  368.   79.0 -0.24        0.1   0.78  0.63\n#&gt; 5        2 Lower    24  368.  181.   1.17        0.16  0.67  0.77\n#&gt; 6        3 Lower    36  368.  244.   1.99        0.2   0.64  0.73\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\n\n\nCompared with group sequential design with FH(0.5, 0.5)\n\n\ngsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  ratio = ratio, alpha = alpha, beta = beta,\n  upar = x$upper$bound,\n  lpar = x$lower$bound,\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  372.   79.8  3.71        0     0.79  0.68\n#&gt; 2        2 Upper    24  372.  183.   2.51        0.5   0.68  0.93\n#&gt; 3        3 Upper    36  372.  246.   1.99        0.8   0.65  0.97\n#&gt; 4        1 Lower    12  372.   79.8 -0.24        0.11  0.79  0.68\n#&gt; 5        2 Lower    24  372.  183.   1.17        0.16  0.68  0.93\n#&gt; 6        3 Lower    36  372.  246.   1.99        0.2   0.65  0.97\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#outline-of-technical-details",
    "href": "maxcombo.html#outline-of-technical-details",
    "title": "8  MaxCombo test",
    "section": "\n8.4 Outline of technical details",
    "text": "8.4 Outline of technical details\nWe describe the details of calculating the sample size and events required for WLR under fixed design.\nIt can be skipped for the first read of this training material.\n\n8.4.1 With a pre-defined upper and lower bound\n\nDerive correlations between test and analysis time point\nDerive effect size\nPower and sample size calculation",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#maxcombo-sequential-test-correlation-matrix",
    "href": "maxcombo.html#maxcombo-sequential-test-correlation-matrix",
    "title": "8  MaxCombo test",
    "section": "\n8.5 MaxCombo sequential test correlation matrix",
    "text": "8.5 MaxCombo sequential test correlation matrix\n\nReference: Section 3.1 of Wang, Luo, and Zheng (2019)\n\n\n\\(Z_{ij}\\): \\(i\\)-th test in \\(j\\)-th analysis.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#between-test-correlation",
    "href": "maxcombo.html#between-test-correlation",
    "title": "8  MaxCombo test",
    "section": "\n8.6 Between test correlation",
    "text": "8.6 Between test correlation\n\nWithin each interim analysis, the correlation between tests.\n\nRecall the discussion in fixed design\n\n\n\n\\[\n\\hbox{Cov}(Z_{1k}, Z_{2k}) = \\hbox{Var}(Z_k(\\frac{\\rho_1 + \\rho_{2}}{2}, \\frac{\\gamma_1 + \\gamma_2}{2}, \\tau))\n\\]\n\nEven though one test is used in all interim analysis, correlation of all tests are needed in each interim analysis.\n\n\n8.6.1 Example 1\n\nu_fh_test1 &lt;- unique(fh_test1[, c(\"test\", \"rho\", \"gamma\", \"tau\")])\nu_fh_test1\n#&gt;   test rho gamma tau\n#&gt; 1    1 0.0   0.0  -1\n#&gt; 4    2 0.0   0.5  -1\n#&gt; 5    3 0.5   0.5  -1\n\n\ncorr_test1 &lt;- with(\n  u_fh_test1,\n  lapply(analysisTimes, function(tmax) {\n    cov2cor(gsdmvn:::gs_sigma2_combo(arm0, arm1,\n      tmax = tmax,\n      rho = rho, gamma = gamma, tau = tau\n    ))\n  })\n)\nnames(corr_test1) &lt;- analysisTimes\ncorr_test1\n#&gt; $`12`\n#&gt;           [,1]      [,2]      [,3]\n#&gt; [1,] 1.0000000 0.9277654 0.9415781\n#&gt; [2,] 0.9277654 1.0000000 0.9986153\n#&gt; [3,] 0.9415781 0.9986153 1.0000000\n#&gt; \n#&gt; $`24`\n#&gt;           [,1]      [,2]      [,3]\n#&gt; [1,] 1.0000000 0.9407774 0.9612878\n#&gt; [2,] 0.9407774 1.0000000 0.9955313\n#&gt; [3,] 0.9612878 0.9955313 1.0000000\n#&gt; \n#&gt; $`36`\n#&gt;           [,1]      [,2]      [,3]\n#&gt; [1,] 1.0000000 0.9417454 0.9690488\n#&gt; [2,] 0.9417454 1.0000000 0.9894930\n#&gt; [3,] 0.9690488 0.9894930 1.0000000\n\n\n8.6.2 Example 2\n\nu_fh_test2 &lt;- unique(fh_test2[, c(\"test\", \"rho\", \"gamma\", \"tau\")])\nu_fh_test2\n#&gt;   test rho gamma tau\n#&gt; 1    1   0   0.0  -1\n#&gt; 2    2   0   0.5  -1\n\n\ncorr_test2 &lt;- with(\n  unique(fh_test2[, c(\"rho\", \"gamma\", \"tau\")]),\n  lapply(analysisTimes, function(tmax) {\n    cov2cor(gsdmvn:::gs_sigma2_combo(arm0, arm1,\n      tmax = tmax,\n      rho = rho, gamma = gamma, tau = tau\n    ))\n  })\n)\nnames(corr_test2) &lt;- analysisTimes\ncorr_test2\n#&gt; $`12`\n#&gt;           [,1]      [,2]\n#&gt; [1,] 1.0000000 0.9277654\n#&gt; [2,] 0.9277654 1.0000000\n#&gt; \n#&gt; $`24`\n#&gt;           [,1]      [,2]\n#&gt; [1,] 1.0000000 0.9407774\n#&gt; [2,] 0.9407774 1.0000000\n#&gt; \n#&gt; $`36`\n#&gt;           [,1]      [,2]\n#&gt; [1,] 1.0000000 0.9417454\n#&gt; [2,] 0.9417454 1.0000000",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#between-analysis-correlation",
    "href": "maxcombo.html#between-analysis-correlation",
    "title": "8  MaxCombo test",
    "section": "\n8.7 Between analysis correlation",
    "text": "8.7 Between analysis correlation\n\nWithin each test, the correlation among interim analysis.\n\nRecall the discussion in group sequential design for weighted logrank test.\n\n\n\n\n8.7.1 Example 1\n\ninfo1 &lt;- gsdmvn:::gs_info_combo(enrollRates, failRates, ratio,\n  analysisTimes = analysisTimes,\n  rho = u_fh_test1$rho,\n  gamma = u_fh_test1$gamma\n)\ninfo1 %&gt;% round(digits = 2)\n#&gt;   test Analysis Time   N Events  AHR delta sigma2 theta  info\n#&gt; 1    1        1   12 500 107.39 0.84 -0.01   0.05  0.17 26.84\n#&gt; 2    1        2   24 500 246.28 0.72 -0.04   0.12  0.33 61.35\n#&gt; 3    1        3   36 500 331.29 0.68 -0.06   0.16  0.38 81.92\n#&gt; 4    2        1   12 500 107.39 0.78  0.00   0.01  0.63  3.60\n#&gt; 5    2        2   24 500 246.28 0.67 -0.02   0.03  0.77 15.37\n#&gt; 6    2        3   36 500 331.29 0.64 -0.04   0.05  0.73 27.21\n#&gt; 7    3        1   12 500 107.39 0.79  0.00   0.01  0.68  2.90\n#&gt; 8    3        2   24 500 246.28 0.68 -0.02   0.02  0.93 10.15\n#&gt; 9    3        3   36 500 331.29 0.65 -0.03   0.03  0.97 15.07\n#&gt;   info0\n#&gt; 1 26.90\n#&gt; 2 62.09\n#&gt; 3 83.94\n#&gt; 4  3.62\n#&gt; 5 15.74\n#&gt; 6 28.48\n#&gt; 7  2.91\n#&gt; 8 10.33\n#&gt; 9 15.53\n\n\ninfo &lt;- info1\ninfo_split &lt;- split(info, info$test)\ncorr_time1 &lt;- lapply(info_split, function(x) {\n  corr &lt;- with(x, outer(sqrt(info), sqrt(info), function(x, y) pmin(x, y) / pmax(x, y)))\n  rownames(corr) &lt;- analysisTimes\n  colnames(corr) &lt;- analysisTimes\n  corr\n})\ncorr_time1\n#&gt; $`1`\n#&gt;           12        24        36\n#&gt; 12 1.0000000 0.6614295 0.5724133\n#&gt; 24 0.6614295 1.0000000 0.8654185\n#&gt; 36 0.5724133 0.8654185 1.0000000\n#&gt; \n#&gt; $`2`\n#&gt;           12        24        36\n#&gt; 12 1.0000000 0.4842835 0.3640177\n#&gt; 24 0.4842835 1.0000000 0.7516625\n#&gt; 36 0.3640177 0.7516625 1.0000000\n#&gt; \n#&gt; $`3`\n#&gt;           12        24        36\n#&gt; 12 1.0000000 0.5341938 0.4385035\n#&gt; 24 0.5341938 1.0000000 0.8208697\n#&gt; 36 0.4385035 0.8208697 1.0000000\n\n\n8.7.2 Example 2\n\ninfo2 &lt;- gsdmvn:::gs_info_combo(enrollRates, failRates, ratio,\n  analysisTimes = analysisTimes,\n  rho = u_fh_test2$rho,\n  gamma = u_fh_test2$gamma\n)\ninfo2 %&gt;% round(digits = 2)\n#&gt;   test Analysis Time   N Events  AHR delta sigma2 theta  info\n#&gt; 1    1        1   12 500 107.39 0.84 -0.01   0.05  0.17 26.84\n#&gt; 2    1        2   24 500 246.28 0.72 -0.04   0.12  0.33 61.35\n#&gt; 3    1        3   36 500 331.29 0.68 -0.06   0.16  0.38 81.92\n#&gt; 4    2        1   12 500 107.39 0.78  0.00   0.01  0.63  3.60\n#&gt; 5    2        2   24 500 246.28 0.67 -0.02   0.03  0.77 15.37\n#&gt; 6    2        3   36 500 331.29 0.64 -0.04   0.05  0.73 27.21\n#&gt;   info0\n#&gt; 1 26.90\n#&gt; 2 62.09\n#&gt; 3 83.94\n#&gt; 4  3.62\n#&gt; 5 15.74\n#&gt; 6 28.48\n\n\ninfo &lt;- info2\ninfo_split &lt;- split(info, info$test)\ncorr_time2 &lt;- lapply(info_split, function(x) {\n  corr &lt;- with(x, outer(sqrt(info), sqrt(info), function(x, y) pmin(x, y) / pmax(x, y)))\n  rownames(corr) &lt;- analysisTimes\n  colnames(corr) &lt;- analysisTimes\n  corr\n})\ncorr_time2\n#&gt; $`1`\n#&gt;           12        24        36\n#&gt; 12 1.0000000 0.6614295 0.5724133\n#&gt; 24 0.6614295 1.0000000 0.8654185\n#&gt; 36 0.5724133 0.8654185 1.0000000\n#&gt; \n#&gt; $`2`\n#&gt;           12        24        36\n#&gt; 12 1.0000000 0.4842835 0.3640177\n#&gt; 24 0.4842835 1.0000000 0.7516625\n#&gt; 36 0.3640177 0.7516625 1.0000000",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#correlation-matrix-for-all-tests-across-analysis",
    "href": "maxcombo.html#correlation-matrix-for-all-tests-across-analysis",
    "title": "8  MaxCombo test",
    "section": "\n8.8 Correlation matrix for all tests across analysis",
    "text": "8.8 Correlation matrix for all tests across analysis\n\nReference: Section 3.1 of Wang, Luo, and Zheng (2019)\n\n\n\\(Z_{ij}\\): \\(i\\)-th test in \\(j\\)-th analysis.\n\n\\[\n\\hbox{Cor}(Z_{11}, Z_{22}) = \\hbox{Cor}(Z_{22}, Z_{11}) \\approx \\hbox{Cor}(Z_{11}, Z_{21}) \\hbox{Cor}(Z_{21}Z_{22})\n\\]\nwhich implies\n\\[\n\\hbox{Cor}(Z_{11}, Z_{22}) = \\frac{\\hbox{Cov}(Z_{11}, Z_{21})} {\\sqrt{\\hbox{Var}(Z_{11})\\hbox{Var}(Z_{22})}}\n\\]\n\n8.8.1 Example 1\n\ncorr_test &lt;- corr_test1\ncorr_time &lt;- corr_time1\ninfo &lt;- info1\n# Overall Correlation\ncorr_combo &lt;- diag(1, nrow = nrow(info))\nfor (i in 1:nrow(info)) {\n  for (j in 1:nrow(info)) {\n    t1 &lt;- as.numeric(info$Analysis[i])\n    t2 &lt;- as.numeric(info$Analysis[j])\n    if (t1 &lt;= t2) {\n      test1 &lt;- as.numeric(info$test[i])\n      test2 &lt;- as.numeric(info$test[j])\n      corr_combo[i, j] &lt;- corr_test[[t1]][test1, test2] * corr_time[[test2]][t1, t2]\n      corr_combo[j, i] &lt;- corr_combo[i, j]\n    }\n  }\n}\ncorr_combo1 &lt;- corr_combo\ncorr_combo1 %&gt;% round(2)\n#&gt;       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n#&gt;  [1,] 1.00 0.66 0.57 0.93 0.45 0.34 0.94 0.50 0.41\n#&gt;  [2,] 0.66 1.00 0.87 0.61 0.94 0.71 0.62 0.96 0.79\n#&gt;  [3,] 0.57 0.87 1.00 0.53 0.81 0.94 0.54 0.83 0.97\n#&gt;  [4,] 0.93 0.61 0.53 1.00 0.48 0.36 1.00 0.53 0.44\n#&gt;  [5,] 0.45 0.94 0.81 0.48 1.00 0.75 0.48 1.00 0.82\n#&gt;  [6,] 0.34 0.71 0.94 0.36 0.75 1.00 0.36 0.75 0.99\n#&gt;  [7,] 0.94 0.62 0.54 1.00 0.48 0.36 1.00 0.53 0.44\n#&gt;  [8,] 0.50 0.96 0.83 0.53 1.00 0.75 0.53 1.00 0.82\n#&gt;  [9,] 0.41 0.79 0.97 0.44 0.82 0.99 0.44 0.82 1.00\n\n\nCompared with simulation results based on 10,000 replications.\n\n\n\n\n\n\n\n\n\n\n8.8.2 Example 2\n\ncorr_test &lt;- corr_test2\ncorr_time &lt;- corr_time2\ninfo &lt;- info2\n# Overall Correlation\ncorr_combo &lt;- diag(1, nrow = nrow(info))\nfor (i in 1:nrow(info)) {\n  for (j in 1:nrow(info)) {\n    t1 &lt;- as.numeric(info$Analysis[i])\n    t2 &lt;- as.numeric(info$Analysis[j])\n    if (t1 &lt;= t2) {\n      test1 &lt;- as.numeric(info$test[i])\n      test2 &lt;- as.numeric(info$test[j])\n      corr_combo[i, j] &lt;- corr_test[[t1]][test1, test2] * corr_time[[test2]][t1, t2]\n      corr_combo[j, i] &lt;- corr_combo[i, j]\n    }\n  }\n}\ncorr_combo2 &lt;- corr_combo\ncorr_combo2 %&gt;% round(2)\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6]\n#&gt; [1,] 1.00 0.66 0.57 0.93 0.45 0.34\n#&gt; [2,] 0.66 1.00 0.87 0.61 0.94 0.71\n#&gt; [3,] 0.57 0.87 1.00 0.53 0.81 0.94\n#&gt; [4,] 0.93 0.61 0.53 1.00 0.48 0.36\n#&gt; [5,] 0.45 0.94 0.81 0.48 1.00 0.75\n#&gt; [6,] 0.34 0.71 0.94 0.36 0.75 1.00\n\n\nCompared with simulation results based on 10,000 replications.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#power",
    "href": "maxcombo.html#power",
    "title": "8  MaxCombo test",
    "section": "\n8.9 Power",
    "text": "8.9 Power\n\nFirst interim analysis\n\n\\[\n\\text{Pr}( G_1 &gt; b_1 \\mid H_1) = 1 - \\text{Pr}(G_1 &lt; b_1 \\mid H_1)\n\\]\n\nSecond interim analysis\n\n\\[\n\\text{Pr}( a_1 &lt; G_1 &lt; b_1, G_2 &gt; b_2  \\mid H_1) = \\text{Pr}(G_1 &lt; a_1, G_2 &lt; b_2 \\mid H_1)\n\\]\n\\[\n- \\text{Pr}(G_1 &lt; b_1, G_2 &lt; b_2 \\mid H_1) - \\text{Pr}(G_1 &lt; a_1, G_2 &lt; \\infty \\mid H_1)\n\\]\n\\[\n+ \\text{Pr}(G_1 &lt; b_1, G_2 &lt; \\infty \\mid H_1)\n\\]\n\nGeneral interim analysis\nDenote \\(l = (a_1, a_{k-1}, b_k)\\) and \\(u = (b_1, b_{k-1}, \\infty)\\)\n\\(\\xi = \\{\\xi_j; \\; j=1,\\dots, 2^k\\}\\): is all \\(2^k\\) possible combination of the elements in \\(l\\) and \\(u\\)\n\n\\[\n\\text{Pr}( \\cap_{i=1}^{k-1} a_i &lt; G_i &lt; b_i, G_k &gt; b_k \\mid H_1)  = \\sum_{j=1}^{2^k} (-1)^{\\sum_{i=1}^{k} I(\\xi_i = l_i)} \\text{Pr}(\\cap_{i=1}^k G_k &lt; \\xi_i)\n\\]\n\nThe computation can be simplified if \\(a_i = - \\infty\\) up to \\(k\\)-th interim analysis.\nThe computation can be simplified if \\(G_i\\) contains only one test up to \\(k\\)-th interim analysis\n\n\n8.9.1 Example 1\n\nn &lt;- 500\n# Restricted to actual analysis\ninfo_fh &lt;- merge(info1, fh_test1, all = TRUE)\ncorr_fh &lt;- corr_combo1[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\ninfo_fh &lt;- subset(info_fh, !is.na(gamma))\ntheta_fh &lt;- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\npower &lt;- gsdmvn:::gs_prob_combo(\n  upper_bound = x$upper$bound,\n  lower_bound = x$lower$bound,\n  fh_test = fh_test1,\n  analysis = info_fh$Analysis,\n  theta = theta_fh * sqrt(n),\n  corr = corr_fh\n)\npower\n#&gt;   Bound Probability\n#&gt; 1 Upper 0.002411474\n#&gt; 2 Upper 0.525796329\n#&gt; 3 Upper 0.828237848\n#&gt; 4 Lower 0.129688918\n#&gt; 5 Lower 0.162696908\n#&gt; 6 Lower 0.171763506\n\n\n8.9.2 Example 2\n\nn &lt;- 500\n# Restricted to actual analysis\ninfo_fh &lt;- merge(info2, fh_test2, all = TRUE)\ncorr_fh &lt;- corr_combo2[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\ninfo_fh &lt;- subset(info_fh, !is.na(gamma))\ntheta_fh &lt;- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\npower &lt;- gsdmvn:::gs_prob_combo(\n  upper_bound = x$upper$bound,\n  lower_bound = x$lower$bound,\n  fh_test = fh_test2,\n  analysis = info_fh$Analysis,\n  theta = theta_fh * sqrt(n),\n  corr = corr_fh\n)\npower\n#&gt;   Bound Probability\n#&gt; 1 Upper 0.006333066\n#&gt; 2 Upper 0.674439644\n#&gt; 3 Upper 0.896146115\n#&gt; 4 Lower 0.068860093\n#&gt; 5 Lower 0.089484513\n#&gt; 6 Lower 0.103860133",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo.html#sample-size",
    "href": "maxcombo.html#sample-size",
    "title": "8  MaxCombo test",
    "section": "\n8.10 Sample size",
    "text": "8.10 Sample size\n\nRoot finding based on target power\n\n\n8.10.1 Example 1\n\nfun &lt;- function(n) {\n  info_fh &lt;- merge(info1, fh_test1, all = TRUE)\n  corr_fh &lt;- corr_combo1[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\n  info_fh &lt;- subset(info_fh, !is.na(gamma))\n  theta_fh &lt;- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\n  power &lt;- gsdmvn:::gs_prob_combo(\n    upper_bound = x$upper$bound,\n    lower_bound = x$lower$bound,\n    fh_test = fh_test1,\n    analysis = info_fh$Analysis,\n    theta = theta_fh * sqrt(n),\n    corr = corr_fh\n  )\n  1 - beta - max(subset(power, Bound == \"Upper\")$Probability)\n}\n\nuniroot(fun, c(1, 1000), extendInt = \"yes\")$root\n#&gt; [1] 444.8102\n\n\n8.10.2 Example 2\n\nfun &lt;- function(n) {\n  info_fh &lt;- merge(info2, fh_test2, all = TRUE)\n  corr_fh &lt;- corr_combo2[!is.na(info_fh$gamma), !is.na(info_fh$gamma)]\n  info_fh &lt;- subset(info_fh, !is.na(gamma))\n  theta_fh &lt;- abs(info_fh$delta) / sqrt(info_fh$sigma2)\n\n  power &lt;- gsdmvn:::gs_prob_combo(\n    upper_bound = x$upper$bound,\n    lower_bound = x$lower$bound,\n    fh_test = fh_test2,\n    analysis = info_fh$Analysis,\n    theta = theta_fh * sqrt(n),\n    corr = corr_fh\n  )\n\n  1 - beta - max(subset(power, Bound == \"Upper\")$Probability)\n}\n\nuniroot(fun, c(1, 1000), extendInt = \"yes\")$root\n#&gt; [1] 348.2162\n\n\n\n\n\nWang, Lili, Xiaodong Luo, and Cheng Zheng. 2019. “A Simulation-Free Group Sequential Design with Max-Combo Tests in the Presence of Non-Proportional Hazards.” arXiv Preprint arXiv:1911.05684.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html",
    "href": "maxcombo-boundary.html",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "",
    "text": "9.1 Sample size calculation based on spending function\ngs_design_combo(\n  enrollRates,\n  failRates,\n  fh_test,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE, # test.type = 4 non-binding futility bound\n  upper = gs_spending_combo,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025), # alpha spending\n  lower = gs_spending_combo,\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2), # beta spending\n) %&gt;%\n  mutate(`Probability_Null (%)` = Probability_Null * 100) %&gt;%\n  select(-Probability_Null) %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\n#&gt;   Analysis Bound Time      N Events     Z Probability\n#&gt; 1        1 Upper   12 301.31  64.72  6.18        0.00\n#&gt; 3        2 Upper   24 301.31 148.42  2.80        0.22\n#&gt; 5        3 Upper   36 301.31 199.65  2.10        0.80\n#&gt; 2        1 Lower   12 301.31  64.72 -2.72        0.00\n#&gt; 4        2 Lower   24 301.31 148.42  0.65        0.08\n#&gt; 6        3 Lower   36 301.31 199.65  2.10        0.20\n#&gt;   Probability_Null (%)\n#&gt; 1                 0.00\n#&gt; 3                 0.26\n#&gt; 5                 2.50\n#&gt; 2                   NA\n#&gt; 4                   NA\n#&gt; 6                   NA\n#&gt;     n      d analysisTimes lower upper\n#&gt; 7 302  64.78            12  0.00  0.00\n#&gt; 8 302 148.75            24  0.08  0.22\n#&gt; 9 302 200.18            36  0.20  0.80\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  377.    81   3.79        0     0.84  0.17\n#&gt; 2        2 Upper    24  377.   186.  2.36        0.46  0.72  0.33\n#&gt; 3        3 Upper    36  377.   250.  2.01        0.8   0.68  0.38\n#&gt; 4        1 Lower    12  377.    81  -1.18        0.03  0.84  0.17\n#&gt; 5        2 Lower    24  377.   186.  1.15        0.14  0.72  0.33\n#&gt; 6        3 Lower    36  377.   250.  2.01        0.2   0.68  0.38\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0.5, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  304.   65.3  5.05        0     0.79  0.68\n#&gt; 2        2 Upper    24  304.  150.   2.51        0.42  0.68  0.93\n#&gt; 3        3 Upper    36  304.  201.   1.99        0.8   0.65  0.97\n#&gt; 4        1 Lower    12  304.   65.3 -1.8         0     0.79  0.68\n#&gt; 5        2 Lower    24  304.  150.   1.12        0.12  0.68  0.93\n#&gt; 6        3 Lower    36  304.  201.   1.99        0.2   0.65  0.97\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;\nx &lt;- gsdmvn::gs_design_wlr(\n  enrollRates = enrollRates, failRates = failRates,\n  ratio = ratio, alpha = alpha, beta = beta,\n  weight = function(x, arm0, arm1) {\n    gsdmvn::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)\n  },\n  upper = gs_spending_bound,\n  lower = gs_spending_bound,\n  upar = list(sf = gsDesign::sfLDOF, total_spend = alpha),\n  lpar = list(sf = gsDesign::sfLDOF, total_spend = beta),\n  analysisTimes = analysisTimes\n)$bounds %&gt;%\n  mutate_if(is.numeric, round, digits = 2)\nx\n#&gt; # A tibble: 6 × 11\n#&gt;   Analysis Bound  Time     N Events     Z Probability   AHR theta\n#&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1        1 Upper    12  288.   61.9  6.18        0     0.78  0.63\n#&gt; 2        2 Upper    24  288.  142    2.8         0.3   0.67  0.77\n#&gt; 3        3 Upper    36  288.  191.   1.97        0.8   0.64  0.73\n#&gt; 4        1 Lower    12  288.   61.9 -2.43        0     0.78  0.63\n#&gt; 5        2 Lower    24  288.  142    0.93        0.09  0.67  0.77\n#&gt; 6        3 Lower    36  288.  191.   1.97        0.2   0.64  0.73\n#&gt; # ℹ 2 more variables: info &lt;dbl&gt;, info0 &lt;dbl&gt;",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#sample-size-calculation-based-on-spending-function",
    "href": "maxcombo-boundary.html#sample-size-calculation-based-on-spending-function",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "",
    "text": "Implementation in gsdmvn\n\n\n\nSimulation results based on 10,000 replications.\n\n\n\nCompared with \\(FH(0, 0)\\) using boundary based on test.type = 4\n\n\n\n\nCompared with \\(FH(0.5, 0.5)\\) using boundary based on test.type = 4\n\n\n\n\nCompared with \\(FH(0, 0.5)\\) using boundary based on test.type = 4",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#information-fraction",
    "href": "maxcombo-boundary.html#information-fraction",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "\n9.2 Information fraction",
    "text": "9.2 Information fraction\nUnder the same design assumption, information fraction is different from different weight parameters in WLR.\nThere are two potential strategies to calculate the information fraction:\n\nOption 1: Use minimal information fraction of all candidate tests (implemented in gsdmvn).\nOption 2: Use the weighted average of information fraction.\n\n\n\n\n\n\n\n\n\n\nutility &lt;- gsdmvn:::gs_utility_combo(enrollRates, failRates, fh_test = fh_test, ratio = 1)\nutility$info_all %&gt;% mutate_if(is.numeric, round, digits = 3)\n#&gt;   test Analysis Time   N  Events   AHR  delta sigma2 theta\n#&gt; 1    1        1   12 500 107.394 0.842 -0.009  0.054 0.172\n#&gt; 2    1        2   24 500 246.283 0.716 -0.041  0.123 0.333\n#&gt; 3    1        3   36 500 331.291 0.683 -0.062  0.164 0.381\n#&gt; 4    2        1   12 500 107.394 0.781 -0.005  0.007 0.626\n#&gt; 5    2        2   24 500 246.283 0.666 -0.024  0.031 0.765\n#&gt; 6    2        3   36 500 331.291 0.639 -0.040  0.054 0.732\n#&gt; 7    3        1   12 500 107.394 0.790 -0.004  0.006 0.676\n#&gt; 8    3        2   24 500 246.283 0.675 -0.019  0.020 0.927\n#&gt; 9    3        3   36 500 331.291 0.650 -0.029  0.030 0.973\n#&gt;     info  info0\n#&gt; 1 26.841 26.899\n#&gt; 2 61.352 62.087\n#&gt; 3 81.918 83.944\n#&gt; 4  3.605  3.624\n#&gt; 5 15.371 15.737\n#&gt; 6 27.205 28.484\n#&gt; 7  2.898  2.910\n#&gt; 8 10.155 10.335\n#&gt; 9 15.070 15.526\n\n\ninfo_frac &lt;- tapply(utility$info_all$info0, utility$info_all$test, function(x) x / max(x))\ninfo_frac\n#&gt; $`1`\n#&gt; [1] 0.3204395 0.7396239 1.0000000\n#&gt; \n#&gt; $`2`\n#&gt; [1] 0.1272245 0.5525079 1.0000000\n#&gt; \n#&gt; $`3`\n#&gt; [1] 0.1874599 0.6656420 1.0000000\n\n\nmin_info_frac &lt;- apply(do.call(rbind, info_frac), 2, min)\nmin_info_frac\n#&gt; [1] 0.1272245 0.5525079 1.0000000",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#spending-function",
    "href": "maxcombo-boundary.html#spending-function",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "\n9.3 Spending function",
    "text": "9.3 Spending function\n\n\ngsDesign bound can not be directly used for MaxCombo test:\n\nMultiple test statistics are considered in interim analysis or final analysis.\n\n\n\nDesign example:\n\n\\(\\alpha = 0.025\\)\n\\(\\beta = 0.2\\)\n\n\\(K=3\\) total analysis.\n\ntest.type=4: A two-sided, asymmetric, beta-spending with non-binding lower bound.\n\n\nLan-DeMets spending function to approximate an O’Brien-Fleming bound Gordon Lan and DeMets (1983). (gsDesign::sfLDOF)\n\\(t\\) is information fraction in the formula below.\n\n\\[\nf(t; \\alpha)=2-2\\Phi\\left(\\Phi^{-1}\\left(\\frac{1-\\alpha/2}{t^{\\rho/2}}\\right)\\right)\n\\]\n\nOther spending functions are discussed in the gsDesign technical manual and implemented in gsDesign::sf*() functions.\n\nUpper bound:\n\nNon-binding lower bound: lower bound are all -Inf.\n\n\n\n\nalpha_spend &lt;- gsDesign::sfLDOF(alpha = 0.025, t = min_info_frac)$spend\nalpha_spend\n#&gt; [1] 3.300225e-10 2.566068e-03 2.500000e-02\n\n\nLower bound:\n\n\nbeta_spend &lt;- gsDesign::sfLDOF(alpha = 0.2, t = min_info_frac)$spend\nbeta_spend\n#&gt; [1] 0.0003269604 0.0846866352 0.2000000000",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#technical-details",
    "href": "maxcombo-boundary.html#technical-details",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "\n9.4 Technical details",
    "text": "9.4 Technical details\nThis section describes the details of calculating the sample size and events required for WLR under group sequential design. It can be skipped for the first read of this training material.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#upper-bound-in-group-sequential-design",
    "href": "maxcombo-boundary.html#upper-bound-in-group-sequential-design",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "\n9.5 Upper bound in group sequential design",
    "text": "9.5 Upper bound in group sequential design\n\n9.5.1 One test in each interim analysis\n\nFirst interim analysis\n\n\\[\n\\alpha_1 = \\text{Pr}(Z_1 &gt; b_1 \\mid H_0)\n\\]\n\nqnorm(1 - alpha_spend[1])\n#&gt; [1] 6.17538\n\n\nGeneral formula (non-binding futility bound)\n\n\\[\n\\alpha_k = \\text{Pr}(\\cap_{i=1}^{i=k-1} Z_i &lt; b_i, Z_k &gt; b_k \\mid H_0)\n\\]\n\n9.5.2 MaxCombo upper bound\n\nFirst interim analysis upper bound\n\n\\[\n\\alpha_1 = \\text{Pr}(G_1 &gt; b_1 \\mid H_0) = 1 - \\text{Pr}(\\cap_i Z_{i1} &lt; b_1 \\mid H_0)\n\\]\n\nGeneral formula (non-binding futility bound)\n\n\\[\n\\alpha_k = \\text{Pr}(\\cap_{i=1}^{i=k-1} G_i &lt; b_i, G_k &gt; b_k \\mid H_0)\n\\]\n\\[\n= \\text{Pr}(\\cap_{i=1}^{i=k-1} G_i &lt; b_i \\mid H_0) - \\text{Pr}(\\cap_{i=1}^{i=k} G_i &lt; b_i \\mid H_0)\n\\]\n\ngsdmvn implementation for MaxCombo test\n\n\ngsdmvn:::gs_bound(alpha_spend, beta_spend,\n  analysis = utility$info$Analysis, # Analysis indicator\n  theta = rep(0, nrow(fh_test)), # Under the null hypothesis\n  corr = utility$corr # Correlation\n)$upper\n#&gt; [1] 6.175397 2.798651 2.097516\n\n\nCompared with upper bound calculated from gsDesign\n\n\nx &lt;- gsDesign::gsSurv(\n  k = 3, test.type = 4, alpha = 0.025,\n  beta = 0.2, astar = 0, timing = c(1),\n  sfu = sfLDOF, sfupar = c(0), sfl = sfLDOF,\n  sflpar = c(0), lambdaC = c(0.1),\n  hr = 0.6, hr0 = 1, eta = 0.01,\n  gamma = c(10),\n  R = c(12), S = NULL,\n  T = 36, minfup = 24, ratio = 1\n)\n\n\nx$upper$bound\n#&gt; [1] 3.710303 2.511407 1.992970",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#lower-bound-in-group-sequential-design",
    "href": "maxcombo-boundary.html#lower-bound-in-group-sequential-design",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "\n9.6 Lower bound in group sequential design",
    "text": "9.6 Lower bound in group sequential design\n\n9.6.1 One test in each interim analysis\n\nFirst interim analysis\n\n\\[\n\\beta_1 = \\text{Pr}(Z_1 &lt; a_1 \\mid H_1)\n\\]\n\nn &lt;- 400\nqnorm(beta_spend[1], mean = utility$theta[1] * sqrt(n))\n#&gt; [1] -2.610665\n\n\nn &lt;- 500\nqnorm(beta_spend[1], mean = utility$theta[1] * sqrt(n))\n#&gt; [1] -2.516528\n\n\nGeneral formula\n\n\\[\n\\beta_k = \\text{Pr}(\\cap_{i=1}^{i=k-1}  a_i &lt; Z_i &lt; b_i, Z_k &lt; a_k \\mid H_1)\n\\]\n\n9.6.2 MaxCombo lower bound\n\nFirst interim analysis upper bound\n\n\\[\n\\beta_1 = \\text{Pr}(G_1 &lt; a_1 \\mid H_1) = \\text{Pr}(\\cap_i Z_{i1} &lt; a_1 \\mid H_0)\n\\]\n\nGeneral formula (non-binding futility bound)\n\n\\[\n\\beta_k = \\text{Pr}(\\cap_{i=1}^{i=k-1} a_i &lt;G_i &lt; b_i, G_k &lt; a_k \\mid H_0)\n\\]\n\ngsdmvn implementation for MaxCombo test\n\n\nn &lt;- 400\nutility &lt;- gsdmvn:::gs_utility_combo(\n  enrollRates, failRates, fh_test = fh_test, ratio = 1\n)\nbound &lt;- gsdmvn:::gs_bound(\n  alpha_spend, beta_spend,\n  analysis = utility$info$Analysis, # Analysis indicator\n  theta = utility$theta * sqrt(n), # Under the alternative hypothesis\n  corr = utility$corr # Correlation\n)\n\nbound$lower\n#&gt; [1] -2.6106678  0.9619422  2.0972654\n\n\nCompared with lower bound calculated from gsDesign\n\n\nx$lower$bound\n#&gt; [1] -0.2361874  1.1703638  1.9929702",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "maxcombo-boundary.html#sample-size-calculation",
    "href": "maxcombo-boundary.html#sample-size-calculation",
    "title": "9  Group sequential design boundary with MaxCombo test",
    "section": "\n9.7 Sample size calculation",
    "text": "9.7 Sample size calculation\n\nSample size and boundaries are set simultaneously using an iterative algorithm.\n\n\n9.7.1 Initiate the calculation from lower bound derived at \\(N = 400\\)\n\n\nbound\n#&gt;      upper      lower\n#&gt; 1 6.175397 -2.6106678\n#&gt; 2 2.798651  0.9619422\n#&gt; 3 2.097265  2.0972654\n\n\ngs_design_combo(\n  enrollRates,\n  failRates,\n  fh_test,\n  alpha = 0.025,\n  beta = 0.2,\n  ratio = 1,\n  binding = FALSE,\n  upar = bound$upper,\n  lpar = bound$lower,\n) %&gt;% mutate_if(is.numeric, round, digits = 2)\n#&gt;   Analysis Bound Time      N Events     Z Probability\n#&gt; 1        1 Upper   12 324.95  69.80  6.18        0.00\n#&gt; 3        2 Upper   24 324.95 160.06  2.80        0.24\n#&gt; 5        3 Upper   36 324.95 215.30  2.10        0.80\n#&gt; 2        1 Lower   12 324.95  69.80 -2.61        0.00\n#&gt; 4        2 Lower   24 324.95 160.06  0.96        0.13\n#&gt; 6        3 Lower   36 324.95 215.30  2.10        0.20\n#&gt;   Probability_Null\n#&gt; 1             0.00\n#&gt; 3             0.00\n#&gt; 5             0.03\n#&gt; 2               NA\n#&gt; 4               NA\n#&gt; 6               NA\n\n\n9.7.2 Update bound based on newly calculated sample size\n\nn &lt;- 355\nutility &lt;- gsdmvn:::gs_utility_combo(enrollRates, failRates, fh_test = fh_test, ratio = 1)\nbound &lt;- gsdmvn:::gs_bound(alpha_spend, beta_spend,\n  analysis = utility$info$Analysis, # Analysis indicator\n  theta = utility$theta * sqrt(n), # Under the alternative hypothesis\n  corr = utility$corr # Correlation\n)\n\nbound\n#&gt;      upper      lower\n#&gt; 1 6.175397 -2.6568642\n#&gt; 2 2.798651  0.8266125\n#&gt; 3 2.097386  2.0973860\n\n\nRepeat the procedure above until the sample size and lower bound converge.\n\n\n\n\n\nGordon Lan, KK, and David L DeMets. 1983. “Discrete Sequential Boundaries for Clinical Trials.” Biometrika 70 (3): 659–63.",
    "crumbs": [
      "Others tests",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Group sequential design boundary with MaxCombo test</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "De Castro, Mario, Vicente G Cancho, and Josemar Rodrigues. 2010.\n“A Hands-on Approach for Fitting Long-Term Survival Models Under\nthe GAMLSS Framework.” Computer Methods and Programs in\nBiomedicine 97 (2): 168–77.\n\n\nDowns, John R, Polly A Beere, Edwin Whitney, Michael Clearfield, Stephen\nWeis, Jeffrey Rochen, Evan A Stein, Deborah R Shapiro, Alexandra\nLangendorfer, and Antonio M Gotto Jr. 1997. “Design &\nRationale of the Air Force/Texas Coronary Atherosclerosis Prevention\nStudy (AFCAPS/TexCAPS).” The American Journal of\nCardiology 80 (3): 287–93.\n\n\nDowns, John R, Michael Clearfield, Stephen Weis, Edwin Whitney, Deborah\nR Shapiro, Polly A Beere, Alexandra Langendorfer, et al. 1998.\n“Primary Prevention of Acute Coronary Events with Lovastatin in\nMen and Women with Average Cholesterol Levels: Results of\nAFCAPS/TexCAPS.” Journal of the American Medical\nAssociation 279 (20): 1615–22.\n\n\nGandhi, Leena, Delvys Rodrı́guez-Abreu, Shirish Gadgeel, Emilio Esteban,\nEnriqueta Felip, Flávia De Angelis, Manuel Domine, et al. 2018.\n“Pembrolizumab Plus Chemotherapy in Metastatic Non–Small-Cell Lung\nCancer.” New England Journal of Medicine 378 (22):\n2078–92.\n\n\nGordon Lan, KK, and David L DeMets. 1983. “Discrete Sequential\nBoundaries for Clinical Trials.” Biometrika 70 (3):\n659–63.\n\n\nHarrington, David P, and Thomas R Fleming. 1982. “A Class of Rank\nTest Procedures for Censored Survival Data.” Biometrika\n69 (3): 553–66.\n\n\nHaybittle, JL. 1971. “Repeated Assessment of Results in Clinical\nTrials of Cancer Treatment.” The British Journal of\nRadiology 44 (526): 793–97.\n\n\nHellmann, Matthew David, Junshui Ma, Edward B Garon, Rina Hui, Leena\nGandhi, Jean-Charles Soria, Keaven M Anderson, Gregory M Lubiniecki,\nBilal Piperdi, and Roy S Herbst. 2017. “Estimating Long-Term\nSurvival of PD-L1-Expressing, Previously Treated, Non-Small Cell Lung\nCancer Patients Who Received Pembrolizumab in KEYNOTE-001\nand-010.” American Society of Clinical Oncology.\n\n\nHwang, Irving K, Weichung J Shih, and John S De Cani. 1990. “Group\nSequential Designs Using a Family of Type I Error\nProbability Spending Functions.” Statistics in Medicine\n9 (12): 1439–45.\n\n\nJennison, Christopher, and Bruce W. Turnbull. 2000. Group Sequential\nMethods with Applications to Clinical Trials. Boca Raton, FL:\nChapman; Hall/CRC.\n\n\nKarrison, Theodore G. 2016. “Versatile Tests for Comparing\nSurvival Curves Based on Weighted Log-Rank Statistics.” The\nStata Journal 16 (3): 678–90.\n\n\nKim, Kyungmann, and Anastasios A. Tsiatis. 1990. “Study Duration\nfor Clinical Trials with Survival Response and Early Stopping\nRule.” Biometrics 46: 81–92.\n\n\nLachin, John M. 2005. “A Review of Methods for Futility Stopping\nBased on Conditional Power.” Statistics in Medicine 24\n(18): 2747–64.\n\n\nLachin, John M., and Mary A. Foulkes. 1986a. “Evaluation of Sample\nSize and Power for Analyses of Survival with Allowance for Nonuniform\nPatient Entry, Losses to Follow-up, Noncompliance, and\nStratification.” Biometrics 42: 507–19.\n\n\nLachin, John M, and Mary A Foulkes. 1986b. “Evaluation of Sample\nSize and Power for Analyses of Survival with Allowance for Nonuniform\nPatient Entry, Losses to Follow-up, Noncompliance, and\nStratification.” Biometrics, 507–19.\n\n\nLan, K. K. G., and David L. DeMets. 1983. “Discrete Sequential\nBoundaries for Clinical Trials.” Biometrika 70: 659–63.\n\n\n———. 1989. “Group Sequential Procedures: Calendar Versus\nInformation Time.” Statistics in Medicine 8: 1191–98.\n\n\nLin, Ray S, Ji Lin, Satrajit Roychoudhury, Keaven M Anderson, Tianle Hu,\nBo Huang, Larry F Leon, et al. 2020. “Alternative Analysis Methods\nfor Time to Event Endpoints Under Nonproportional Hazards: A Comparative\nAnalysis.” Statistics in Biopharmaceutical Research 12\n(2): 187–98.\n\n\nMagirr, Dominic, and Carl-Fredrik Burman. 2019b. “Modestly\nWeighted Logrank Tests.” Statistics in Medicine 38 (20):\n3782–90.\n\n\n———. 2019a. “Modestly Weighted Logrank Tests.”\nStatistics in Medicine 38 (20): 3782–90.\n\n\nMaurer, Willi, and Frank Bretz. 2013. “Multiple Testing in Group\nSequential Trials Using Graphical Approaches.” Statistics in\nBiopharmaceutical Research 5: 311–20.\n\n\nMiettinen, Tatu A, Kalevi Pyörälä, Anders G Olsson, Thomas A Musliner,\nThomas J Cook, Ole Faergeman, Kåre Berg, Terje Pedersen, John Kjekshus,\nand for the Scandinavian Simvastatin Study Group. 1997.\n“Cholesterol-Lowering Therapy in Women and Elderly Patients with\nMyocardial Infarction or Angina Pectoris: Findings from the Scandinavian\nSimvastatin Survival Study (4S).” Circulation 96 (12):\n4211–18.\n\n\nO’Brien, Peter C, and Thomas R Fleming. 1979. “A Multiple Testing\nProcedure for Clinical Trials.” Biometrics, 549–56.\n\n\nPocock, Stuart J. 1977. “Group Sequential Methods in the Design\nand Analysis of Clinical Trials.” Biometrika 64 (2):\n191–99.\n\n\nRoychoudhury, Satrajit, Keaven M Anderson, Jiabu Ye, and Pralay\nMukhopadhyay. 2021. “Robust Design and Analysis of Clinical Trials\nwith Non-Proportional Hazards: A Straw Man Guidance from a Cross-Pharma\nWorking Group.” Statistics in Biopharmaceutical\nResearch, 1–37.\n\n\nScharfstein, Daniel O, Anastasios A Tsiatis, and James M Robins. 1997.\n“Semiparametric Efficiency and Its Implication on the Design and\nAnalysis of Group-Sequential Studies.” Journal of the\nAmerican Statistical Association 92 (440): 1342–50.\n\n\nSchoenfeld, David. 1981. “The Asymptotic Properties of\nNonparametric Tests for Comparing Survival Distributions.”\nBiometrika 68 (1): 316–19.\n\n\nWang, Lili, Xiaodong Luo, and Cheng Zheng. 2019. “A\nSimulation-Free Group Sequential Design with Max-Combo Tests in the\nPresence of Non-Proportional Hazards.” arXiv Preprint\narXiv:1911.05684.\n\n\nWang, Samuel K, and Anastasios A Tsiatis. 1987. “Approximately\nOptimal One-Parameter Boundaries for Group Sequential Trials.”\nBiometrics, 193–99.\n\n\nWhite, William B, George L Bakris, Richard M Bergenstal, Christopher P\nCannon, William C Cushman, Penny Fleck, Simon Heller, et al. 2011.\n“EXamination of cArdiovascular outcoMes with alogliptIN Versus\nStandard of carE in Patients with Type 2 Diabetes Mellitus and Acute\nCoronary Syndrome (EXAMINE): A Cardiovascular Safety Study of the\nDipeptidyl Peptidase 4 Inhibitor Alogliptin in Patients with Type 2\nDiabetes with Acute Coronary Syndrome.” American Heart\nJournal 162 (4): 620–26.\n\n\nWhite, William B, Christopher P Cannon, Simon R Heller, Steven E Nissen,\nRichard M Bergenstal, George L Bakris, Alfonso T Perez, et al. 2013.\n“Alogliptin After Acute Coronary Syndrome in Patients with Type 2\nDiabetes.” New England Journal of Medicine 369: 1327–35.\n\n\nYung, Godwin, and Yi Liu. 2019. “Sample Size and Power for the\nWeighted Log-Rank Test and Kaplan-Meier Based Tests with Allowance for\nNonproportional Hazards.” Biometrics.\n\n\nZeng, Donglin, Guosheng Yin, and Joseph G Ibrahim. 2006.\n“Semiparametric Transformation Models for Survival Data with a\nCure Fraction.” Journal of the American Statistical\nAssociation 101 (474): 670–84.\n\n\nZhang, Yilong, and Yongzhao Shao. 2018. “Concordance Measure and\nDiscriminatory Accuracy in Transformation Cure Models.”\nBiostatistics 19 (1): 14–26.",
    "crumbs": [
      "References"
    ]
  }
]