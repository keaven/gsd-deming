# Generated by pkglite: do not edit by hand
# Use pkglite::unpack() to restore the packages

Package: gsdmvn
File: .Rbuildignore
Format: text
Content:
  ^renv$
  ^renv\.lock$
  ^.*\.Rproj$
  ^\.Rproj\.user$
  ^_pkgdown\.yml$
  ^pkgdown$
  ^docs$
  ^pkgdown$
  ^.*\.groovy$
  ^jenkins$
  ^dockerfiles$
  ^pipelines-definition.groovy$
  ^README\.Rmd$
  ^\.gitignore$
  ^\.DS_Store$
  ^\.git\$
  ^LICENSE$
  ^LICENSES_THIRD_PARTY$
  ^Meta$
  ^jenkins$
  ^jobs-configuration.groovy$
  ^codecov\.yml$
  ^tests/testthat/_snaps$
  ^library$
  ^data-raw$
  ^\.github$
  ^simulation$

Package: gsdmvn
File: DESCRIPTION
Format: text
Content:
  Package: gsdmvn
  Title: Group sequential design with non-constant effect
  Version: 0.1.1.9000
  Authors@R: c(
      person("Keaven", "Anderson", email = "keaven_anderson@merck.com", role = c("aut", "cre")),
      person("Merck Sharp & Dohme Corp", role = "cph")
      )
  Description: Basic group sequential design computations extended.
  License: GPL-3
  URL: https://merck.github.io/gsdmvn/, https://github.com/Merck/gsdmvn
  BugReports: https://github.com/Merck/gsdmvn/issues
  Encoding: UTF-8
  VignetteBuilder: knitr
  Imports:
      gsDesign,
      gsDesign2,
      dplyr,
      tibble,
      npsurvSS,
      corpcor,
      utils,
      mvtnorm
  Suggests:
      testthat,
      knitr,
      rmarkdown,
      purrr,
      devtools,
      simtrial,
      kableExtra,
      gt,
      ggplot2,
      tidyr
  RoxygenNote: 7.1.2

Package: gsdmvn
File: LICENSE
Format: text
Content:
                    GNU GENERAL PUBLIC LICENSE
                     Version 3, 29 June 2007
  
  Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
    Everyone is permitted to copy and distribute verbatim copies
  of this license document, but changing it is not allowed.
  
  Preamble
  
  The GNU General Public License is a free, copyleft license for
  software and other kinds of works.
  
  The licenses for most software and other practical works are designed
  to take away your freedom to share and change the works.  By contrast,
  the GNU General Public License is intended to guarantee your freedom to
  share and change all versions of a program--to make sure it remains free
  software for all its users.  We, the Free Software Foundation, use the
  GNU General Public License for most of our software; it applies also to
  any other work released this way by its authors.  You can apply it to
  your programs, too.
  
  When we speak of free software, we are referring to freedom, not
  price.  Our General Public Licenses are designed to make sure that you
  have the freedom to distribute copies of free software (and charge for
                                                          them if you wish), that you receive source code or can get it if you
  want it, that you can change the software or use pieces of it in new
  free programs, and that you know you can do these things.
  
  To protect your rights, we need to prevent others from denying you
  these rights or asking you to surrender the rights.  Therefore, you have
  certain responsibilities if you distribute copies of the software, or if
  you modify it: responsibilities to respect the freedom of others.
  
  For example, if you distribute copies of such a program, whether
  gratis or for a fee, you must pass on to the recipients the same
  freedoms that you received.  You must make sure that they, too, receive
  or can get the source code.  And you must show them these terms so they
  know their rights.
  
  Developers that use the GNU GPL protect your rights with two steps:
    (1) assert copyright on the software, and (2) offer you this License
  giving you legal permission to copy, distribute and/or modify it.
  
  For the developers' and authors' protection, the GPL clearly explains
  that there is no warranty for this free software.  For both users' and
  authors' sake, the GPL requires that modified versions be marked as
  changed, so that their problems will not be attributed erroneously to
  authors of previous versions.
  
  Some devices are designed to deny users access to install or run
  modified versions of the software inside them, although the manufacturer
  can do so.  This is fundamentally incompatible with the aim of
  protecting users' freedom to change the software.  The systematic
  pattern of such abuse occurs in the area of products for individuals to
  use, which is precisely where it is most unacceptable.  Therefore, we
  have designed this version of the GPL to prohibit the practice for those
  products.  If such problems arise substantially in other domains, we
  stand ready to extend this provision to those domains in future versions
  of the GPL, as needed to protect the freedom of users.
  
    Finally, every program is threatened constantly by software patents.
  States should not allow patents to restrict development and use of
  software on general-purpose computers, but in those that do, we wish to
  avoid the special danger that patents applied to a free program could
  make it effectively proprietary.  To prevent this, the GPL assures that
  patents cannot be used to render the program non-free.
  
    The precise terms and conditions for copying, distribution and
  modification follow.
  
                         TERMS AND CONDITIONS
  
    0. Definitions.
  
    "This License" refers to version 3 of the GNU General Public License.
  
    "Copyright" also means copyright-like laws that apply to other kinds of
  works, such as semiconductor masks.
  
    "The Program" refers to any copyrightable work licensed under this
  License.  Each licensee is addressed as "you".  "Licensees" and
  "recipients" may be individuals or organizations.
  
    To "modify" a work means to copy from or adapt all or part of the work
  in a fashion requiring copyright permission, other than the making of an
  exact copy.  The resulting work is called a "modified version" of the
  earlier work or a work "based on" the earlier work.
  
    A "covered work" means either the unmodified Program or a work based
  on the Program.
  
    To "propagate" a work means to do anything with it that, without
  permission, would make you directly or secondarily liable for
  infringement under applicable copyright law, except executing it on a
  computer or modifying a private copy.  Propagation includes copying,
  distribution (with or without modification), making available to the
  public, and in some countries other activities as well.
  
    To "convey" a work means any kind of propagation that enables other
  parties to make or receive copies.  Mere interaction with a user through
  a computer network, with no transfer of a copy, is not conveying.
  
    An interactive user interface displays "Appropriate Legal Notices"
  to the extent that it includes a convenient and prominently visible
  feature that (1) displays an appropriate copyright notice, and (2)
  tells the user that there is no warranty for the work (except to the
  extent that warranties are provided), that licensees may convey the
  work under this License, and how to view a copy of this License.  If
  the interface presents a list of user commands or options, such as a
  menu, a prominent item in the list meets this criterion.
  
    1. Source Code.
  
    The "source code" for a work means the preferred form of the work
  for making modifications to it.  "Object code" means any non-source
  form of a work.
  
    A "Standard Interface" means an interface that either is an official
  standard defined by a recognized standards body, or, in the case of
  interfaces specified for a particular programming language, one that
  is widely used among developers working in that language.
  
    The "System Libraries" of an executable work include anything, other
  than the work as a whole, that (a) is included in the normal form of
  packaging a Major Component, but which is not part of that Major
  Component, and (b) serves only to enable use of the work with that
  Major Component, or to implement a Standard Interface for which an
  implementation is available to the public in source code form.  A
  "Major Component", in this context, means a major essential component
  (kernel, window system, and so on) of the specific operating system
  (if any) on which the executable work runs, or a compiler used to
  produce the work, or an object code interpreter used to run it.
  
    The "Corresponding Source" for a work in object code form means all
  the source code needed to generate, install, and (for an executable
  work) run the object code and to modify the work, including scripts to
  control those activities.  However, it does not include the work's
  System Libraries, or general-purpose tools or generally available free
  programs which are used unmodified in performing those activities but
  which are not part of the work.  For example, Corresponding Source
  includes interface definition files associated with source files for
  the work, and the source code for shared libraries and dynamically
  linked subprograms that the work is specifically designed to require,
  such as by intimate data communication or control flow between those
  subprograms and other parts of the work.
  
  The Corresponding Source need not include anything that users
  can regenerate automatically from other parts of the Corresponding
  Source.
  
  The Corresponding Source for a work in source code form is that
  same work.
  
  2. Basic Permissions.
  
  All rights granted under this License are granted for the term of
  copyright on the Program, and are irrevocable provided the stated
  conditions are met.  This License explicitly affirms your unlimited
  permission to run the unmodified Program.  The output from running a
  covered work is covered by this License only if the output, given its
  content, constitutes a covered work.  This License acknowledges your
  rights of fair use or other equivalent, as provided by copyright law.
  
  You may make, run and propagate covered works that you do not
  convey, without conditions so long as your license otherwise remains
  in force.  You may convey covered works to others for the sole purpose
  of having them make modifications exclusively for you, or provide you
  with facilities for running those works, provided that you comply with
  the terms of this License in conveying all material for which you do
  not control copyright.  Those thus making or running the covered works
  for you must do so exclusively on your behalf, under your direction
  and control, on terms that prohibit them from making any copies of
  your copyrighted material outside their relationship with you.
  
  Conveying under any other circumstances is permitted solely under
  the conditions stated below.  Sublicensing is not allowed; section 10
  makes it unnecessary.
  
  3. Protecting Users' Legal Rights From Anti-Circumvention Law.
  
    No covered work shall be deemed part of an effective technological
  measure under any applicable law fulfilling obligations under article
  11 of the WIPO copyright treaty adopted on 20 December 1996, or
  similar laws prohibiting or restricting circumvention of such
  measures.
  
    When you convey a covered work, you waive any legal power to forbid
  circumvention of technological measures to the extent such circumvention
  is effected by exercising rights under this License with respect to
  the covered work, and you disclaim any intention to limit operation or
  modification of the work as a means of enforcing, against the work's
  users, your or third parties' legal rights to forbid circumvention of
  technological measures.
  
    4. Conveying Verbatim Copies.
  
    You may convey verbatim copies of the Program's source code as you
  receive it, in any medium, provided that you conspicuously and
  appropriately publish on each copy an appropriate copyright notice;
  keep intact all notices stating that this License and any
  non-permissive terms added in accord with section 7 apply to the code;
  keep intact all notices of the absence of any warranty; and give all
  recipients a copy of this License along with the Program.
  
  You may charge any price or no price for each copy that you convey,
  and you may offer support or warranty protection for a fee.
  
  5. Conveying Modified Source Versions.
  
  You may convey a work based on the Program, or the modifications to
  produce it from the Program, in the form of source code under the
  terms of section 4, provided that you also meet all of these conditions:
    
    a) The work must carry prominent notices stating that you modified
  it, and giving a relevant date.
  
  b) The work must carry prominent notices stating that it is
  released under this License and any conditions added under section
  7.  This requirement modifies the requirement in section 4 to
  "keep intact all notices".
  
  c) You must license the entire work, as a whole, under this
  License to anyone who comes into possession of a copy.  This
  License will therefore apply, along with any applicable section 7
  additional terms, to the whole of the work, and all its parts,
  regardless of how they are packaged.  This License gives no
  permission to license the work in any other way, but it does not
  invalidate such permission if you have separately received it.
  
  d) If the work has interactive user interfaces, each must display
  Appropriate Legal Notices; however, if the Program has interactive
  interfaces that do not display Appropriate Legal Notices, your
  work need not make them do so.
  
  A compilation of a covered work with other separate and independent
  works, which are not by their nature extensions of the covered work,
  and which are not combined with it such as to form a larger program,
  in or on a volume of a storage or distribution medium, is called an
  "aggregate" if the compilation and its resulting copyright are not
  used to limit the access or legal rights of the compilation's users
  beyond what the individual works permit.  Inclusion of a covered work
  in an aggregate does not cause this License to apply to the other
  parts of the aggregate.
  
    6. Conveying Non-Source Forms.
  
    You may convey a covered work in object code form under the terms
  of sections 4 and 5, provided that you also convey the
  machine-readable Corresponding Source under the terms of this License,
  in one of these ways:
  
      a) Convey the object code in, or embodied in, a physical product
      (including a physical distribution medium), accompanied by the
      Corresponding Source fixed on a durable physical medium
      customarily used for software interchange.
  
      b) Convey the object code in, or embodied in, a physical product
      (including a physical distribution medium), accompanied by a
      written offer, valid for at least three years and valid for as
      long as you offer spare parts or customer support for that product
      model, to give anyone who possesses the object code either (1) a
      copy of the Corresponding Source for all the software in the
      product that is covered by this License, on a durable physical
      medium customarily used for software interchange, for a price no
      more than your reasonable cost of physically performing this
      conveying of source, or (2) access to copy the
      Corresponding Source from a network server at no charge.
  
      c) Convey individual copies of the object code with a copy of the
      written offer to provide the Corresponding Source.  This
      alternative is allowed only occasionally and noncommercially, and
      only if you received the object code with such an offer, in accord
      with subsection 6b.
  
      d) Convey the object code by offering access from a designated
      place (gratis or for a charge), and offer equivalent access to the
      Corresponding Source in the same way through the same place at no
      further charge.  You need not require recipients to copy the
      Corresponding Source along with the object code.  If the place to
      copy the object code is a network server, the Corresponding Source
      may be on a different server (operated by you or a third party)
      that supports equivalent copying facilities, provided you maintain
      clear directions next to the object code saying where to find the
      Corresponding Source.  Regardless of what server hosts the
      Corresponding Source, you remain obligated to ensure that it is
      available for as long as needed to satisfy these requirements.
  
      e) Convey the object code using peer-to-peer transmission, provided
      you inform other peers where the object code and Corresponding
      Source of the work are being offered to the general public at no
      charge under subsection 6d.
  
    A separable portion of the object code, whose source code is excluded
  from the Corresponding Source as a System Library, need not be
  included in conveying the object code work.
  
    A "User Product" is either (1) a "consumer product", which means any
  tangible personal property which is normally used for personal, family,
  or household purposes, or (2) anything designed or sold for incorporation
  into a dwelling.  In determining whether a product is a consumer product,
  doubtful cases shall be resolved in favor of coverage.  For a particular
  product received by a particular user, "normally used" refers to a
  typical or common use of that class of product, regardless of the status
  of the particular user or of the way in which the particular user
  actually uses, or expects or is expected to use, the product.  A product
  is a consumer product regardless of whether the product has substantial
  commercial, industrial or non-consumer uses, unless such uses represent
  the only significant mode of use of the product.
  
    "Installation Information" for a User Product means any methods,
  procedures, authorization keys, or other information required to install
  and execute modified versions of a covered work in that User Product from
  a modified version of its Corresponding Source.  The information must
  suffice to ensure that the continued functioning of the modified object
  code is in no case prevented or interfered with solely because
  modification has been made.
  
    If you convey an object code work under this section in, or with, or
  specifically for use in, a User Product, and the conveying occurs as
  part of a transaction in which the right of possession and use of the
  User Product is transferred to the recipient in perpetuity or for a
  fixed term (regardless of how the transaction is characterized), the
  Corresponding Source conveyed under this section must be accompanied
  by the Installation Information.  But this requirement does not apply
  if neither you nor any third party retains the ability to install
  modified object code on the User Product (for example, the work has
  been installed in ROM).
  
    The requirement to provide Installation Information does not include a
  requirement to continue to provide support service, warranty, or updates
  for a work that has been modified or installed by the recipient, or for
  the User Product in which it has been modified or installed.  Access to a
  network may be denied when the modification itself materially and
  adversely affects the operation of the network or violates the rules and
  protocols for communication across the network.
  
    Corresponding Source conveyed, and Installation Information provided,
  in accord with this section must be in a format that is publicly
  documented (and with an implementation available to the public in
  source code form), and must require no special password or key for
  unpacking, reading or copying.
  
    7. Additional Terms.
  
    "Additional permissions" are terms that supplement the terms of this
  License by making exceptions from one or more of its conditions.
  Additional permissions that are applicable to the entire Program shall
  be treated as though they were included in this License, to the extent
  that they are valid under applicable law.  If additional permissions
  apply only to part of the Program, that part may be used separately
  under those permissions, but the entire Program remains governed by
  this License without regard to the additional permissions.
  
    When you convey a copy of a covered work, you may at your option
  remove any additional permissions from that copy, or from any part of
  it.  (Additional permissions may be written to require their own
  removal in certain cases when you modify the work.)  You may place
  additional permissions on material, added by you to a covered work,
  for which you have or can give appropriate copyright permission.
  
    Notwithstanding any other provision of this License, for material you
  add to a covered work, you may (if authorized by the copyright holders of
  that material) supplement the terms of this License with terms:
  
      a) Disclaiming warranty or limiting liability differently from the
      terms of sections 15 and 16 of this License; or
  
      b) Requiring preservation of specified reasonable legal notices or
      author attributions in that material or in the Appropriate Legal
      Notices displayed by works containing it; or
  
      c) Prohibiting misrepresentation of the origin of that material, or
      requiring that modified versions of such material be marked in
      reasonable ways as different from the original version; or
  
      d) Limiting the use for publicity purposes of names of licensors or
      authors of the material; or
  
      e) Declining to grant rights under trademark law for use of some
      trade names, trademarks, or service marks; or
  
      f) Requiring indemnification of licensors and authors of that
      material by anyone who conveys the material (or modified versions of
      it) with contractual assumptions of liability to the recipient, for
      any liability that these contractual assumptions directly impose on
      those licensors and authors.
  
    All other non-permissive additional terms are considered "further
  restrictions" within the meaning of section 10.  If the Program as you
  received it, or any part of it, contains a notice stating that it is
  governed by this License along with a term that is a further
  restriction, you may remove that term.  If a license document contains
  a further restriction but permits relicensing or conveying under this
  License, you may add to a covered work material governed by the terms
  of that license document, provided that the further restriction does
  not survive such relicensing or conveying.
  
    If you add terms to a covered work in accord with this section, you
  must place, in the relevant source files, a statement of the
  additional terms that apply to those files, or a notice indicating
  where to find the applicable terms.
  
    Additional terms, permissive or non-permissive, may be stated in the
  form of a separately written license, or stated as exceptions;
  the above requirements apply either way.
  
    8. Termination.
  
    You may not propagate or modify a covered work except as expressly
  provided under this License.  Any attempt otherwise to propagate or
  modify it is void, and will automatically terminate your rights under
  this License (including any patent licenses granted under the third
  paragraph of section 11).
  
    However, if you cease all violation of this License, then your
  license from a particular copyright holder is reinstated (a)
  provisionally, unless and until the copyright holder explicitly and
  finally terminates your license, and (b) permanently, if the copyright
  holder fails to notify you of the violation by some reasonable means
  prior to 60 days after the cessation.
  
    Moreover, your license from a particular copyright holder is
  reinstated permanently if the copyright holder notifies you of the
  violation by some reasonable means, this is the first time you have
  received notice of violation of this License (for any work) from that
  copyright holder, and you cure the violation prior to 30 days after
  your receipt of the notice.
  
    Termination of your rights under this section does not terminate the
  licenses of parties who have received copies or rights from you under
  this License.  If your rights have been terminated and not permanently
  reinstated, you do not qualify to receive new licenses for the same
  material under section 10.
  
    9. Acceptance Not Required for Having Copies.
  
    You are not required to accept this License in order to receive or
  run a copy of the Program.  Ancillary propagation of a covered work
  occurring solely as a consequence of using peer-to-peer transmission
  to receive a copy likewise does not require acceptance.  However,
  nothing other than this License grants you permission to propagate or
  modify any covered work.  These actions infringe copyright if you do
  not accept this License.  Therefore, by modifying or propagating a
  covered work, you indicate your acceptance of this License to do so.
  
    10. Automatic Licensing of Downstream Recipients.
  
    Each time you convey a covered work, the recipient automatically
  receives a license from the original licensors, to run, modify and
  propagate that work, subject to this License.  You are not responsible
  for enforcing compliance by third parties with this License.
  
    An "entity transaction" is a transaction transferring control of an
  organization, or substantially all assets of one, or subdividing an
  organization, or merging organizations.  If propagation of a covered
  work results from an entity transaction, each party to that
  transaction who receives a copy of the work also receives whatever
  licenses to the work the party's predecessor in interest had or could
  give under the previous paragraph, plus a right to possession of the
  Corresponding Source of the work from the predecessor in interest, if
  the predecessor has it or can get it with reasonable efforts.
  
  You may not impose any further restrictions on the exercise of the
  rights granted or affirmed under this License.  For example, you may
  not impose a license fee, royalty, or other charge for exercise of
  rights granted under this License, and you may not initiate litigation
  (including a cross-claim or counterclaim in a lawsuit) alleging that
  any patent claim is infringed by making, using, selling, offering for
  sale, or importing the Program or any portion of it.
  
  11. Patents.
  
  A "contributor" is a copyright holder who authorizes use under this
  License of the Program or a work on which the Program is based.  The
  work thus licensed is called the contributor's "contributor version".
  
    A contributor's "essential patent claims" are all patent claims
  owned or controlled by the contributor, whether already acquired or
  hereafter acquired, that would be infringed by some manner, permitted
  by this License, of making, using, or selling its contributor version,
  but do not include claims that would be infringed only as a
  consequence of further modification of the contributor version.  For
  purposes of this definition, "control" includes the right to grant
  patent sublicenses in a manner consistent with the requirements of
  this License.
  
  Each contributor grants you a non-exclusive, worldwide, royalty-free
  patent license under the contributor's essential patent claims, to
  make, use, sell, offer for sale, import and otherwise run, modify and
  propagate the contents of its contributor version.
  
    In the following three paragraphs, a "patent license" is any express
  agreement or commitment, however denominated, not to enforce a patent
  (such as an express permission to practice a patent or covenant not to
  sue for patent infringement).  To "grant" such a patent license to a
  party means to make such an agreement or commitment not to enforce a
  patent against the party.
  
    If you convey a covered work, knowingly relying on a patent license,
  and the Corresponding Source of the work is not available for anyone
  to copy, free of charge and under the terms of this License, through a
  publicly available network server or other readily accessible means,
  then you must either (1) cause the Corresponding Source to be so
  available, or (2) arrange to deprive yourself of the benefit of the
  patent license for this particular work, or (3) arrange, in a manner
  consistent with the requirements of this License, to extend the patent
  license to downstream recipients.  "Knowingly relying" means you have
  actual knowledge that, but for the patent license, your conveying the
  covered work in a country, or your recipient's use of the covered work
  in a country, would infringe one or more identifiable patents in that
  country that you have reason to believe are valid.
  
  If, pursuant to or in connection with a single transaction or
  arrangement, you convey, or propagate by procuring conveyance of, a
  covered work, and grant a patent license to some of the parties
  receiving the covered work authorizing them to use, propagate, modify
  or convey a specific copy of the covered work, then the patent license
  you grant is automatically extended to all recipients of the covered
  work and works based on it.
  
  A patent license is "discriminatory" if it does not include within
  the scope of its coverage, prohibits the exercise of, or is
  conditioned on the non-exercise of one or more of the rights that are
  specifically granted under this License.  You may not convey a covered
  work if you are a party to an arrangement with a third party that is
  in the business of distributing software, under which you make payment
  to the third party based on the extent of your activity of conveying
  the work, and under which the third party grants, to any of the
  parties who would receive the covered work from you, a discriminatory
  patent license (a) in connection with copies of the covered work
  conveyed by you (or copies made from those copies), or (b) primarily
  for and in connection with specific products or compilations that
  contain the covered work, unless you entered into that arrangement,
  or that patent license was granted, prior to 28 March 2007.
  
  Nothing in this License shall be construed as excluding or limiting
  any implied license or other defenses to infringement that may
  otherwise be available to you under applicable patent law.
  
  12. No Surrender of Others' Freedom.
  
    If conditions are imposed on you (whether by court order, agreement or
  otherwise) that contradict the conditions of this License, they do not
  excuse you from the conditions of this License.  If you cannot convey a
  covered work so as to satisfy simultaneously your obligations under this
  License and any other pertinent obligations, then as a consequence you may
  not convey it at all.  For example, if you agree to terms that obligate you
  to collect a royalty for further conveying from those to whom you convey
  the Program, the only way you could satisfy both those terms and this
  License would be to refrain entirely from conveying the Program.
  
    13. Use with the GNU Affero General Public License.
  
    Notwithstanding any other provision of this License, you have
  permission to link or combine any covered work with a work licensed
  under version 3 of the GNU Affero General Public License into a single
  combined work, and to convey the resulting work.  The terms of this
  License will continue to apply to the part which is the covered work,
  but the special requirements of the GNU Affero General Public License,
  section 13, concerning interaction through a network will apply to the
  combination as such.
  
    14. Revised Versions of this License.
  
    The Free Software Foundation may publish revised and/or new versions of
  the GNU General Public License from time to time.  Such new versions will
  be similar in spirit to the present version, but may differ in detail to
  address new problems or concerns.
  
    Each version is given a distinguishing version number.  If the
  Program specifies that a certain numbered version of the GNU General
  Public License "or any later version" applies to it, you have the
  option of following the terms and conditions either of that numbered
  version or of any later version published by the Free Software
  Foundation.  If the Program does not specify a version number of the
  GNU General Public License, you may choose any version ever published
  by the Free Software Foundation.
  
    If the Program specifies that a proxy can decide which future
  versions of the GNU General Public License can be used, that proxy's
  public statement of acceptance of a version permanently authorizes you
  to choose that version for the Program.
  
  Later license versions may give you additional or different
  permissions.  However, no additional obligations are imposed on any
  author or copyright holder as a result of your choosing to follow a
  later version.
  
  15. Disclaimer of Warranty.
  
  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
  APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
  HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
  OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
  IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
  ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
  
  16. Limitation of Liability.
  
  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
  WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
  THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
  GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
  USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
                                       DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
                                       PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
  EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGES.
  
  17. Interpretation of Sections 15 and 16.
  
  If the disclaimer of warranty and limitation of liability provided
  above cannot be given local legal effect according to their terms,
  reviewing courts shall apply local law that most closely approximates
  an absolute waiver of all civil liability in connection with the
  Program, unless a warranty or assumption of liability accompanies a
  copy of the Program in return for a fee.
  
  END OF TERMS AND CONDITIONS
  
  How to Apply These Terms to Your New Programs
  
  If you develop a new program, and you want it to be of the greatest
  possible use to the public, the best way to achieve this is to make it
  free software which everyone can redistribute and change under these terms.
  
  To do so, attach the following notices to the program.  It is safest
  to attach them to the start of each source file to most effectively
  state the exclusion of warranty; and each file should have at least
  the "copyright" line and a pointer to where the full notice is found.
  
  <one line to give the program's name and a brief idea of what it does.>
      Copyright (C) <year>  <name of author>
  
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
  
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
  
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  Also add information on how to contact you by electronic and paper mail.
  
    If the program does terminal interaction, make it output a short
  notice like this when it starts in an interactive mode:
  
      <program>  Copyright (C) <year>  <name of author>
      This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
  This is free software, and you are welcome to redistribute it
  under certain conditions; type `show c' for details.
  
  The hypothetical commands `show w' and `show c' should show the appropriate
  parts of the General Public License.  Of course, your program's commands
  might be different; for a GUI interface, you would use an "about box".
  
  You should also get your employer (if you work as a programmer) or school,
  if any, to sign a "copyright disclaimer" for the program, if necessary.
  For more information on this, and how to apply and follow the GNU GPL, see
  <http://www.gnu.org/licenses/>.
  
  The GNU General Public License does not permit incorporating your program
  into proprietary programs.  If your program is a subroutine library, you
  may consider it more useful to permit linking proprietary applications with
  the library.  If this is what you want to do, use the GNU Lesser General
  Public License instead of this License.  But first, please read
  <http://www.gnu.org/philosophy/why-not-lgpl.html>.

Package: gsdmvn
File: NAMESPACE
Format: text
Content:
  # Generated by roxygen2: do not edit by hand
  
  export(ahr_blinded)
  export(gridpts)
  export(gs_b)
  export(gs_design_ahr)
  export(gs_design_combo)
  export(gs_design_npe)
  export(gs_design_nph)
  export(gs_design_wlr)
  export(gs_info_ahr)
  export(gs_info_wlr)
  export(gs_power_ahr)
  export(gs_power_combo)
  export(gs_power_npe)
  export(gs_power_nph)
  export(gs_power_wlr)
  export(gs_prob)
  export(gs_spending_bound)
  export(gs_spending_combo)
  export(h1)
  export(hupdate)
  export(pmvnorm_combo)
  export(wlr_weight_1)
  export(wlr_weight_fh)
  export(wlr_weight_n)
  importFrom(dplyr,"%>%")
  importFrom(dplyr,arrange)
  importFrom(dplyr,desc)
  importFrom(dplyr,filter)
  importFrom(dplyr,full_join)
  importFrom(dplyr,lag)
  importFrom(dplyr,left_join)
  importFrom(dplyr,mutate)
  importFrom(dplyr,n)
  importFrom(dplyr,right_join)
  importFrom(dplyr,select)
  importFrom(dplyr,summarise)
  importFrom(dplyr,summarize)
  importFrom(gsDesign,gsDesign)
  importFrom(gsDesign,sfLDOF)
  importFrom(gsDesign2,AHR)
  importFrom(gsDesign2,eAccrual)
  importFrom(gsDesign2,tEvents)
  importFrom(mvtnorm,GenzBretz)
  importFrom(stats,dnorm)
  importFrom(stats,pnorm)
  importFrom(stats,qnorm)
  importFrom(stats,uniroot)
  importFrom(tibble,tibble)
  importFrom(utils,tail)

Package: gsdmvn
File: NEWS.md
Format: text
Content:
  # gsdmvn 0.1.0
  
  - Initial version.
  - Added a `NEWS.md` file to track changes to the package.

Package: gsdmvn
File: README.md
Format: text
Content:
  
  <!-- README.md is generated from README.Rmd. Please edit that file -->
  
  # gsdmvn
  
  <!-- badges: start -->
  
  [![R-CMD-check](https://github.com/Merck/gsdmvn/workflows/R-CMD-check/badge.svg)](https://github.com/Merck/gsdmvn/actions)
  [![Codecov test
  coverage](https://codecov.io/gh/Merck/gsdmvn/branch/master/graph/badge.svg)](https://codecov.io/gh/Merck/gsdmvn?branch=master)
  <!-- badges: end -->
  
  The goal of **gsdmvn** is to enable fixed or group sequential design
  under non-proportional hazards. Piecewise constant enrollment, failure
  rates and dropout rates for a stratified population are available to
  enable highly flexible enrollment, time-to-event and time-to-dropout
  assumptions. Substantial flexibility on top of what is in the gsDesign
  package is intended for selecting boundaries. While this work is in
  progress, substantial capabilities have been enabled. Comments on
  usability and features are encouraged as this is a development version
  of the package.
  
  The goal of gsdmvn is to enable group sequential trial design for
  time-to-event endpoints under non-proportional hazards assumptions. The
  package is still maturing; as the package functions become more stable,
  they will likely be included in the
  [gsDesign2](https://github.com/Merck/gsDesign2) package.
  
  ## Installation
  
  You can install `gsdmvn` with:
  
  ``` r
  remotes::install_github("Merck/gsdmvn")
  ```
  
  ## Specifying enrollment and failure rates
  
  This is a basic example which shows you how to solve a common problem.
  We assume there is a 4 month delay in treatment effect. Specifically, we
  assume a hazard ratio of 1 for 4 months and 0.6 thereafter. For this
  example we assume an exponential failure rate and low exponential
  dropout rate. The `enrollRates` specification indicates an expected
  enrollment duration of 12 months with exponential inter-arrival times.
  
  ``` r
  library(gsdmvn)
  library(gsDesign)
  library(gsDesign2)
  library(dplyr)
  library(knitr)
  
  ## basic example code
  
  ## Constant enrollment over 12 months
  ## rate will be adjusted later by gsDesignNPH to get sample size
  enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 1)
  
  ## 12 month median exponential failure rate in control
  ## 4 month delay in effect with HR=0.6 after
  ## Low exponential dropout rate
  medianSurv <- 12
  failRates <- tibble::tibble(
    Stratum = "All",
    duration = c(4, Inf),
    failRate = log(2) / medianSurv,
    hr = c(1, .6),
    dropoutRate = .001
  )
  ```
  
  The resulting failure rate specification is the following table. As many
  rows and strata as needed can be specified to approximate whatever
  patterns you wish.
  
  ``` r
  failRates %>% kable()
  ```
  
  | Stratum | duration |  failRate |  hr | dropoutRate |
  |:--------|---------:|----------:|----:|------------:|
  | All     |        4 | 0.0577623 | 1.0 |       0.001 |
  | All     |      Inf | 0.0577623 | 0.6 |       0.001 |
  
  Computing a fixed sample size design with 2.5% one-sided Type I error
  and 90% power. We specify a trial duration of 36 months with
  `analysisTimes`. Since there is a single analysis, we specify an upper
  p-value bound of 0.025 with `upar = qnorm(0.975)`. There is no lower
  bound which is specified with `lpar = -Inf`.
  
  ``` r
  design <-
    gs_design_ahr(enrollRates, failRates, upar = qnorm(.975), lpar = -Inf, IF = 1, analysisTimes = 36)
  ```
  
  The input enrollment rates are scaled to achieve power:
  
  ``` r
  design$enrollRates %>% kable()
  ```
  
  | Stratum | duration |     rate |
  |:--------|---------:|---------:|
  | All     |       12 | 35.05288 |
  
  The failure and dropout rates remain unchanged from what was input:
  
  ``` r
  design$failRates %>% kable()
  ```
  
  | Stratum | duration |  failRate |  hr | dropoutRate |
  |:--------|---------:|----------:|----:|------------:|
  | All     |        4 | 0.0577623 | 1.0 |       0.001 |
  | All     |      Inf | 0.0577623 | 0.6 |       0.001 |
  
  Finally, the expected analysis time is in `Time`, sample size `N`,
  events required `Events` and bound `Z` are in `design$bounds`. Note that
  `AHR` is the average hazard ratio used to calculate the targeted event
  counts. The natural parameter (`log(AHR)`) is in theta and corresponding
  statistical information under the alternate hypothesis are in `info` and
  under the null hypothesis in `info0`.
  
  ``` r
  design$bounds %>% kable()
  ```
  
  | Analysis | Bound | Time |        N |   Events |        Z | Probability |       AHR |     theta |     info |    info0 |
  |---------:|:------|-----:|---------:|---------:|---------:|------------:|----------:|----------:|---------:|---------:|
  |        1 | Upper |   36 | 420.6346 | 311.0028 | 1.959964 |         0.9 | 0.6917244 | 0.3685676 | 76.74383 | 77.75069 |

Package: gsdmvn
File: R/ahr_blinded.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Blinded estimation of average hazard ratio
  #'
  #' Based on blinded data and assumed hazard ratios in different intervals, compute
  #' a blinded estimate of average hazard ratio (AHR) and corresponding estimate of statistical information.
  #' This function is intended for use in computing futility bounds based on spending assuming
  #' the input hazard ratio (hr) values for intervals specified here.
  #'
  #' @param Srv input survival object (see \code{Surv}); note that only 0=censored, 1=event for \code{Surv}
  #' @param intervals Vector containing positive values indicating interval lengths where the
  #' exponential rates are assumed.
  #' Note that a final infinite interval is added if any events occur after the final interval
  #' specified.
  #' @param hr vector of hazard ratios assumed for each interval
  #' @param ratio ratio of experimental to control randomization.
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if input hr is a numeric vector.
  #'    \item Validate if input hr is non-negative.
  #'    \item Simulate piece-wise exponential survival estimation with the inputs survival object Srv
  #'    and intervals.
  #'    \item Save the length of  hr and events to an object, and if the length of hr is shorter than
  #'    the intervals, add replicates of the last element of hr and the corresponding numbers of events
  #'    to hr.
  #'    \item Compute the blinded estimation of average hazard ratio.
  #'    \item Compute adjustment for information.
  #'    \item Return a tibble of the sum of events, average hazard raito, blinded average hazard
  #'    ratio, and the information.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return A \code{tibble} with one row containing
  #' `AHR` blinded average hazard ratio based on assumed period-specific hazard ratios input in `failRates`
  #' and observed events in the corresponding intervals
  #' `Events` total observed number of events, `info` statistical information based on Schoenfeld approximation,
  #' and info0 (information under related null hypothesis) for each value of `totalDuration` input;
  #' if `simple=FALSE`, `Stratum` and `t` (beginning of each constant HR period) are also returned
  #' and `HR` is returned instead of `AHR`
  #'
  #' @examples
  #' \dontrun{
  #' library(simtrial)
  #' library(survival)
  #' ahr_blinded(Srv = Surv(time = simtrial::Ex2delayedEffect$month,
  #'                        event = simtrial::Ex2delayedEffect$evntd),
  #'             intervals = c(4,100),
  #'             hr = c(1, .55),
  #'             ratio = 1)
  #' }
  #'
  #' @export
  ahr_blinded <- function (Srv = survival::Surv(time = simtrial::Ex1delayedEffect$month,
                                                event = simtrial::Ex1delayedEffect$evntd),
                           intervals = array(3, 3),
                           hr = c(1,.6),
                           ratio = 1)
  {   msg <- "hr must be a vector of positive numbers"
  if (!is.vector(hr, mode="numeric")) stop(msg)
  if (min(hr) <= 0) stop(msg)
  
  events <- simtrial::pwexpfit(Srv, intervals)[,3]
  nhr <- length(hr)
  nx <- length(events)
  # Add to hr if length shorter than intervals
  if (length(hr) < length(events)) hr <- c(hr, rep(hr[nhr], nx - nhr))
  
  # Compute blinded AHR
  theta <- sum(log(hr[1:nx]) * events) / sum(events)
  
  # Compute adjustment for information
  Qe <- ratio / (1 + ratio)
  
  return(tibble::tibble(Events = sum(events), AHR=exp(theta), theta = theta, info0 = sum(events) * (1-Qe) * Qe))
  }

Package: gsdmvn
File: R/global.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  # These global variables are declared to eliminate associated R cmd check warnings.
  # There is no other identified functional impact of these global declarations.
  # These are column names used in input and/or output tibbles in the package
  
  utils::globalVariables(
    c(
      'Events',
      'Probability',
      'Stratum',
      'Time',
      'AHR',
      'N',
      'Z',
      'dropoutRate',
      'failRate',
      'duration',
      'rate',
      'theta',
      'info',
      'info0',
      'Bound',
      'Analysis',
      'h'
    )
  )

Package: gsdmvn
File: R/gridpts.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  NULL
  #' Grid points for group sequential design numerical integration
  #'
  #' Points and weights for Simpson's rule numerical integration from
  #' p 349 - 350 of Jennison and Turnbull book.
  #' This is not used for arbitrary integration, but for the canonical form of Jennison and Turnbull.
  #' mu is computed elsewhere as drift parameter times sqrt of information.
  #' Since this is a lower-level routine, no checking of input is done; calling routines should
  #' ensure that input is correct.
  #' Lower limit of integration can be \code{-Inf} and upper limit of integration can be \code{Inf}
  #'
  #' @details
  #' Jennison and Turnbull (p 350) claim accuracy of \code{10E-6} with \code{r=16}.
  #' The numerical integration grid spreads out at the tail to enable accurate tail probability calcuations.
  #'
  #'
  #' @param r Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param mu Mean of normal distribution (scalar) under consideration
  #' @param a lower limit of integration (scalar)
  #' @param b upper limit of integration (scalar \code{> a})
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Define odd numbered grid points for real line.
  #'    \item Trim points outside of [a, b] and include those points.
  #'    \item If extreme, include only 1 point where density will be essentially 0.
  #'    \item Define even numbered grid points between the odd ones.
  #'    \item Compute weights for odd numbered grid points.
  #'    \item Combine odd- and even-numbered grid points with their corresponding weights.
  #'    \item Return a tibble of with grid points in z and numerical integration weights in z.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return A \code{tibble} with grid points in \code{z} and numerical integration weights in \code{w}
  #' @export
  #'
  #' @examples
  #' library(dplyr)
  #'
  #' # approximate variance of standard normal (i.e., 1)
  #' gridpts() %>% summarise(var = sum(z^2 * w * dnorm(z)))
  #'
  #' # approximate probability above .95 quantile (i.e., .05)
  #' gridpts(a = qnorm(.95), b = Inf) %>% summarise(p05 = sum(w * dnorm(z)))
  gridpts <- function(r = 18, mu = 0, a = -Inf, b = Inf){
    # Define odd numbered grid points for real line
    x <- c(mu - 3 - 4 * log(r / (1:(r - 1))),
           mu - 3 + 3 * (0:(4 * r)) / 2 / r,
           mu + 3 + 4 * log(r / (r - 1):1)
           )
    # Trim points outside of [a, b] and include those points
    if (min(x) < a) x <- c(a, x[x > a])
    if (max(x) > b) x <- c(x[x < b], b)
    # If extreme, include only 1 point where density will be essentially 0
    m <- length(x)
    if (m == 1) return(tibble::tibble(z=x, w=1))
  
    # Define even numbered grid points between the odd ones
    y <- (x[2:m] + x[1:(m-1)]) / 2
  
    # Compute weights for odd numbered grid points
    i <- 2:(m-1)
    wodd <- c(x[2] - x[1],
              (x[i + 1] - x[i - 1]),
              x[m] - x[m - 1]) / 6
  
    weven <- 4 * (x[2:m] - x[1:(m-1)]) / 6
  
    # Now combine odd- and even-numbered grid points with their
    # corresponding weights
    z <- rep(0, 2*m - 1)
    z[2 * (1:m) - 1] <- x
    z[2 * (1:(m-1))] <- y
    w <- z
    w[2 * (1:m) - 1] <- wodd
    w[2 * (1:(m-1))] <- weven
  
    return(tibble::tibble(z=z, w=w))
  }

Package: gsdmvn
File: R/gs_b.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' gs_b: Default boundary generation
  #'
  #' \code{gs_b()} is the simplest version of a function to be used with the \code{upper} and \code{lower}
  #' arguments in \code{gs_prob()},
  #' \code{gs_power_nph} and \code{gs_design_nph()};
  #' it simply returns the vector input in the input vector \code{Z} or, if \code{k} is specified \code{par[k]j} is returned.
  #' Note that if bounds need to change with changing information at analyses, \code{gs_b()} should not be used.
  #' For instance, for spending function bounds use
  #' @param par For \code{gs_b()}, this is just Z-values for the boundaries; can include infinite values
  #' @param info Information at analyses; not used in this function; present as it is a standard parameter for other boundary computation routines
  #' @param k is NULL (default), return \code{par}, else return \code{par[k]}
  #' @param ... further arguments passed to or from other methods
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if the input k is null as default.
  #'    \itemize{
  #'      \item If the input k is null as default, return the whole vector of Z-values of the boundaries.
  #'      \item If the input k is not null, return the corresponding boundary in the vector of Z-values.
  #'      }
  #'    \item Return a vector of boundaries.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return returns the vector input \code{par} if \code{k} is NULL, otherwise, \code{par[k]}
  #' @export
  #'
  #' @examples
  #' # Simple: enter a vector of length 3 for bound
  #' gs_b(par = 4:2)
  #'
  #' # 2nd element of par
  #' gs_b(4:2, k = 2)
  #'
  #' # Generate an efficacy bound using a spending function
  #' # Use Lan-DeMets spending approximation of O'Brien-Fleming bound
  #' # as 50%, 75% and 100% of final spending
  #' # Information fraction
  #' IF <- c(.5, .75, 1)
  #' gs_b(par = gsDesign::gsDesign(alpha = .025, k= length(IF), 
  #'      test.type = 1, sfu = gsDesign::sfLDOF, 
  #'      timing = IF)$upper$bound)
  gs_b <- function(par = NULL, info = NULL, k = NULL,...){
    if(is.null(k)){return(par)}else return(par[k])
  }

Package: gsdmvn
File: R/gs_bound.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Lower and Upper Bound of Group Sequential Design
  #'
  #' @param alpha a numeric vector of cumulative allocated alpha in each interim analysis
  #' @param beta  a numeric vector of cumulative allocated beta in each interim analysis
  #' @param theta a numeric vector of effect size under alternative.
  #' @param corr  a matrix of correlation matrix
  #' @param analysis a numeric vector of interim analysis indicator. Default is 1:length(alpha).
  #' @param theta0 a numeric vector of effect size under null hypothesis. Default is 0.
  #' @param binding_lower_bound a logical value to indicate binding lower bound.
  #' @param alpha_bound logical value to indicate if alpha is Type I error or upper bound. Default is FALSE.
  #' @param beta_bound logical value to indicate if beta is Type II error or lower bound. Default is FALSE.
  #' @inheritParams pmvnorm_combo
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Create a vector of allocated alpha in each interim analysis from the cumulative allocated alpha.
  #'    \item Create a vector of allocated beta in each interim analysis from the cumulative allocated beta.
  #'    \item Extract the number of analysis.
  #'    \item Find the upper and lower bound by solving multivariate normal distribution using \code{pmvnorm_combo}
  #'    \item
  #'    \item Return a data frame of upper and lower boundaries of group sequential design.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @examples
  #' library(gsDesign)
  #'
  #' x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,
  #'                        beta = 0.2 , astar = 0 , timing = c( 1 ) ,
  #'                        sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,
  #'                        sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,
  #'                        hr = 0.6 , hr0 = 1 , eta = 0.01 ,
  #'                        gamma = c( 10 ) ,
  #'                        R = c( 12 ) , S = NULL ,
  #'                        T = 36 , minfup = 24 , ratio = 1 )
  #'
  #' cbind(x$lower$bound, x$upper$bound)
  #'
  #' gsdmvn:::gs_bound(alpha = sfLDOF(0.025, 1:3/3)$spend,
  #'          beta = sfLDOF(0.2, 1:3/3)$spend,
  #'          analysis = 1:3,
  #'          theta = x$theta[2] * sqrt(x$n.I),
  #'          corr = outer(1:3, 1:3, function(x,y) pmin(x,y) / pmax(x,y)))
  #'
  #' @importFrom mvtnorm GenzBretz
  #'
  gs_bound <- function(alpha,
                       beta,
                       theta,
                       corr,
                       analysis = 1:length(alpha),
                       theta0 = rep(0, length(analysis)),
                       binding_lower_bound = FALSE,
                       algorithm = GenzBretz(maxpts= 1e5, abseps= 1e-5),
                       alpha_bound = FALSE,
                       beta_bound = FALSE,
                       ...){
  
  
    alpha <- c(alpha[1], diff(alpha))
    beta  <- c(beta[1],  diff(beta))
  
    lower <- NULL
    upper <- NULL
    .lower <- -Inf
  
    n_analysis <- length(unique(analysis))
  
    for(k in 1:n_analysis){
      k_ind <- analysis <= k
  
      bound_fun <- function(.lower, .upper, .prob, .theta, binding_lower_bound = FALSE){
  
        if(binding_lower_bound){
          lower_bound <- c(lower, .lower)
        }else{
          lower_bound <- c(rep(-Inf, k-1), .lower)
        }
        upper_bound <- c(upper, .upper)
  
        p <- pmvnorm_combo(lower_bound,
                           upper_bound,
                           group = analysis[k_ind],
                           mean = .theta[k_ind],
                           corr = corr[k_ind, k_ind], ...)
  
        p - .prob
      }
  
  
      # change .lower for different type of test (gsDesign test.type)
      if(beta_bound){
        .lower <- sum(beta[1:k])
      }else{
        .lower <- uniroot(bound_fun, .lower = -Inf, .prob = beta[k], .theta = theta,
                          binding_lower_bound = TRUE,
                          interval = c(-20, 20), extendInt = "yes")$root
      }
  
      if(alpha_bound){
        .upper <- sum(alpha[1:k])
      }else{
        .upper <- uniroot(bound_fun, .upper = Inf, .prob = alpha[k], .theta = theta0,
                          binding_lower_bound = FALSE,
                          interval = c(-20, 20), extendInt = "yes")$root
      }
  
      lower <- c(lower, .lower)
      upper <- c(upper, .upper)
    }
  
    # Ensure final analysis bound are the same
    lower[n_analysis] <- upper[n_analysis]
  
    data.frame(upper = upper, lower = lower)
  
  }

Package: gsdmvn
File: R/gs_create_arm.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Create "npsurvSS" arm object
  #'
  #' @param total_time total analysis time
  #' @inheritParams gs_info_ahr
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if there is only one stratum.
  #'    \item Calculate the accrual duration.
  #'    \item calculate the accrual intervals.
  #'    \item Calculate the accrual parameter as the proportion of enrollment rate*duration.
  #'    \item Set cure proportion to zero.
  #'    \item set survival intervals and shape.
  #'    \item Set fail rate in failRates to the Weibull scale parameter for the survival distribution in the arm 0.
  #'    \item Set the multiplication of hazard ratio and fail rate to the Weibull scale parameter
  #'    for the survival distribution in the arm 1.
  #'    \item Set the shape parameter to one as the exponential distribution for
  #'    shape parameter for the loss to follow-up distribution
  #'    \item Set the scale parameter to one as the scale parameter for the loss to follow-up
  #'     distribution since the exponential distribution is supported only
  #'    \item Create arm 0 using \code{npsurvSS::create_arm()} using the parameters for arm 0.
  #'    \item Create arm 1 using \code{npsurvSS::create_arm()} using the parameters for arm 1.
  #'    \item Set the class of the two arms.
  #'    \item Return a list of the two arms.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  gs_create_arm <- function(enrollRates,
                            failRates,
                            ratio,
                            total_time = 1e6){
  
    n_stratum <- length(unique(enrollRates$Stratum))
    if(n_stratum > 1){
      stop("Only one stratum is supported")
    }
  
    accr_time     <- sum(enrollRates$duration)
    accr_interval <- cumsum(enrollRates$duration)
    accr_param    <- enrollRates$rate * enrollRates$duration / sum(enrollRates$rate * enrollRates$duration)
  
    surv_cure     <- 0                    # No cure proportion
    surv_interval <- c(0, c(utils::head(failRates$duration, -1), Inf))
    surv_shape    <- 1                    # Exponential Distribution
    surv_scale0   <- failRates$failRate
    surv_scale1   <- failRates$hr * failRates$failRate
  
    loss_shape    <- 1                         # Exponential Distribution
    loss_scale    <- failRates$dropoutRate[1]  # Only Exponential Distribution is supported
  
    # Control Group
    arm0 <- npsurvSS::create_arm(size = 1,
  
                                 accr_time = accr_time,
                                 accr_dist = "pieceuni",
                                 accr_interval = accr_interval,
                                 accr_param = accr_param,
  
                                 surv_cure = surv_cure,
                                 surv_interval = surv_interval,
                                 surv_shape = surv_shape,
                                 surv_scale = surv_scale0,
  
                                 loss_shape = loss_shape,
                                 loss_scale = loss_scale,
  
                                 total_time = total_time)
  
  
    # Control Group
    arm1 <- npsurvSS::create_arm(size = ratio,
  
                                 accr_time = accr_time,
                                 accr_dist = "pieceuni",
                                 accr_interval = accr_interval,
                                 accr_param = accr_param,
  
                                 surv_cure = surv_cure,
                                 surv_interval = surv_interval,
                                 surv_shape = surv_shape,
                                 surv_scale = surv_scale1,
  
                                 loss_shape = loss_shape,
                                 loss_scale = loss_scale,
  
                                 total_time = total_time)
  
    class(arm0) <- c("list", "arm")
    class(arm1) <- c("list", "arm")
  
    list(arm0 = arm0,
         arm1 = arm1)
  
  }
  

Package: gsdmvn
File: R/gs_design_ahr.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom gsDesign gsDesign sfLDOF
  #' @importFrom stats qnorm
  #' @importFrom gsDesign2 AHR
  #' @importFrom dplyr mutate full_join select arrange desc
  NULL
  #' Group sequential design using average hazard ratio under non-proportional hazards
  #'
  #' @param enrollRates enrollment rates
  #' @param failRates failure and dropout rates
  #' @param ratio Experimental:Control randomization ratio (not yet implemented)
  #' @param alpha One-sided Type I error
  #' @param beta Type II error
  #' @param IF Targeted information fraction at each analysis
  #' @param analysisTimes Minimum time of analysis
  #' @param binding indicator of whether futility bound is binding; default of FALSE is recommended
  #' @param upper Function to compute upper bound
  #' @param upar Parameter passed to \code{upper()}
  #' @param lower Function to compute lower bound
  #' @param lpar Parameter passed to \code{lower()}
  #' @param h1_spending Indicator that lower bound to be set by spending under alternate hypothesis (input \code{failRates})
  #' if spending is used for lower bound
  #' @param test_upper indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  #' otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound
  #' @param test_lower indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  #' single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  #' lower bound
  #' @param r  Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param tol Tolerance parameter for boundary convergence (on Z-scale)
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if input analysisTimes is a positive number or positive increasing sequence.
  #'    \item Validate if input IF is a positive number or positive increasing sequence
  #'    on (0, 1] with final value of 1.
  #'    \item Validate if input IF and analysisTimes  have the same length if both have length > 1.
  #'    \item Get information at input analysisTimes
  #'    \itemize{
  #'      \item Use \code{gs_info_ahr()} to get the information and effect size based on AHR approximation.
  #'      \item Extract the final event.
  #'      \item Check if input If needed for (any) interim analysis timing.
  #'    }
  #'    \item Add the analysis column to the information at input analysisTimes.
  #'    \item Add the sample size column to the information at input analysisTimes using \code{eAccrual()}.
  #'    \item Get sample size and bounds using \code{gs_design_npe()} and save them to bounds.
  #'    \item Add Time, Events, AHR, N that have already been calculated to the bounds.
  #'    \item Return a list of design enrollment, failure rates, and bounds.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return a \code{tibble} with columns Analysis, Bound, Z, Probability, theta, Time, AHR, Events
  #' @details Need to be added
  #' @export
  #'
  #' @examples
  #' library(gsDesign)
  #' library(gsDesign2)
  #' library(dplyr)
  #' # call with defaults
  #' gs_design_ahr()
  #'
  #' # Single analysis
  #' gs_design_ahr(analysisTimes = 40)
  #'
  #' # Multiple analysisTimes
  #' gs_design_ahr(analysisTimes = c(12,24,36))
  #'
  #' # Specified information fraction
  #' gs_design_ahr(IF = c(.25,.75,1), analysisTimes = 36)
  #'
  #' # multiple analysis times & IF
  #' # driven by times
  #' gs_design_ahr(IF = c(.25,.75,1), analysisTimes = c(12,25,36))
  #' # driven by IF
  #' gs_design_ahr(IF = c(1/3, .8, 1), analysisTimes = c(12,25,36))
  #'
  #' # 2-sided symmetric design with O'Brien-Fleming spending
  #' gs_design_ahr(analysisTimes = c(12, 24, 36),
  #'               binding = TRUE,
  #'               upper = gs_spending_bound,
  #'               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                           param = NULL, timing = NULL),
  #'               lower = gs_spending_bound,
  #'               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                           param = NULL, timing = NULL),
  #'               h1_spending = FALSE)
  #'
  #' # 2-sided asymmetric design with O'Brien-Fleming upper spending
  #' # Pocock lower spending under H1 (NPH)
  #' gs_design_ahr(analysisTimes = c(12, 24, 36),
  #'               binding = TRUE,
  #'               upper = gs_spending_bound,
  #'               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                           param = NULL, timing = NULL),
  #'               lower = gs_spending_bound,
  #'               lpar = list(sf = gsDesign::sfLDPocock, total_spend = 0.1, 
  #'                           param = NULL, timing = NULL),
  #'               h1_spending = TRUE)
  gs_design_ahr <- function(enrollRates=tibble::tibble(Stratum="All",
                                                       duration=c(2,2,10),
                                                       rate=c(3,6,9)),
                            failRates=tibble::tibble(Stratum="All",
                                                     duration=c(3,100),
                                                     failRate=log(2)/c(9,18),
                                                     hr=c(.9,.6),
                                                     dropoutRate=rep(.001,2)),
                            ratio=1,               # Experimental:Control randomization ratio
                            alpha = 0.025,         # One-sided Type I error
                            beta = 0.1,            # NULL if enrollment is not adapted
                            IF = NULL,             # relative information fraction timing (vector, if not NULL; increasing to 1)
                            analysisTimes = 36,    # Targeted times of analysis or just planned study duration
                            binding = FALSE,
                            upper = gs_b,
                            # Default is Lan-DeMets approximation of
                            upar = gsDesign(k=3, test.type=1,
                                            n.I=c(.25, .75, 1),
                                            sfu=sfLDOF, sfupar = NULL)$upper$bound,
                            lower = gs_b,
                            lpar = c(qnorm(.1), -Inf, -Inf), # Futility only at IA1
                            h1_spending = TRUE,
                            test_upper = TRUE,
                            test_lower = TRUE,
                            r = 18,
                            tol = 1e-6
  ){
      ################################################################################
      # Check input values
      msg <- "analysisTimes must be a positive number or positive increasing sequence"
      if (!is.vector(analysisTimes,mode = "numeric")) stop(msg)
      if (min(analysisTimes - dplyr::lag(analysisTimes, def=0))<=0) stop(msg)
      msg <- "gs_design_ahr(): IF must be a positive number or positive increasing sequence on (0, 1] with final value of 1"
      if (is.null(IF)){IF <- 1}
      if (!is.vector(IF,mode = "numeric")) stop(msg)
      if (min(IF - dplyr::lag(IF, def=0))<=0) stop(msg)
      if (max(IF) != 1) stop(msg)
      msg <- "gs_design_ahr() IF and analysisTimes must have the same length if both have length > 1"
      if ((length(analysisTimes)>1) & (length(IF) > 1) & (length(IF) != length(analysisTimes))) stop(msg)
      # end check input values
      ################################################################################
      # Get information at input analysisTimes
      y <- gs_info_ahr(enrollRates, failRates, ratio = ratio, events = NULL, analysisTimes=analysisTimes)
      finalEvents <- y$Events[nrow(y)]
      IFalt <- y$Events / finalEvents
      # Check if IF needed for (any) IA timing
      K <- max(length(analysisTimes), length(IF))
      nextTime <- max(analysisTimes)
      if(length(IF)==1){IF <- IFalt}else{
          IFindx <- IF[1:(K-1)]
          for(i in seq_along(IFindx)){
              if(length(IFalt) == 1){y <-
                  rbind(tEvents(enrollRates, failRates, targetEvents = IF[K - i] * finalEvents, ratio = ratio,
                                interval = c(.01, nextTime)) %>% mutate(theta=-log(AHR), Analysis=K-i),
                        y)
              }else if (IF[K-i] > IFalt[K-i]) y[K - i,] <-
                      tEvents(enrollRates, failRates, targetEvents = IF[K - i] * finalEvents, ratio = ratio,
                              interval = c(.01, nextTime)) %>%
                      dplyr::transmute(Analysis = K - i, Time, Events, AHR, theta=-log(AHR), info, info0)
              nextTime <- y$Time[K - i]
          }
      }
      y$Analysis <- 1:K
      y$N <- eAccrual(x = y$Time, enrollRates = enrollRates)
      if(h1_spending){
          theta1 <- y$theta
          info1 <- y$info
      }else{
          theta1 <- 0
          info1 <- y$info0
      }
  
      # Get sample size and bounds using gs_design_npe
      bounds <- gs_design_npe(theta = y$theta,
                              theta1 = theta1,
                              info = y$info,
                              info0 = y$info0,
                              info1 = info1,
                              alpha = alpha,
                              beta = beta,
                              binding = binding,
                              upper = upper,
                              lower = lower,
                              upar = upar,
                              lpar = lpar,
                              test_upper = test_upper,
                              test_lower = test_lower,
                              r = r,
                              tol = tol) %>%
                 # Add Time, Events, AHR, N from gs_info_ahr call above
                 full_join(y %>% select(-c(info,info0,theta)), by = "Analysis") %>%
                 select(c("Analysis", "Bound", "Time", "N", "Events", "Z", "Probability", "AHR", "theta", "info", "info0")) %>%
                 arrange(desc(Bound), Analysis)
      bounds$Events <- bounds$Events * bounds$info[K] / y$info[K]
      bounds$N <- bounds$N * bounds$info[K] / y$info[K]
  
      # Document design enrollment, failure rates, and bounds
      return(list(enrollRates = enrollRates %>%
                                mutate(rate = rate * bounds$info[K] / y$info[K]),
                  failRates = failRates,
                  bounds = bounds)
      )
  }

Package: gsdmvn
File: R/gs_design_combo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Group sequential design using MaxCombo test under non-proportional hazards
  #'
  #' @inheritParams gs_design_ahr
  #' @inheritParams pmvnorm_combo
  #' @param fh_test a data frame to summarize the test in each analysis.
  #'                Refer examples for its data structure.
  #' @param n_upper_bound a numeric value of upper limit of sample size
  #'
  #'
  #' @examples
  #' \dontrun{
  #'
  #' # The example is slow to run
  #'
  #' library(dplyr)
  #' library(mvtnorm)
  #' library(gsDesign)
  #' 
  #' enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500/12)
  #'
  #' failRates <- tibble::tibble(Stratum = "All",
  #'                             duration = c(4, 100),
  #'                             failRate = log(2) / 15,  # median survival 15 month
  #'                             hr = c(1, .6),
  #'                             dropoutRate = 0.001)
  #'
  #' fh_test <- rbind( data.frame(rho = 0, gamma = 0, tau = -1,
  #'                              test = 1,
  #'                              Analysis = 1:3,
  #'                              analysisTimes = c(12, 24, 36)),
  #'                   data.frame(rho = c(0, 0.5), gamma = 0.5, tau = -1,
  #'                              test = 2:3,
  #'                              Analysis = 3, analysisTimes = 36)
  #' )
  #'
  #' x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,
  #'                        beta = 0.2 , astar = 0 , timing = c( 1 ) ,
  #'                        sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,
  #'                        sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,
  #'                        hr = 0.6 , hr0 = 1 , eta = 0.01 ,
  #'                        gamma = c( 10 ) ,
  #'                        R = c( 12 ) , S = NULL ,
  #'                        T = 36 , minfup = 24 , ratio = 1 )
  #'
  #' # User defined boundary
  #' gs_design_combo(enrollRates,
  #'                 failRates,
  #'                 fh_test,
  #'                 alpha = 0.025,
  #'                 beta = 0.2,
  #'                 ratio = 1,
  #'                 binding = FALSE,                 # test.type = 4 non-binding futility bound
  #'                 upar = x$upper$bound,
  #'                 lpar = x$lower$bound)
  #'
  #' # Boundary derived by spending function
  #' gs_design_combo(enrollRates,
  #'                 failRates,
  #'                 fh_test,
  #'                 alpha = 0.025,
  #'                 beta = 0.2,
  #'                 ratio = 1,
  #'                 binding = FALSE,                 # test.type = 4 non-binding futility bound
  #'                 upper = gs_spending_combo,
  #'                 upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   # alpha spending
  #'                 lower = gs_spending_combo,
  #'                 lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2),     # beta spending
  #' )
  #' }
  #'
  #'
  #' @importFrom mvtnorm GenzBretz
  #'
  #' @export
  gs_design_combo <- function(enrollRates,
                              failRates,
                              fh_test,
                              ratio = 1,
                              alpha = 0.025,
                              beta = 0.2,
                              binding = FALSE,
                              upper = gs_b,
                              upar = c(3,2,1),
                              lower = gs_b,
                              lpar = c(-1, 0, 1),
                              algorithm = GenzBretz(maxpts= 1e5, abseps= 1e-5),
                              n_upper_bound = 1e3,
                              ...){
  
    # Currently only support user defined lower and upper bound
    stopifnot( identical(upper, gs_b) | identical(upper, gs_spending_combo) )
    stopifnot( identical(lower, gs_b) | identical(lower, gs_spending_combo) )
  
    n_analysis <- length(unique(fh_test$Analysis))
  
    # Obtain utilities
    utility <- gs_utility_combo(enrollRates = enrollRates,
                                failRates = failRates,
                                fh_test = fh_test,
                                ratio = ratio,
                                algorithm = algorithm, ...)
  
    info     <- utility$info_all
    info_fh  <- utility$info
    theta_fh <- utility$theta
    corr_fh  <- utility$corr
  
    # Information Fraction
    if(n_analysis == 1){
      min_info_frac <- 1
    }else{
      info_frac <- tapply(info$info0, info$test, function(x) x / max(x))
      min_info_frac <- apply(do.call(rbind, info_frac), 2, min)
    }
  
  
    # Function to calculate power
    foo <- function(n, beta, ...){
  
      # Probability Cross Boundary
      prob <- gs_prob_combo(upper_bound = bound$upper,
                            lower_bound = bound$lower,
                            analysis = info_fh$Analysis,
                            theta = theta_fh * sqrt(n),
                            corr = corr_fh,
                            algorithm = algorithm, ...)
  
      max(subset(prob, Bound == "Upper")$Probability) - (1 - beta)
    }
  
    # Find sample isze and bound
    n <- max(info$N)
    n0 <- 0
    while( (abs(n - n0)) > 1e-2){
      # print(n)
      n0 <- n
  
      # Obtain spending function
      bound <- gs_bound(alpha = upper(upar, min_info_frac),
                        beta = lower(lpar, min_info_frac),
                        analysis = info_fh$Analysis,
                        theta = theta_fh * sqrt(n),
                        corr = corr_fh,
                        binding_lower_bound = binding,
                        algorithm = algorithm,
                        alpha_bound = identical(upper, gs_b),
                        beta_bound = identical(lower, gs_b),
                        ...)
  
  
      n <- uniroot(foo, c(1, n_upper_bound), extendInt = "yes", beta = beta, ...)$root
  
    }
  
  
    # Probability Cross Boundary
    prob <- gs_prob_combo(upper_bound = bound$upper,
                          lower_bound = bound$lower,
                          analysis = info_fh$Analysis,
                          theta = theta_fh * sqrt(n),
                          corr = corr_fh,
                          algorithm = algorithm, ...)
  
    # Probability Cross Boundary under Null
    prob_null <- gs_prob_combo(upper_bound = bound$upper,
                               lower_bound = if(binding){bound$lower}else{rep(-Inf, nrow(bound))},
                               analysis = info_fh$Analysis,
                               theta = rep(0, nrow(info_fh)),
                               corr = corr_fh,
                               algorithm = algorithm, ...)
  
    if(binding == FALSE){
      prob_null$Probability[prob_null$Bound == "Lower"] <- NA
    }
  
    prob$Probability_Null <- prob_null$Probability
  
    # Prepare output
    db <- merge(data.frame(Analysis = 1:(nrow(prob)/2), prob, Z = unlist(bound)),
                unique(info_fh[, c("Analysis", "Time", "N", "Events")])
    )
  
  
    # update sample size and events
    db$Events <- db$Events * n / max(db$N)
    db$N <- db$N * n / max(db$N)
  
    db[order(db$Bound, decreasing = TRUE), c("Analysis", "Bound", "Time", "N", "Events", "Z", "Probability", "Probability_Null")]
  
  }

Package: gsdmvn
File: R/gs_design_npe.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom stats qnorm uniroot
  NULL
  #' Group sequential design computation with non-constant effect and information
  #'
  #' \code{gs_design_npe()} derives group sequential design size, bounds and boundary crossing probabilities based on proportionate
  #' information and effect size at analyses.
  #' It allows a non-constant treatment effect over time, but also can be applied for the usual homogeneous effect size designs.
  #' It requires treatment effect and proportionate statistical information at each analysis as well as a method of deriving bounds, such as spending.
  #' The routine enables two things not available in the gsDesign package: 1) non-constant effect, 2) more flexibility in boundary selection.
  #' For many applications, the non-proportional-hazards design function \code{gs_design_nph()} will be used; it calls this function.
  #' Initial bound types supported are 1) spending bounds, 2) fixed bounds, and 3) Haybittle-Peto-like bounds.
  #' The requirement is to have a boundary update method that can each bound without knowledge of future bounds.
  #' As an example, bounds based on conditional power that require knowledge of all future bounds are not supported by this routine;
  #' a more limited conditional power method will be demonstrated.
  #' Boundary family designs Wang-Tsiatis designs including the original (non-spending-function-based) O'Brien-Fleming and Pocock designs
  #' are not supported by \code{gs_power_npe()}.
  #' @param theta natural parameter for group sequential design representing expected incremental drift at all analyses;
  #' used for power calculation
  #' @param theta1 natural parameter used for lower bound spending; if \code{NULL}, this will be set to \code{theta}
  #' which yields the usual beta-spending. If set to 0, spending is 2-sided under null hypothesis.
  #' @param info proportionate statistical information at all analyses for input \code{theta}
  #' @param info0 proportionate statistical information under null hypothesis, if different than alternative;
  #' impacts null hypothesis bound calculation
  #' @param info1 proportionate statistical information under alternate hypothesis;
  #' impacts null hypothesis bound calculation
  #' @param alpha One-sided Type I error
  #' @param beta Type II error
  #' @param binding indicator of whether futility bound is binding; default of FALSE is recommended
  #' @param upper function to compute upper bound
  #' @param lower function to compare lower bound
  #' @param upar parameter to pass to function provided in \code{upper}
  #' @param lpar Parameter passed to function provided in \code{lower}
  #' @param test_upper indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  #' otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound
  #' @param test_lower indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  #' single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  #' lower bound
  #' @param r  Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param tol Tolerance parameter for boundary convergence (on Z-scale)
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if input info is a numeric vector  or NULL, if non-NULL validate if it
  #'    is strictly increasing and positive.
  #'    \item Validate if input info0 is a numeric vector or NULL, if non-NULL validate if it
  #'     is strictly increasing and positive.
  #'    \item Validate if input info1 is a numeric vector or NULL, if non-NULL validate if it
  #'    is strictly increasing and positive.
  #'    \item Validate if input theta is a real vector and has the same length as info.
  #'    \item Validate if input theta1 is a real vector and has the same length as info.
  #'    \item Validate if input test_upper and test_lower are logical and have the same length as info.
  #'    \item Validate if input test_upper value is TRUE.
  #'    \item Validate if input alpha and beta are positive and of length one.
  #'    \item Validate if input alpha and beta are from the unit interval and alpha is smaller than beta.
  #'    \item Initialize bounds, numerical integration grids, boundary crossing probabilities.
  #'    \item Compute fixed sample size for desired power and Type I error.
  #'    \item Find an interval for information inflation to give correct power using \code{gs_power_npe()}.
  
  #'    \item
  #'    \item If there is no interim analysis, return a tibble including Analysis time, upper bound, Z-value,
  #'    Probability of crossing bound, theta, info0 and info1.
  #'    \item If the desing is a group sequential design, return a tibble of Analysis,
  #'     Bound, Z, Probability,  theta, info, info0.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return a \code{tibble} with columns Analysis, Bound, Z, Probability,  theta, info, info0
  #' @details The inputs \code{info} and \code{info0} should be vectors of the same length with increasing positive numbers.
  #' The design returned will change these by some constant scale factor to ensure the design has power \code{1 - beta}.
  #' The bound specifications in \code{upper, lower, upar, lpar} will be used to ensure Type I error and other boundary properties are as specified.
  #' @author Keaven Anderson \email{keaven\_anderson@@merck.}
  #'
  #' @export
  #'
  #' @examples
  #'
  #' library(gsDesign)
  #' library(dplyr)
  #' # Single analysis
  #' # Lachin book p 71 difference of proportions example
  #' pc <- .28 # Control response rate
  #' pe <- .40 # Experimental response rate
  #' p0 <- (pc + pe) / 2 # Ave response rate under H0
  #' # Information per increment of 1 in sample size
  #' info0 <- 1 / (p0 * (1 - p0) * 4)
  #' info1 <- 1 / (pc * (1 - pc) * 2 + pe * (1 - pe) * 2)
  #' # Result should round up to next even number = 652
  #' # Divide information needed under H1 by information per patient added
  #' gs_design_npe(theta = pe - pc, info = info1, info0 = info0)$info[1] / info1
  #'
  #' # Fixed bound
  #' design <-
  #' gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 80, 
  #'               info0 = (1:3) * 80, info1 = (1:3) * 80,
  #'               upper = gs_b, upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,
  #'               lower = gs_b, lpar = c(-1, 0, 0))
  #' design
  #'
  #' # Same fixed bounds, null hypothesis
  #' gs_power_npe(theta = rep(0,3), info = design$info0[1:3], 
  #'             upar = design$Z[1:3], lpar = design$Z[4:6])
  #'
  #' # Same upper bound; this represents non-binding Type I error and will total 0.025
  #' gs_power_npe(theta = rep(0,3), info = design$info0[1:3], 
  #'              upar = design$Z[1:3], lpar = rep(-Inf,3)) %>% 
  #'   filter(Bound=="Upper")
  #'
  #' # Spending bound examples
  #'
  #' # Design with futility only at analysis 1; efficacy only at analyses 2, 3
  #' # Spending bound for efficacy; fixed bound for futility
  #' # NOTE: test_upper and test_lower DO NOT WORK with gs_b; must explicitly make bounds infinite
  #' # test_upper and test_lower DO WORK with gs_spending_bound
  #' design <-
  #' gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 40, info0 = (1:3) * 40,
  #'               upper = gs_spending_bound,
  #'               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                           param = NULL, timing = NULL),
  #'               lower = gs_b, lpar = c(-1, -Inf, -Inf),
  #'               test_upper = c(FALSE, TRUE, TRUE))
  #' design
  #'
  #' # Spending function bounds
  #' # 2-sided asymmetric bounds
  #' # Lower spending based on non-zero effect
  #' gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
  #'              upper = gs_spending_bound,
  #'              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                          param = NULL, timing = NULL),
  #'              lower = gs_spending_bound,
  #'              lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1, 
  #'                          param = -1, timing = NULL))
  #'
  #' # Two-sided symmetric spend, O'Brien-Fleming spending
  #' # Typically, 2-sided bounds are binding
  #' xx <- gs_design_npe(theta = c(.1, .2, .3), theta1 = rep(0, 3), info = (1:3) * 40,
  #'                     binding = TRUE,
  #'                     upper = gs_spending_bound,
  #'                     upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                                 param = NULL, timing = NULL),
  #'                     lower = gs_spending_bound,
  #'                     lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
  #'                                 param = NULL, timing = NULL))
  #' xx
  #'
  #' # Re-use these bounds under alternate hypothesis
  #' # Always use binding = TRUE for power calculations
  #' upar <- (xx %>% filter(Bound=="Upper"))$Z
  #' gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
  #'              binding = TRUE,
  #'              upar = upar,
  #'              lpar = -upar)
  #'
  gs_design_npe <- function(theta = .1, theta1 = NULL, info = 1, info0 = NULL, info1 = NULL,
                            alpha = 0.025, beta = .1, binding = FALSE,
                            upper=gs_b, lower=gs_b, upar = qnorm(.975), lpar= -Inf,
                            test_upper = TRUE, test_lower = TRUE,
                            r = 18, tol = 1e-6){
    #######################################################################################
    # WRITE INPUT CHECK TESTS AND RETURN APPROPRIATE ERROR MESSAGES
    # info should be a scalar or vector of positive increasing values
    # info0, info1 should be NULL or of the same form as info
    # theta should be a scalar or vector of real values; if vector, same length as info
    # theta0, theta1 should be NULL or same form and length as theta
    # test_upper and test_lower should be logical scalar or vector; if vector same length as info
    # alpha and beta should be scalars with 0 < alpha < 1 - beta < 1
  
    # CHECK STATISTICAL INFORMATION PARAMETERS: info, info0, info1
    if (!is.vector(info, mode = "numeric")) stop("gs_design_npe(): info must be specified numeric vector")
    K <- length(info)
    if (is.null(info0)) info0 <- info
    if (is.null(info1)) info1 <- info
    if (!is.vector(info0, mode = "numeric")) stop("gs_design_npe(): info0 must be specified numeric vector or NULL")
    if (!is.vector(info1, mode = "numeric")) stop("gs_design_npe(): info1 must be specified numeric vector or NULL")
    if (length(info1) != length(info) || length(info0) != length(info) ) stop("gs_design_npe(): length of info, info0, info1 must be the same")
    if (min(info - lag(info,default = 0)<=0)) stop("gs_design_npe(): info much be strictly increasing and positive")
    if (min(info0 - lag(info0,default = 0)<=0)) stop("gs_design_npe(): info0 much be NULL or strictly increasing and positive")
    if (min(info1 - lag(info1,default = 0)<=0)) stop("gs_design_npe(): info1 much be NULL or strictly increasing and positive")
  
    # CHECK TREATMENT EFFECT PARAMETERS: theta, theta0, theta1
    if (!is.vector(theta, mode = "numeric")) stop("gs_design_npe(): theta must be a real vector")
    if (length(theta) == 1 && K > 1) theta <- rep(theta, K)
    if (length(theta) != K) stop("gs_design_npe(): if length(theta) > 1, must be same as info")
    if (theta[K] <= 0) stop("gs_design_npe(): final effect size must be > 0")
    if (is.null(theta1)){theta1 <- theta}else if (length(theta1)==1) theta1 <- rep(theta1,K)
    if (!is.vector(theta1, mode = "numeric")) stop("gs_design_npe(): theta1 must be a real vector")
    if (length(theta1) != K) stop("gs_design_npe(): if length(theta1) > 1, must be same as info")
    # CHECK CORRECT SPEC OF test_upper and test_lower
    if (length(test_upper) == 1 && K > 1) test_upper <- rep(test_upper, K)
    if (length(test_lower) == 1 && K > 1) test_lower <- rep(test_lower, K)
  
    ## Check test_upper and test_lower are logical and correct length
    if (!is.vector(test_upper, mode = "logical") || !is.vector(test_lower, mode = "logical"))
      stop("gs_design_npe(): test_upper and test_lower must be logical")
    if (!(length(test_upper) == 1 || length(test_upper) == K))
      stop("gs_design_npe(): test_upper must be length 1 or same length as info")
    if (!(length(test_lower) == 1 || length(test_lower) == K))
      stop("gs_design_npe(): test_lower must be length 1 or same length as info")
    ## Check that final test_upper value is TRUE
    if (!dplyr::last(test_upper)) stop("gs_design_npe(): last value of test_upper must be TRUE")
  
    ## Check alpha and beta numeric, scalar, 0 < alpha < 1 - beta
    if (!is.numeric(alpha)) stop("gs_design_npe(): alpha must be numeric")
    if (!is.numeric(beta)) stop("gs_design_npe(): beta must be numeric")
    if (length(alpha) != 1 || length(beta) != 1) stop("gs_design_npe(): alpha and beta must be length 1")
    if (alpha <= 0 || 1 - beta <= alpha || beta <= 0) stop("gs_design_npe(): must have 0 < alpha < 1 - beta < 1")
  
  ## END OF INPUT CHECKS ############################################################################
  
    # Initialize bounds, numerical integration grids, boundary crossing probabilities
    a <- rep(-Inf, K)
    b <- rep(Inf, K)
    hgm1_0 <- NULL
    hgm1_1 <- NULL
    upperProb <- rep(NA, K)
    lowerProb <- rep(NA, K)
  
    ## Compute fixed sample size for desired power and Type I error.
    minx <- ((qnorm(alpha) / sqrt(info0[K]) + qnorm(beta) / sqrt(info[K])) / theta[K])^2
  
    ## For a fixed design, this is all you need.
    if (K == 1) return(tibble::tibble(
      Analysis = 1,
      Bound = "Upper",
      Z= qnorm(1-alpha),
      Probability = 1 - beta,
      theta = theta,
      info = info * minx,
      info0 =info0 * minx)
    )
  
    ## Find an interval for information inflation to give correct power
    minpwr <- gs_power_npe(theta = theta, theta1 = theta1,
                           info = info * minx, info1 = info * minx, info0 = info0 * minx,
                           binding = binding,
                           upper=upper, lower=lower, upar = upar, lpar= lpar,
                           test_upper = test_upper, test_lower = test_lower,
                           r = r, tol = tol)$Probability[K]
  
    ##### FOLLOWING IS PAINFUL AND SHOULD NEVER BE NEEDED
    ##### BUT IF IT IS NEEDED, IT TELLS YOU WHAT WENT WRONG!
    ##### NEED TO BRACKET TARGETED POWER BEFORE ROOT FINDING
  
    ## Ensure minx gives power < 1 - beta and maxx gives power > 1 - beta
    if (minpwr < 1 - beta){
      maxx <- 1.05 * minx
      ## Ensure maxx is sufficient information inflation to overpower
      err <- 1
      for(i in 1:10){
        maxpwr <- gs_power_npe(theta = theta, theta1 = theta1,
                               info = info * maxx, info1 = info * maxx, info0 = info0 * maxx,
                               binding = binding,
                               upper=upper, lower=lower, upar = upar, lpar= lpar,
                               test_upper = test_upper, test_lower = test_lower,
                               r = r, tol = tol)$Probability[K]
        if (1  - beta > maxpwr){
          minx <- maxx
          maxx <- 1.05 * maxx
        }else{
            err <- 0
            break
        }
      }
      if (err) stop("gs_design_npe: could not inflate information to bracket power before root finding")
    }else{
      maxx <- minx
      minx <- maxx / 1.05
      err <- 1
      for(i in 1:10){
        if (1  - beta < gs_power_npe(theta = theta, theta1 = theta1,
                                     info = info * minx, info1 = info1 * minx, info0 = info0 * minx,
                                     binding = binding,
                                     upper=upper, lower=lower, upar = upar, lpar= lpar,
                                     test_upper = test_upper, test_lower = test_lower,
                                     r = r, tol = tol)$Probability[K]
        ){maxx <- minx
          minx <- minx / 1.05}else{err <- 0
                                break
                               }
      }
      if (err) stop("gs_design_npe: could not deflate information to bracket targeted power before root finding")
    }
    #### EITHER TARGETED POWER NOW BRACKETED OR ERROR MESSAGE HAS BEEN RETURNED
    #### AND WE CAN ACTUALLY GO ON TO FIND THE ROOT
  
    ## Use root finding with the above function to find needed sample size inflation
    # Now we can solve for the inflation factor for the enrollment rate to achieve the desired power
    res <- try(
      uniroot(errbeta, lower = minx, upper = maxx,
              theta = theta, theta1 = theta1, K = K, beta = beta,
              info = info, info1 = info1, info0 = info0, binding = binding,
              Zupper=upper, Zlower=lower, upar = upar, lpar= lpar,
              test_upper = test_upper, test_lower = test_lower,
              r = r, tol = tol)
    )
    if(inherits(res,"try-error")){stop("gs_design_npe: Sample size solution not found")}
  
    ## Update targeted info, info0 based on inflation factor and return a tibble with
    ## bounds, targeted information, and boundary crossing probabilities at each analysis
    return(gs_power_npe(theta = theta, theta1 = theta1,
                        info = info * res$root, info1 = info1 * res$root, info0 = info0 * res$root,
                        binding = binding,
                        upper=upper, lower=lower, upar = upar, lpar= lpar,
                        test_upper = test_upper, test_lower = test_lower,
                        r = r, tol = tol))
  }
  ## Create a function that uses gs_power_npe to compute difference from targeted power
  ## for a given sample size inflation factor
  errbeta <- function(x = 1, K = 1, beta = .1, theta = .1, theta1 = .1, info = 1, info1 = 1, info0 = 1, binding = FALSE,
                      Zupper=gs_b, Zlower=gs_b, upar = qnorm(.975), lpar= -Inf,
                      test_upper = TRUE, test_lower = TRUE,
                      r = 18, tol = 1e-6){
    return(1 -  beta -
             gs_power_npe(theta = theta, theta1 = theta1,
                          info = info * x, info1 = info1 * x, info0 = info0 * x, binding = binding,
                          upper = Zupper, lower = Zlower, upar = upar, lpar= lpar,
                          test_upper = test_upper, test_lower = test_lower,
                          r = r, tol = tol)$Probability[K])
  }
  

Package: gsdmvn
File: R/gs_design_nph.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom stats qnorm uniroot
  #' @importFrom utils tail
  #' @importFrom gsDesign gsDesign sfLDOF
  #' @importFrom gsDesign2 eAccrual AHR tEvents
  #' @importFrom dplyr select left_join n
  
  NULL
  #' Fixed and group sequential design under non-proportional hazards
  #'
  #' \code{gs_design_nph()} is a flexible routine to provide a sample size or power for a fixed or
  #' group sequential design under
  #' various non-proportional hazards assumptions for either single or multiple strata studies.
  #' The piecewise exponential distribution allows a simple method to specify a distribtuion
  #' and enrollment pattern
  #' where the enrollment, failure and dropout rates changes over time.
  #' @param enrollRates Piecewise constant enrollment rates by stratum and time period.
  #' @param failRates Piecewise constant control group failure rates, duration for each piecewise constant period,
  #' hazard ratio for experimental vs control, and dropout rates by stratum and time period.
  #' @param ratio Experimental:Control randomization ratio
  #' @param alpha One-sided Type I error
  #' @param beta Type II error
  #' @param analysisTimes Final calendar time if beta is not NULL
  #' @param IF information fraction at planned analyses
  #' @param upper Function to produce upper bound
  #' @param lower Function to produce lower bound
  #' @param upar Parameters to pass to upper
  #' @param lpar Parameters to pass to lower
  #' @param r  Control for grid size; normally leave at default of \code{r=18}
  #'
  #' @return A list with 3 tibbles: 1) \code{enrollRates} with \code{enrollRates$rate} adjusted by sample size calculation and adding \code{N} with
  #' cumulative enrollment at end of each enrollment rate period,
  #' 2) \code{failRates} as input,
  #' 3) code{bounds} with a row for each bound and each analysis; rows contain the variables \code{Analysis} with analysis number, \code{Z} with Z-value bound,
  #' \code{Probability} with cumulative probability of crossing bound at each analysis under the alternate hypothesis input,
  #' \code{theta} standardized effect size at each analysis, \code{info} cumulative statistical information for \code{theta} at each analysis,
  #' \code{Time} expected timing of analysis,
  #' \code{avehr} expected average hazard ratio at time of analysis,
  #' \code{Events} expected events an time of analysis under alternate hypothesis,
  #' \code{info0} information under null hypothesis with same expected total events under alternate hypothesis, and
  #' \code{N} expected enrollment at time of analysis.
  #' @examples
  #' library(dplyr)
  #' 
  #' # Design
  #' library(dplyr)
  #' 
  #' x <- gs_design_nph()
  #' # Notice cumulative power is 0.9 (90%)
  #' x
  #' # Check Type I error, non-binding; should be .025 (2.5%)
  #' gs_power_nph(enrollRates = x$enrollRates,
  #'            failRates = x$failRates %>% mutate(hr = 1),
  #'            events = (x$bounds %>% filter(Bound == "Upper"))$Events,
  #'            upar = (x$bounds %>% filter(Bound == "Upper"))$Z,
  #'            lpar = rep(-Inf,3),
  #'            upper = gs_b,
  #'            lower = gs_b
  #'            ) %>% filter(abs(Z) < Inf)
  #' # Power under proportional hazards, HR = 0.75
  #' gs_power_nph(enrollRates = x$enrollRates,
  #'            failRates = x$failRates %>% mutate(hr = .75),
  #'            events = (x$bounds %>% filter(Bound == "Upper"))$Events,
  #'            upar = (x$bounds %>% filter(Bound == "Upper"))$Z,
  #'            lpar = rep(-Inf,3),
  #'            upper = gs_b,
  #'            lower = gs_b
  #'            ) %>% filter(abs(Z) < Inf)
  #'
  #' @export
  #'
  gs_design_nph <- function(enrollRates=tibble::tibble(Stratum="All",
                                                     duration=c(2,2,10),
                                                     rate=c(3,6,9)),
                          failRates=tibble::tibble(Stratum="All",
                                                   duration=c(3,100),
                                                   failRate=log(2)/c(9,18),
                                                   hr=c(.9,.6),
                                                   dropoutRate=rep(.001,2)),
                          ratio = 1, # NOT YET IMPLEMENTED
                          alpha = 0.025,       # One-sided Type I error
                          beta = 0.1,          # NULL if enrollment is not adapted
                          analysisTimes = 30,  # CURRENTLY GIVES ONLY FINAL CALENDAR TIME OF TRIAL
                          IF = c(.25, .75, 1), # relative information fraction timing (vector, if not NULL)
                          upper = gs_b,
                          # Default is Lan-DeMets approximation of
                          upar = gsDesign::gsDesign(k=3, test.type=1, n.I=c(.25, .75, 1), maxn.IPlan = 1,
                                                    sfu=sfLDOF, sfupar = NULL)$upper$bound,
                          lower = gs_b,
                          lpar = c(qnorm(.1), rep(-Inf, length(IF) - 1)),
                          r = 18
  ){
    errbeta <- function(x = 1, info, theta, Zupper, Zlower, upar, lpar, beta){
       1 -  beta - max((gs_prob(theta = theta, upper = Zupper, lower = Zlower, upar = upar, lpar = lpar,
                               info = x * info, r = r) %>% filter(Bound == "Upper"))$Probability)
    }
    avehr <- gsDesign2::AHR(enrollRates = enrollRates,
                            failRates = failRates,
                            totalDuration = analysisTimes,
                            ratio = ratio)
    K <- max(length(IF), length(analysisTimes))
    finalEvents <- max(avehr$Events)
    if (is.null(IF)){
      avehr$IF <- avehr$Events/finalEvents
    }else{
      avehr <- NULL
      for(i in seq_along(IF)){
        avehr <- rbind(avehr,
                       gsDesign2::tEvents(enrollRates = enrollRates,
                                          failRates = failRates,
                                          ratio = ratio,
                                          targetEvents = IF[i] * finalEvents)
                 )
      }
      avehr$IF <- IF
    }
    targ <- (qnorm(alpha) + qnorm(beta))^2 / log(avehr$AHR[nrow(avehr)])^2 * (1 + ratio)^2 / ratio
    interval <- c(.9, 1.5) * targ / finalEvents
  
  # Now we can solve for the inflation factor for the enrollment rate to achieve the desired power
    res <- try(
      uniroot(errbeta,
              interval = interval,
              theta = -log(avehr$AHR),
              Zupper = upper,
              Zlower = lower,
              upar = upar,
              lpar = lpar,
              info = avehr$info,
              beta = beta,
              tol = .0001
      )
    )
    if(inherits(res,"try-error")){stop("gs_design_nph(): Sample size solution not found")}
    enrollRates = enrollRates %>% mutate(rate = rate * res$root)
    avehr <- gsDesign2::AHR(enrollRates,
                            failRates = failRates,
                            totalDuration = avehr$Time) %>%
             mutate(Analysis = 1:n())
    N <- cumsum(enrollRates$rate * enrollRates$duration) %>% tail(1)
    # Once eAccrual is fixed, should not need pmin in the following
    avehr$N <- pmin(N, gsDesign2::eAccrual(avehr$Time, enrollRates))
    bounds <-
      gs_prob(theta = -log(avehr$AHR),
              upper = upper,
              lower = lower,
              upar = upar,
              lpar = lpar,
              info = avehr$info
      )
    return(list(enrollRates=enrollRates %>% mutate(N=cumsum(duration * rate)),
                failRates=failRates,
                bounds = bounds %>% left_join(avehr %>% select(-info), by="Analysis"))
    )
  }

Package: gsdmvn
File: R/gs_design_wlr.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Group sequential design using weighted log-rank test under non-proportional hazards
  #'
  #' @inheritParams gs_design_ahr
  #' @inheritParams gs_info_wlr
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if input analysisTimes is a positive number or a positive increasing sequence.
  #'    \item Validate if input IF is a positive number or positive increasing sequence on (0, 1] with final value of 1.
  #'    \item Validate if inputs IF and analysisTimes  have the same length if both have length > 1.
  #'    \item Compute information at input analysisTimes using \code{gs_info_wlr()}.
  #'    \item Compute sample size and bounds using \code{gs_design_npe()}.
  #'    \item Return a list of design enrollment, failure rates, and bounds.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @examples
  #' library(dplyr)
  #' library(mvtnorm)
  #' library(gsDesign)
  #' 
  #' enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500/12)
  #'
  #' failRates <- tibble::tibble(Stratum = "All",
  #'                             duration = c(4, 100),
  #'                             failRate = log(2) / 15,  # median survival 15 month
  #'                             hr = c(1, .6),
  #'                             dropoutRate = 0.001)
  #'
  #'
  #' x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,
  #'                        beta = 0.2 , astar = 0 , timing = c( 1 ) ,
  #'                        sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,
  #'                        sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,
  #'                        hr = 0.6 , hr0 = 1 , eta = 0.01 ,
  #'                        gamma = c( 10 ) ,
  #'                        R = c( 12 ) , S = NULL ,
  #'                        T = 36 , minfup = 24 , ratio = 1 )
  #'
  #' # User defined boundary
  #' gs_design_wlr(enrollRates = enrollRates, failRates = failRates,
  #'              ratio = 1, alpha = 0.025, beta = 0.2,
  #'              weight = function(x, arm0, arm1){
  #'                  gsdmvn:::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)
  #'              },
  #'              upar = x$upper$bound,
  #'              lpar = x$lower$bound,
  #'              analysisTimes = c(12, 24, 36))
  #'
  #' # Boundary derived by spending function
  #' gs_design_wlr(enrollRates = enrollRates, failRates = failRates,
  #'              ratio = 1, alpha = 0.025, beta = 0.2,
  #'              weight = function(x, arm0, arm1){
  #'                   gsdmvn:::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)
  #'              },
  #'              upper = gs_spending_bound,
  #'              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
  #'              lower = gs_spending_bound,
  #'              lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2),
  #'              analysisTimes = c(12, 24, 36))
  #' @export
  gs_design_wlr <- function(enrollRates=tibble::tibble(Stratum="All",
                                                       duration=c(2,2,10),
                                                       rate=c(3,6,9)),
                            failRates=tibble::tibble(Stratum="All",
                                                     duration=c(3,100),
                                                     failRate=log(2)/c(9,18),
                                                     hr=c(.9,.6),
                                                     dropoutRate=rep(.001,2)),
                            ratio=1,               # Experimental:Control randomization ratio
                            weight = wlr_weight_fh,
                            approx = "asymptotic",
                            alpha = 0.025,         # One-sided Type I error
                            beta = 0.1,            # NULL if enrollment is not adapted
                            IF = NULL,             # relative information fraction timing (vector, if not NULL; increasing to 1)
                            analysisTimes = 36,    # Targeted times of analysis or just planned study duration
                            binding = FALSE,
                            upper = gs_b,
                            # Default is Lan-DeMets approximation of
                            upar = gsDesign(k=3, test.type=1,
                                            n.I=c(.25, .75, 1),
                                            sfu=sfLDOF, sfupar = NULL)$upper$bound,
                            lower = gs_b,
                            lpar = c(qnorm(.1), -Inf, -Inf), # Futility only at IA1
                            h1_spending = TRUE,
                            test_upper = TRUE,
                            test_lower = TRUE,
                            r = 18,
                            tol = 1e-6
  ){
    ################################################################################
    # Check input values
    msg <- "gs_design_wlr(): analysisTimes must be a positive number or positive increasing sequence"
    if (!is.vector(analysisTimes,mode = "numeric")) stop(msg)
    if (min(analysisTimes - dplyr::lag(analysisTimes, def=0))<=0) stop(msg)
    msg <- "gs_design_wlr(): IF must be a positive number or positive increasing sequence on (0, 1] with final value of 1"
    if (is.null(IF)){IF <- 1}
    if (!is.vector(IF,mode = "numeric")) stop(msg)
    if (min(IF - dplyr::lag(IF, def=0))<=0) stop(msg)
    if (max(IF) != 1) stop(msg)
    msg <- "gs_design_wlr(): IF and analysisTimes must have the same length if both have length > 1"
    if ((length(analysisTimes)>1) & (length(IF) > 1) & (length(IF) != length(analysisTimes))) stop(msg)
    # end check input values
    ################################################################################
    # Get information at input analysisTimes
    y <- gs_info_wlr(enrollRates, failRates, ratio = ratio, events = NULL, analysisTimes=analysisTimes,
                     weight = weight, approx = approx)
    finalEvents <- y$Events[nrow(y)]
    IFalt <- y$Events / finalEvents
    # Check if IF needed for (any) IA timing
    K <- max(length(analysisTimes), length(IF))
    nextTime <- max(analysisTimes)
    if(length(IF)==1){IF <- IFalt}else{
      IFindx <- IF[1:(K-1)]
      for(i in seq_along(IFindx)){
        if(length(IFalt) == 1){y <-
          rbind(tEvents(enrollRates, failRates, targetEvents = IF[K - i] * finalEvents, ratio = ratio,
                        interval = c(.01, nextTime)) %>% mutate(theta=-log(AHR), Analysis=K-i),
                y)
        }else if (IF[K-i] > IFalt[K-i]) y[K - i,] <-
            tEvents(enrollRates, failRates, targetEvents = IF[K - i] * finalEvents, ratio = ratio,
                    interval = c(.01, nextTime)) %>%
            dplyr::transmute(Analysis = K - i, Time, Events, AHR, theta=-log(AHR), info, info0)
        nextTime <- y$Time[K - i]
      }
    }
    y$Analysis <- 1:K
    y$N <- eAccrual(x = y$Time, enrollRates = enrollRates)
    if(h1_spending){
      theta1 <- y$theta
      info1 <- y$info
    }else{
      theta1 <- 0
      info1 <- y$info0
    }
  
    # Get sample size and bounds using gs_design_npe
    bounds <- gs_design_npe(theta = y$theta,
                            theta1 = theta1,
                            info = y$info,
                            info0 = y$info0,
                            info1 = info1,
                            alpha = alpha,
                            beta = beta,
                            binding = binding,
                            upper = upper,
                            lower = lower,
                            upar = upar,
                            lpar = lpar,
                            test_upper = test_upper,
                            test_lower = test_lower,
                            r = r,
                            tol = tol) %>%
      # Add Time, Events, AHR, N from gs_info_ahr call above
      full_join(y %>% select(-c(info,info0,theta)), by = "Analysis") %>%
      select(c("Analysis", "Bound", "Time", "N", "Events", "Z", "Probability", "AHR", "theta", "info", "info0")) %>%
      arrange(desc(Bound), Analysis)
    bounds$Events <- bounds$Events * bounds$info[K] / y$info[K]
    bounds$N <- bounds$N * bounds$info[K] / y$info[K]
  
    # Document design enrollment, failure rates, and bounds
    return(list(enrollRates = enrollRates %>%
                  mutate(rate = rate * bounds$info[K] / y$info[K]),
                failRates = failRates,
                bounds = bounds)
    )
  }

Package: gsdmvn
File: R/gs_info_ahr.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom dplyr lag
  NULL
  #' Information and effect size based on AHR approximation
  #'
  #' Based on piecewise enrollment rate, failure rate, and dropout rates computes
  #' approximate information and effect size using an average hazard ratio model.
  #' @param enrollRates enrollment rates
  #' @param failRates failure and dropout rates
  #' @param ratio Experimental:Control randomization ratio
  #' @param events Targeted minimum events at each analysis
  #' @param analysisTimes Targeted minimum study duration at each analysis
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if input events is a numeric value vector or a vector with increasing values.
  #'    \item Validate if input analysisTime is a numeric value vector or a vector with increasing values.
  #'    \item Validate if inputs events and analysisTime have the same length if they are both specified.
  #'    \item Compute average hazard ratio:
  #'    \itemize{
  #'      \item If analysisTime is specified, calculate average hazard ratio using \code{gsDesign2::AHR()}.
  #'      \item If events is specified, calculate average hazard ratio using \code{gsDesign2::tEvents()}.
  #'    }
  #'    \item Return a tibble of Analysis, Time, AHR, Events, theta, info, info0.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return a \code{tibble} with columns \code{Analysis, Time, AHR, Events, theta, info, info0.}
  #' \code{info, info0} contains statistical information under H1, H0, respectively.
  #' For analysis \code{k}, \code{Time[k]} is the maximum of \code{analysisTimes[k]} and the expected time
  #' required to accrue the targeted \code{events[k]}.
  #' \code{AHR} is expected average hazard ratio at each analysis.
  #' @details The \code{AHR()} function computes statistical information at targeted event times.
  #' The \code{tEvents()} function is used to get events and average HR at targeted \code{analysisTimes}.
  #' @export
  #'
  #' @examples
  #' library(gsDesign)
  #' library(gsDesign2)
  #' 
  #' # Only put in targeted events
  #' gs_info_ahr(events = c(30, 40, 50))
  #' # Only put in targeted analysis times
  #' gs_info_ahr(analysisTimes = c(18, 27, 36))
  #' # Some analysis times after time at which targeted events accrue
  #' # Check that both Time >= input analysisTime and Events >= input events
  #' gs_info_ahr(events = c(30, 40, 50), analysisTimes = c(16, 19, 26))
  #' gs_info_ahr(events = c(30, 40, 50), analysisTimes = c(14, 20, 24))
  gs_info_ahr <- function(enrollRates=tibble::tibble(Stratum="All",
                                                    duration=c(2,2,10),
                                                    rate=c(3,6,9)),
                         failRates=tibble::tibble(Stratum="All",
                                                  duration=c(3,100),
                                                  failRate=log(2)/c(9,18),
                                                  hr=c(.9,.6),
                                                  dropoutRate=rep(.001,2)),
                         ratio=1,               # Experimental:Control randomization ratio
                         events = NULL,         # Events at analyses
                         analysisTimes = NULL   # Times of analyses
  ){
    ################################################################################
    # Check input values
    K <- 0
    if (is.null(analysisTimes) && is.null(events)) stop("gs_info_ahr(): One of
                                                        events and analysisTimes must be a
                                                        numeric value or vector with increasing values")
    if (!is.null(analysisTimes)){
      if (!is.numeric(analysisTimes) || !is.vector(analysisTimes) || min(analysisTimes - dplyr::lag(analysisTimes, def=0))<=0
         )stop("gs_info_ahr(): analysisTimes must be NULL a numeric vector with positive increasing values")
      K <- length(analysisTimes)
    }
    if (!is.null(events)){
      if (!is.numeric(events) || !is.vector(events) || min(events - dplyr::lag(events, default=0))<=0
      )stop("gs_info_ahr(): events must be NULL or a numeric vector with positive increasing values")
      if(K==0){
        K <- length(events)
      }else if (K != length(events)) stop("gs_info_ahr(): If both events and analysisTimes specified, must have same length")
    }
    # end check input values
    ################################################################################
    avehr <- NULL
    if(!is.null(analysisTimes)){
      avehr <- gsDesign2::AHR(enrollRates = enrollRates, failRates = failRates, ratio = ratio,
                              totalDuration = analysisTimes)
      for(i in seq_along(events)){
        if (avehr$Events[i] < events[i]){
          avehr[i,] <- gsDesign2::tEvents(enrollRates = enrollRates, failRates = failRates, ratio = ratio,
                                   targetEvents = events[i])
        }
      }
    }else{
       for(i in seq_along(events)){
          avehr <- rbind(avehr,
                     gsDesign2::tEvents(enrollRates = enrollRates, failRates = failRates, ratio = ratio,
                                        targetEvents = events[i]))
       }
    }
    avehr$Analysis <- 1:nrow(avehr)
    avehr$theta = -log(avehr$AHR)
    return(avehr %>% dplyr::transmute(Analysis, Time, Events, AHR, theta, info, info0))
  }

Package: gsdmvn
File: R/gs_info_combo.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @noRd
  get_combo_weight <- function(rho, gamma, tau){
  
    stopifnot(length(rho) == length(gamma))
    stopifnot(length(rho) == length(tau))
  
    weight <- list()
    for(i in 1:length(rho)){
  
      if(tau[i] == -1){
        tmp_tau <- NULL
      }else{
        tmp_tau <- tau[i]
      }
  
      text <- paste0("weight <- function(x, arm0, arm1){
                              wlr_weight_fh(x, arm0, arm1
                                  ,rho =", rho[i],
                     ", gamma =", gamma[i],
                     ", tau =", tmp_tau,  ")}")
  
      weight[[i]] <- text
  
    }
  
    weight
  }
  
  #' @noRd
  gs_delta_combo <- function(arm0,
                             arm1,
                             tmax = NULL,
                             rho,
                             gamma,
                             tau = rep(-1, length(rho)),
                             approx="asymptotic",
                             normalization = FALSE) {
  
    stopifnot(length(tmax) == 1)
  
    weight <- get_combo_weight(rho, gamma, tau)
    delta <- sapply(weight, function(x){
      x <- eval(parse(text = x))
      gs_delta_wlr(arm0, arm1, tmax = tmax, weight = x,
                   approx = approx, normalization = normalization)
    })
  
    delta
  
  }
  
  #' @noRd
  gs_sigma2_combo <- function(arm0,
                              arm1,
                              tmax = NULL,
                              rho,
                              gamma,
                              tau = rep(-1, length(rho)),
                              approx="asymptotic"){
  
    stopifnot(length(tmax) == 1)
    stopifnot(length(rho) == length(gamma))
    stopifnot(length(rho) == length(tau))
  
    rho1   <- outer(rho, rho, function(x,y) (x+y)/2 )
    gamma1 <- outer(gamma, gamma, function(x,y) (x+y)/2 )
  
    sigma2 <- rho1
    for(i in 1:length(rho)){
  
      weight <- get_combo_weight(rho1[i,], gamma1[i,],tau)
  
      sigma2[i,] <- sapply(weight, function(x){
                      x <- eval(parse(text = x))
                      gs_sigma2_wlr(arm0, arm1, tmax = tmax, weight = x,
                                    approx = approx)
  
                    })
    }
  
    sigma2
  
  }
  
  #' @noRd
  gs_info_combo <- function(enrollRates=tibble::tibble(Stratum="All",
                                                       duration=c(2,2,10),
                                                       rate=c(3,6,9)),
                            failRates=tibble::tibble(Stratum="All",
                                                     duration=c(3,100),
                                                     failRate=log(2)/c(9,18),
                                                     hr=c(.9,.6),
                                                     dropoutRate=rep(.001,2)),
                            ratio=1,                # Experimental:Control randomization ratio
                            events = NULL, # Events at analyses
                            analysisTimes = NULL,   # Times of analyses
                            rho,
                            gamma,
                            tau =  rep(-1, length(rho)),
                            approx = "asymptotic"
  ){
  
    weight <- get_combo_weight(rho, gamma, tau)
  
    info <- lapply(weight, function(x){
      x <- eval(parse(text = x))
      gs_info_wlr(enrollRates, failRates, ratio, events = events, analysisTimes = analysisTimes, weight = x)
    })
  
    info <- dplyr::bind_rows(info, .id = "test")
    info$test <- as.numeric(info$test)
    info
  }

Package: gsdmvn
File: R/gs_info_wlr.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  ###
  # Weighted log-rank test
  ###
  
  # For a subject in the provided arm, calculate the probability he or
  # she is observed to be at risk at time=teval after enrollment.
  prob_risk <- function(arm, teval, tmax) {
    if(is.null(tmax)){
      tmax <- arm$total_time
    }
  
    npsurvSS::psurv(teval, arm, lower.tail=F) *
      npsurvSS::ploss(teval, arm, lower.tail=F) *
      npsurvSS::paccr(pmin(arm$accr_time, tmax-teval), arm)
  }
  
  # For a subject in the provided arm, calculate the density of event
  # at time=teval after enrollment.
  dens_event <- function(arm, teval, tmax = NULL) {
  
    if(is.null(tmax)){
      tmax <- arm$total_time
    }
  
    npsurvSS::dsurv(teval, arm) *
      npsurvSS::ploss(teval, arm, lower.tail=F) *
      npsurvSS::paccr(pmin(arm$accr_time, tmax - teval), arm)
  }
  
  # For a subject in the provided arm, calculate the probability he or
  # she is observed to have experienced an event by time=teval after enrollment.
  prob_event <- function(arm, tmin=0, tmax=arm$total_time) {
    UseMethod("prob_event", arm)
  }
  
  # prob_event for arm of class "arm"
  prob_event.arm <- function(arm, tmin=0, tmax=arm$total_time) {
    l = length(tmax)
    if (l==1) {
      return(stats::integrate(function(x) dens_event(arm, x, tmax = tmax), lower=tmin, upper=tmax)$value)
    } else {
      if (length(tmin)==1) {
        tmin = rep(tmin, l)
      }
      return(sapply(seq(l), function(i) prob_event(arm, tmin[i], tmax[i])))
    }
  }
  
  gs_delta_wlr <- function(arm0,
                           arm1,
                           tmax = NULL,
                           weight= wlr_weight_fh,
                           approx="asymptotic",
                           normalization = FALSE) {
  
    if(is.null(tmax)){
      tmax <- arm0$total_time
    }
  
    p1 <- arm1$size / (arm0$size + arm1$size)
    p0 <- 1 - p1
  
    if (approx == "event driven") {
  
      if (sum(arm0$surv_shape != arm1$surv_shape) > 0 |
          length( unique(arm1$surv_scale / arm0$surv_scale) ) > 1) {
  
        stop("gs_delta_wlr(): Hazard is not proportional over time.", call.=F)
  
      } else if (wlr_weight_fh(seq(0,tmax,length.out = 10), arm0, arm1) != "1") {
  
        stop("gs_delta_wlr(): Weight must equal `1`.", call.=F)
      }
  
      theta <- c(arm0$surv_shape * log( arm1$surv_scale / arm0$surv_scale ))[1] # log hazard ratio
      nu    <- p0 * prob_event(arm0, tmax = tmax) + p1 * prob_event(arm1, tmax = tmax) # probability of event
      delta <- theta * p0 * p1 * nu
  
    } else if (approx == "asymptotic") {
  
      delta <- stats::integrate(function(x){
  
        term0 <- p0 * prob_risk(arm0, x, tmax)
        term1 <- p1 * prob_risk(arm1, x, tmax)
        term  <- (term0 * term1) / (term0 + term1)
        term  <- ifelse(is.na(term), 0, term)
        weight(x, arm0, arm1) *  term * ( npsurvSS::hsurv(x, arm1) - npsurvSS::hsurv(x, arm0) )},
        lower=0,
        upper= tmax, rel.tol = 1e-10)$value
  
  
    } else if (approx == "generalized schoenfeld") {
  
      delta <- stats::integrate(function(x){
  
        if(normalization){
          log_hr_ratio <- 1
        }else{
          log_hr_ratio <- log( npsurvSS::hsurv(x, arm1) / npsurvSS::hsurv(x, arm0) )
        }
  
        weight(x, arm0, arm1) *
          log_hr_ratio *
          p0 * prob_risk(arm0, x, tmax) * p1 * prob_risk(arm1, x, tmax) /
          ( p0 * prob_risk(arm0, x, tmax) + p1 * prob_risk(arm1, x, tmax) )^2 *
          ( p0 * dens_event(arm0, x, tmax) + p1 * dens_event(arm1, x, tmax))},
        lower=0,
        upper= tmax)$valu
    } else {
  
      stop("gs_delta_wlr(): Please specify a valid approximation for the mean.", call.=F)
  
    }
  
    return(delta)
  
  }
  
  
  gs_sigma2_wlr <- function(arm0,
                            arm1,
                            tmax = NULL,
                            weight= wlr_weight_fh,
                            approx="asymptotic") {
  
    if(is.null(tmax)){
      tmax <- arm0$total_time
    }
  
    p1 <- arm1$size / (arm0$size + arm1$size)
    p0 <- 1 - p1
  
    if (approx == "event driven") {
  
      nu      <- p0 * prob_event(arm0, tmax = tmax) + p1 * prob_event(arm1, tmax = tmax)
      sigma2  <- p0 * p1 * nu
  
    } else if (approx %in% c("asymptotic", "generalized schoenfeld")) {
  
      sigma2  <- stats::integrate(function(x) weight(x, arm0, arm1)^2 *
                                    p0 * prob_risk(arm0, x, tmax) * p1 * prob_risk(arm1, x, tmax) /
                                    ( p0 * prob_risk(arm0, x, tmax) + p1 * prob_risk(arm1, x, tmax) )^2 *
                                    ( p0 * dens_event(arm0, x, tmax) + p1 * dens_event(arm1, x, tmax)),
                                  lower=0,
                                  upper= tmax)$value
  
    } else {
      stop("gs_sigma2_wlr(): Please specify a valid approximation for the mean.", call.=F)
    }
  
    return(sigma2)
  
  }
  
  #' Information and effect size for Weighted Log-rank test
  #'
  #' Based on piecewise enrollment rate, failure rate, and dropout rates computes
  #' approximate information and effect size using an average hazard ratio model.
  #' @param enrollRates enrollment rates
  #' @param failRates failure and dropout rates
  #' @param ratio Experimental:Control randomization ratio
  #' @param events Targeted minimum events at each analysis
  #' @param analysisTimes Targeted minimum study duration at each analysis
  #' @param weight weight of weighted log rank test
  #'               * "1"=unweighted,
  #'               * "n"=Gehan-Breslow,
  #'               * "sqrtN"=Tarone-Ware,
  #'               * "FH_p[a]_q[b]"= Fleming-Harrington with p=a and q=b
  #' @param approx approximate estimation method for Z statistics
  #'               * "event driven" = only work under propotional hazard model with log rank test
  #'               * "asymptotic"
  #'
  #' @return a \code{tibble} with columns \code{Analysis, Time, N, Events, AHR, delta, sigma2, theta, info, info0.}
  #' \code{info, info0} contains statistical information under H1, H0, respectively.
  #' For analysis \code{k}, \code{Time[k]} is the maximum of \code{analysisTimes[k]} and the expected time
  #' required to accrue the targeted \code{events[k]}.
  #' \code{AHR} is expected average hazard ratio at each analysis.
  #' @details The \code{AHR()} function computes statistical information at targeted event times.
  #' The \code{tEvents()} function is used to get events and average HR at targeted \code{analysisTimes}.
  #' @export
  gs_info_wlr <- function(enrollRates=tibble::tibble(Stratum="All",
                                                     duration=c(2,2,10),
                                                     rate=c(3,6,9)),
                          failRates=tibble::tibble(Stratum="All",
                                                   duration=c(3,100),
                                                   failRate=log(2)/c(9,18),
                                                   hr=c(.9,.6),
                                                   dropoutRate=rep(.001,2)),
                          ratio=1,                # Experimental:Control randomization ratio
                          events = NULL, # Events at analyses
                          analysisTimes = NULL,   # Times of analyses
                          weight = wlr_weight_fh,
                          approx = "asymptotic"
  ){
  
    if (is.null(analysisTimes) && is.null(events)){
      stop("gs_info_wlr(): One of events and analysisTimes must be a numeric value or vector with increasing values")
    }
  
    # Obtain Analysis time
    avehr <- NULL
    if(!is.null(analysisTimes)){
      avehr <- gsDesign2::AHR(enrollRates = enrollRates, failRates = failRates, ratio = ratio,
                              totalDuration = analysisTimes)
      for(i in seq_along(events)){
        if (avehr$Events[i] < events[i]){
          avehr[i,] <- gsDesign2::tEvents(enrollRates = enrollRates, failRates = failRates, ratio = ratio,
                                          targetEvents = events[i])
        }
      }
    }else{
      for(i in seq_along(events)){
        avehr <- rbind(avehr,
                       gsDesign2::tEvents(enrollRates = enrollRates, failRates = failRates, ratio = ratio,
                                          targetEvents = events[i]))
      }
    }
  
    time <- avehr$Time
  
    # Create Arm object
    gs_arm <- gs_create_arm(enrollRates, failRates, ratio)
  
    arm0 <- gs_arm$arm0
    arm1 <- gs_arm$arm1
  
    arm_null <- arm0
    arm_null$surv_scale <- (arm0$surv_scale + arm1$surv_scale)/2
  
    # Randomization ratio
    p0 <- arm0$size/(arm0$size + arm1$size)
    p1 <- 1 - p0
  
    # Group sequential sample size ratio
    n_ratio <- (npsurvSS::paccr(time, arm0) + npsurvSS::paccr(time, arm1))/2
  
    delta <- c()     # delta of effect size in each analysis
    sigma2_h1 <- c()    # sigma square of effect size in each analysis under null
    sigma2_h0 <- c()    # sigma square of effect size in each analysis under alternative
    p_event <- c()   # probability of events in each analysis
    p_subject <- c() # probability of subjects enrolled
    log_ahr <- c()
    for(i in seq_along(time)){
      t <- time[i]
      p_event[i]      <- p0 * prob_event.arm(arm0, tmax = t) + p1 * prob_event.arm(arm1, tmax = t)
      p_subject[i]    <- p0 * npsurvSS::paccr(t, arm0) + p1 * npsurvSS::paccr(t, arm1)
      delta[i]        <- gs_delta_wlr(arm0, arm1, tmax = t, weight = weight, approx = approx)
      log_ahr[i]          <- delta[i] / gs_delta_wlr(arm0, arm1, tmax = t, weight = weight,
                                                     approx = "generalized schoenfeld", normalization = TRUE)
      sigma2_h1[i]    <- gs_sigma2_wlr(arm0, arm1, tmax = t, weight = weight, approx = approx)
      sigma2_h0[i]    <- gs_sigma2_wlr(arm_null, arm_null, tmax = t, weight = weight, approx = approx)
    }
  
    N <- tail(avehr$Events / p_event,1) * p_subject
    theta <- (- delta) / sigma2_h1
    data.frame(Analysis = 1:length(time),
               Time = time,
               N = N,
               Events = avehr$Events,
               AHR = exp(log_ahr),
               delta = delta,
               sigma2 = sigma2_h1,
               theta = theta,
               info =  sigma2_h1 * N,
               info0 = sigma2_h0 * N)
  
  }

Package: gsdmvn
File: R/gs_power_ahr.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom gsDesign gsDesign sfLDOF
  #' @importFrom stats qnorm
  #' @importFrom dplyr select arrange desc right_join
  NULL
  #' Group sequential design power using average hazard ratio under non-proportional hazards
  #'
  #' @param enrollRates enrollment rates
  #' @param failRates failure and dropout rates
  #' @param ratio Experimental:Control randomization ratio (not yet implemented)
  #' @param events Targeted events at each analysis
  #' @param analysisTimes Minimum time of analysis
  #' @param binding indicator of whether futility bound is binding; default of FALSE is recommended
  #' @param upper Function to compute upper bound
  #' @param upar Parameter passed to \code{upper()}
  #' @param lower Function to compute lower bound
  #' @param lpar Parameter passed to \code{lower()}
  #' @param test_upper indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  #' otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound
  #' @param test_lower indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  #' single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  #' lower bound
  #' @param r  Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param tol Tolerance parameter for boundary convergence (on Z-scale)
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Calculate information and effect size based on AHR approximation using \code{gs_info_ahr()}.
  #'    \item Return a tibble of with columns Analysis, Bound, Z, Probability, theta,
  #'     Time, AHR, Events and  contains a row for each analysis and each bound.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return a \code{tibble} with columns \code{Analysis, Bound, Z, Probability, theta, Time, AHR, Events}.
  #' Contains a row for each analysis and each bound.
  #' @details
  #' Bound satisfy input upper bound specification in \code{upper, upar} and lower bound specification in \code{lower, lpar}.
  #' The \code{AHR()} function computes statistical information at targeted event times.
  #' The \code{tEvents()} function is used to get events and average HR at targeted \code{analysisTimes}.
  #'
  #' @export
  #'
  #' @examples
  #' library(gsDesign2)
  #' library(dplyr)
  #' 
  #' gs_power_ahr() %>% filter(abs(Z) < Inf)
  #'
  #' # 2-sided symmetric O'Brien-Fleming spending bound
  #' # NOT CURRENTLY WORKING
  #' gs_power_ahr(analysisTimes = c(12, 24, 36),
  #'               binding = TRUE,
  #'               upper = gs_spending_bound,
  #'               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
  #'               lower = gs_spending_bound,
  #'               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL))
  #'
  gs_power_ahr <- function(enrollRates=tibble::tibble(Stratum="All",
                                                      duration=c(2,2,10),
                                                      rate=c(3,6,9)),
                           failRates=tibble::tibble(Stratum="All",
                                                    duration=c(3,100),
                                                    failRate=log(2)/c(9,18),
                                                    hr=c(.9,.6),
                                                    dropoutRate=rep(.001,2)),
                           ratio=1,                 # Experimental:Control randomization ratio
                           events = c(30, 40, 50),  # Targeted events of analysis
                           analysisTimes = NULL,    # Targeted times of analysis
                           binding = FALSE,
                           upper = gs_b,
                           # Default is Lan-DeMets approximation of
                           upar = gsDesign(k=length(events), test.type=1,
                                           n.I=events, maxn.IPlan = max(events),
                                           sfu=sfLDOF, sfupar = NULL)$upper$bound,
                           lower = gs_b,
                           lpar = c(qnorm(.1), rep(-Inf, length(events) - 1)), # Futility only at IA1
                           test_upper = TRUE,
                           test_lower = TRUE,
                           r = 18,
                           tol = 1e-6
  ){
    x <- gs_info_ahr(enrollRates = enrollRates,
                    failRates = failRates,
                    ratio = ratio,
                    events = events,
                    analysisTimes = analysisTimes
                   )
    return(gs_power_npe(theta = x$theta, info = x$info, info0 = x$info0, binding = binding,
                        upper=upper, lower=lower, upar = upar, lpar= lpar,
                        test_upper = test_upper, test_lower = test_lower,
                        r = r, tol = tol) %>%
             right_join(x %>% select(-c(info, info0, theta)), by = "Analysis") %>%
             select(c(Analysis, Bound, Time, Events, Z, Probability, AHR, theta, info, info0)) %>%
             arrange(desc(Bound), Analysis)
    )
  }

Package: gsdmvn
File: R/gs_power_combo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Group sequential design power using MaxCombo test under non-proportional hazards
  #'
  #' @inheritParams gs_design_combo
  #' @inheritParams pmvnorm_combo
  #'
  #' @examples
  #' library(dplyr)
  #' library(mvtnorm)
  #' library(gsDesign)
  #' 
  #' enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500/12)
  #'
  #' failRates <- tibble::tibble(Stratum = "All",
  #'                             duration = c(4, 100),
  #'                             failRate = log(2) / 15,  # median survival 15 month
  #'                             hr = c(1, .6),
  #'                             dropoutRate = 0.001)
  #'
  #' fh_test <- rbind( data.frame(rho = 0, gamma = 0, tau = -1,
  #'                              test = 1,
  #'                              Analysis = 1:3,
  #'                              analysisTimes = c(12, 24, 36)),
  #'                   data.frame(rho = c(0, 0.5), gamma = 0.5, tau = -1,
  #'                              test = 2:3,
  #'                              Analysis = 3, analysisTimes = 36)
  #' )
  #'
  #' # User defined bound
  #' gs_power_combo(enrollRates, failRates, fh_test, upar = c(3,2,1), lpar = c(-1, 0, 1))
  #'
  #' # Minimal Information Fraction derived bound
  #' gs_power_combo(enrollRates, failRates, fh_test,
  #'                upper = gs_spending_combo,
  #'                upar  = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
  #'                lower = gs_spending_combo,
  #'                lpar  = list(sf = gsDesign::sfLDOF, total_spend = 0.2))
  #'
  #' @importFrom mvtnorm GenzBretz
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Validate if lower and upper bounds have been specified.
  #'    \item Extract info, info_fh, theta_fh and corr_fh from utility.
  #'    \item Extract sample size via the maximum sample size of info.
  #'    \item Calculate information fraction either for fixed or group sequential design.
  #'    \item Compute spending function using \code{gs_bound()}.
  #'    \item Compute probability of crossing bounds under the null and alternative
  #'     hypotheses using \code{gs_prob_combo()}.
  #'    \item Export required information for boundary and crossing probability
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @export
  gs_power_combo <- function(enrollRates,
                             failRates,
                             fh_test,
                             ratio = 1,
                             binding = FALSE,
                             upper = gs_b,
                             upar = c(3,2,1),
                             lower = gs_b,
                             lpar = c(-1, 0, 1),
                             algorithm = GenzBretz(maxpts= 1e5, abseps= 1e-5),
                             ...){
  
    # Currently only support user defined lower and upper bound
    stopifnot( identical(upper, gs_b) | identical(upper, gs_spending_combo) )
    stopifnot( identical(lower, gs_b) | identical(lower, gs_spending_combo) )
  
    # Obtain utilities
    utility <- gs_utility_combo(enrollRates = enrollRates,
                                failRates = failRates,
                                fh_test = fh_test,
                                ratio = ratio,
                                algorithm = algorithm, ...)
  
    info     <- utility$info_all
    info_fh  <- utility$info
    theta_fh <- utility$theta
    corr_fh  <- utility$corr
  
    # Sample size
    n <- max(info$N)
  
    # Information Fraction
    if(length(unique(fh_test$Analysis)) == 1){
      # Fixed design
      min_info_frac <- 1
  
    }else{
  
      info_frac <- tapply(info$info0, info$test, function(x) x / max(x))
      min_info_frac <- apply(do.call(rbind, info_frac), 2, min)
    }
  
    # Obtain spending function
    bound <- gs_bound(alpha = upper(upar, min_info_frac),
                      beta = lower(lpar, min_info_frac),
                      analysis = info_fh$Analysis,
                      theta = theta_fh * sqrt(n),
                      corr = corr_fh,
                      binding_lower_bound = binding,
                      algorithm = algorithm,
                      alpha_bound = identical(upper, gs_b),
                      beta_bound = identical(lower, gs_b),
                      ...)
  
  
    # Probability Cross Boundary uner Alternative
    prob <- gs_prob_combo(upper_bound = bound$upper,
                          lower_bound = bound$lower,
                          analysis = info_fh$Analysis,
                          theta = theta_fh * sqrt(n),
                          corr = corr_fh,
                          algorithm = algorithm, ...)
  
    # Probability Cross Boundary under Null
    prob_null <- gs_prob_combo(upper_bound = bound$upper,
                               lower_bound = if(binding){bound$lower}else{rep(-Inf, nrow(bound))},
                               analysis = info_fh$Analysis,
                               theta = rep(0, nrow(info_fh)),
                               corr = corr_fh,
                               algorithm = algorithm, ...)
  
    if(binding == FALSE){
      prob_null$Probability[prob_null$Bound == "Lower"] <- NA
    }
  
    prob$Probability_Null <- prob_null$Probability
  
    # Prepare output
    db <- merge(data.frame(Analysis = 1:(nrow(prob)/2), prob, Z = unlist(bound)),
                unique(info_fh[, c("Analysis", "Time", "N", "Events")])
    )
  
    db[order(db$Bound, decreasing = TRUE), c("Analysis", "Bound", "Time", "N", "Events", "Z", "Probability", "Probability_Null")]
  
  }

Package: gsdmvn
File: R/gs_power_npe.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom stats qnorm
  NULL
  #' Group sequential bound computation with non-constant effect
  #'
  #' \code{gs_power_npe()} derives group sequential bounds and boundary crossing probabilities for a design.
  #' It allows a non-constant treatment effect over time, but also can be applied for the usual homogeneous effect size designs.
  #' It requires treatment effect and statistical information at each analysis as well as a method of deriving bounds, such as spending.
  #' The routine enables two things not available in the gsDesign package: 1) non-constant effect, 2) more flexibility in boundary selection.
  #' For many applications, the non-proportional-hazards design function \code{gs_design_nph()} will be used; it calls this function.
  #' Initial bound types supported are 1) spending bounds, 2) fixed bounds, and 3) Haybittle-Peto-like bounds.
  #' The requirement is to have a boundary update method that can each bound without knowledge of future bounds.
  #' As an example, bounds based on conditional power that require knowledge of all future bounds are not supported by this routine;
  #' a more limited conditional power method will be demonstrated.
  #' Boundary family designs Wang-Tsiatis designs including the original (non-spending-function-based) O'Brien-Fleming and Pocock designs
  #' are not supported by \code{gs_power_npe()}.
  #' @param theta natural parameter for group sequential design representing
  #' expected incremental drift at all analyses; used for power calculation
  #' @param theta1 natural parameter for alternate hypothesis, if needed for lower bound computation
  #' @param info statistical information at all analyses for input \code{theta}
  #' @param info0 statistical information under null hypothesis, if different than \code{info};
  #' impacts null hypothesis bound calculation
  #' @param info1 statistical information under hypothesis used for futility bound calculation if different from
  #' \code{info}; impacts futility hypothesis bound calculation
  #' @param binding indicator of whether futility bound is binding; default of FALSE is recommended
  #' @param upper function to compute upper bound
  #' @param lower function to compare lower bound
  #' @param upar parameter to pass to upper
  #' @param lpar parameter to pass to lower
  #' @param test_upper indicator of which analyses should include an upper (efficacy) bound;
  #' single value of TRUE (default)  indicates all analyses; otherwise,
  #' a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound
  #' @param test_lower indicator of which analyses should include a lower bound;
  #' single value of TRUE (default) indicates all analyses;
  #' single value FALSE indicated no lower bound; otherwise,
  #' a logical vector of the same length as \code{info} should indicate which analyses will have a lower bound
  #' @param r  Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param tol Tolerance parameter for boundary convergence (on Z-scale)
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Extract the length of input info as the number of interim analysis.
  #'    \item Validate if input info0 is NULL, so set it equal to info.
  #'    \item Validate if input info1 is NULL, so set it equal to info.
  #'    \item Validate if the length of inputs info, info0, and info1 are the same.
  #'    \item Validate if input theta is a scalar, so replicate the value for all k interim analysis.
  #'    \item Validate if input theta1 is NULL and if it is a scalar. If it is NULL,
  #'    set it equal to input theta. If it is a scalar, replicate the value for all k interim analysis.
  #'    \item Validate if input test_upper is a scalar, so replicate the value for all k interim analysis.
  #'    \item Validate if input test_lower is a scalar, so replicate the value for all k interim analysis.
  #'    \item Define vector a to be -Inf with length equal to the number of interim analysis.
  #'    \item Define vector b to be Inf with length equal to the number of interim analysis.
  #'    \item Define hgm1_0 and hgm1 to be NULL.
  #'    \item Define upperProb and lowerProb to be vectors of NA with length of the number of interim analysis.
  #'    \item Update lower and upper bounds using \code{gs_b()}.
  #'    \item If there are no interim analysis, compute proabilities of crossing upper and lower bounds
  #'    using \code{h1()}.
  #'    \item Compute cross upper and lower bound probabilities using \code{hupdate()} and \code{h1()}.
  #'    \item Return a tibble of analysis number, Bounds, Z-values, Probability of crossing bounds,
  #'    theta, theta1, info, info0, and info1
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @author Keaven Anderson \email{keaven\_anderson@@merck.}
  #'
  #' @export
  #'
  #' @examples
  #'
  #' library(gsDesign)
  #' library(dplyr)
  #' 
  #' # Default (single analysis; Type I error controlled)
  #' gs_power_npe(theta=0) %>% filter(Bound=="Upper")
  #'
  #' # Fixed bound
  #' gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40, info0 = (1:3) * 40,
  #'               upper = gs_b, upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,
  #'               lower = gs_b, lpar = c(-1, 0, 0))
  #'
  #' # Same fixed efficacy bounds, no futility bound (i.e., non-binding bound), null hypothesis
  #' gs_power_npe(theta = rep(0,3), info = (1:3) * 40,
  #' upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,
  #' lpar = rep(-Inf, 3)) %>% filter(Bound=="Upper")
  #'
  #' # Fixed bound with futility only at analysis 1; efficacy only at analyses 2, 3
  #' gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40, 
  #'              upar = c(Inf, 3, 2), lpar = c(qnorm(.1), -Inf, -Inf))
  #'
  #' # Spending function bounds
  #' # Lower spending based on non-zero effect
  #' gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
  #'              upper = gs_spending_bound,
  #'              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
  #'              lower = gs_spending_bound,
  #'              lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1, param = -1, timing = NULL))
  #'
  #' # Same bounds, but power under different theta
  #' gs_power_npe(theta = c(.15, .25, .35), theta1 = c(.1, .2, .3), info = (1:3) * 40,
  #'              upper = gs_spending_bound,
  #'              upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
  #'              lower = gs_spending_bound,
  #'              lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1, param = -1, timing = NULL))
  #'
  #' # Two-sided symmetric spend, O'Brien-Fleming spending
  #' # Typically, 2-sided bounds are binding
  #' xx <- gs_power_npe(theta = rep(0, 3), theta1 = rep(0, 3), info = (1:3) * 40,
  #'                    upper = gs_spending_bound,
  #'                    binding = TRUE,
  #'                    upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
  #'                    lower = gs_spending_bound,
  #'                    lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL))
  #' xx
  #'
  #' # Re-use these bounds under alternate hypothesis
  #' # Always use binding = TRUE for power calculations
  #' upar <- (xx %>% filter(Bound=="Upper"))$Z
  #' gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
  #'              binding = TRUE,
  #'              upar = upar,
  #'              lpar = -upar)
  #'
  gs_power_npe <- function(theta = .1, theta1 = NULL, info = 1, info1 = NULL, info0 = NULL,
                           binding = FALSE,
                           upper=gs_b, lower=gs_b, upar = qnorm(.975), lpar= -Inf,
                           test_upper = TRUE, test_lower = TRUE,
                           r = 18, tol = 1e-6){
    #######################################################################################
    # WRITE INPUT CHECK TESTS AND RETURN APPROPRIATE ERROR MESSAGES
    # theta should be a scalar or vector of real values; if vector, same length as info
    # info should be a scalar or vector of positive increasing values
    # info0 should be NULL or of the same form as info
    # test_upper and test_lower should be logical scalar or vector; if vector same length as info
    # END INPUT CHECKS
    #######################################################################################
    # SET UP PARAMETERS
    K <- length(info)
    if (is.null(info0)) info0 <- info
    if (is.null(info1)) info1 <- info
    if (length(info1) != length(info) || length(info0) != length(info)) stop("gs_power_npe: length of info, info0, info1 must be the same")
    if (length(theta) == 1 && K > 1) theta <- rep(theta, K)
    if (is.null(theta1)){theta1 <- theta}else if (length(theta1)==1) theta1 <- rep(theta1,K)
    if (length(test_upper) == 1 && K > 1) test_upper <- rep(test_upper, K)
    if (length(test_lower) == 1 && K > 1) test_lower <- rep(test_lower, K)
    a <- rep(-Inf, K)
    b <- rep(Inf, K)
    hgm1_0 <- NULL
    hgm1_1 <- NULL
    hgm1 <- NULL
    upperProb <- rep(NA, K)
    lowerProb <- rep(NA, K)
    ######################################################################################
    # COMPUTE BOUNDS
    for(k in 1:K){
      # Lower bound update
      a[k] <- lower(k = k, par = lpar, hgm1 = hgm1_1, theta = theta1, info = info1, r = r, tol = tol, test_bound = test_lower,
                    efficacy = FALSE)
      # Upper bound update
      b[k] <- upper(k = k, par = upar, hgm1 = hgm1_0, info = info0, r = r, tol = tol, test_bound = test_upper)
      if(k==1){
        upperProb[1] <- if(b[1] < Inf) {pnorm(b[1], mean = sqrt(info[1]) * theta[1], lower.tail = FALSE)}else{0}
        lowerProb[1] <- if(a[1] > -Inf){pnorm(a[1], mean = sqrt(info[1]) * theta[1])}else{0}
        hgm1_0 <- h1(r = r, theta = 0,         I = info0[1], a = if(binding){a[1]}else{-Inf}, b = b[1])
        hgm1_1 <- h1(r = r, theta = theta1[1], I = info1[1], a = a[1], b = b[1])
        hgm1   <- h1(r = r, theta = theta[1],  I = info[1],  a = a[1], b = b[1])
      }else{
        # Cross upper bound
        upperProb[k] <- if(b[k]< Inf){
          hupdate(r = r, theta = theta[k], I = info[k], a = b[k], b = Inf,
                  thetam1 = theta[k - 1], Im1 = info[k - 1], gm1 = hgm1) %>%
            summarise(sum(h)) %>% as.numeric()
        }else{0}
        # Cross lower bound
        lowerProb[k] <- if(a[k] > -Inf){
          hupdate(r = r, theta = theta[k], I = info[k], a = -Inf, b = a[k],
                  thetam1 = theta[k - 1], Im1 = info[k - 1], gm1 = hgm1) %>%
            summarise(sum(h)) %>% as.numeric()
        }else{0}
        if(k < K){
          hgm1_0 <- hupdate(r = r, theta = 0,         I = info0[k], a = if(binding){a[k]}else{-Inf}, b = b[k],
                            thetam1 = 0,           Im1 = info0[k-1], gm1 = hgm1_0)
          hgm1_1 <- hupdate(r = r, theta = theta1[k], I = info1[k], a = a[k], b = b[k],
                            thetam1 = theta1[k-1], Im1 = info1[k-1], gm1 = hgm1_1)
          hgm1   <- hupdate(r = r, theta = theta[k],  I = info[k],  a = a[k], b = b[k],
                            thetam1 = theta[k-1],  Im1 = info[k-1],  gm1 = hgm1)
        }
      }
    }
    return(tibble::tibble(
      Analysis = rep(1:K, 2),
      Bound = c(rep("Upper", K), rep("Lower", K)),
      Z= c(b, a),
      Probability = c(cumsum(upperProb),
                      cumsum(lowerProb)),
      theta = rep(theta, 2),
      theta1 = rep(theta1, 2),
      info = rep(info, 2),
      info0 = rep(info0, 2),
      info1 = rep(info1, 2))
    )
  }
  

Package: gsdmvn
File: R/gs_power_nph.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom tibble tibble
  #' @importFrom gsDesign gsDesign sfLDOF
  #' @importFrom stats qnorm
  #' @importFrom dplyr select left_join
  NULL
  #' Group sequential design power under non-proportional hazards
  #'
  #' @param enrollRates enrollment rates
  #' @param failRates failure and dropout rates
  #' @param ratio Experimental:Control randomization ratio (not yet implemented)
  #' @param events Targeted events at each analysis
  #' @param maxEvents Final planned events
  #' @param analysisTimes Not yet implemented
  #' @param upper Function to compute upper bound
  #' @param upar Parameter passed to \code{upper()}
  #' @param lower Function to compute lower bound
  #' @param lpar Parameter passed to \code{lower()}
  #' @param r Control for grid size; normally leave at default of \code{r=18}
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Compute average hazard ratio using \code{gs_info_ahr()}.
  #'    \item Calculate the probability of crossing bounds using \code{gs_prob()}.
  #'    \item Combine the probability of crossing bounds and average hazard ratio and return a
  #'    tibble  with columns Analysis, Bound, Z, Probability, theta, Time, avehr, and Events.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return a \code{tibble} with columns Analysis, Bound, Z, Probability, theta, Time, avehr, Events
  #' @details Need to be added
  #' @export
  #'
  #' @examples
  #' library(gsDesign)
  #' library(gsDesign2)
  #' library(dplyr)
  #' 
  #' gs_power_nph() %>% filter(abs(Z) < Inf)
  gs_power_nph <- function(enrollRates=tibble::tibble(Stratum="All",
                                                    duration=c(2,2,10),
                                                    rate=c(3,6,9)),
                         failRates=tibble::tibble(Stratum="All",
                                                  duration=c(3,100),
                                                  failRate=log(2)/c(9,18),
                                                  hr=c(.9,.6),
                                                  dropoutRate=rep(.001,2)),
                         ratio=1,
                         events = c(30, 40, 50),
                         analysisTimes = NULL,
                         maxEvents = 45,       # max planned events
                         upper = gs_b,
                         # Default is Lan-DeMets approximation of
                         upar = gsDesign(k=length(events), test.type=1,
                                         n.I=events, maxn.IPlan = maxEvents,
                                         sfu=sfLDOF, sfupar = NULL)$upper$bound,
                         lower = gs_b,
                         lpar = c(qnorm(.1), rep(-Inf, 2)), # Futility only at IA1
                         r = 18
  ){
    avehr <- gs_info_ahr(enrollRates = enrollRates,
                     failRates = failRates,
                     ratio = ratio,
                     events = events,
                     analysisTimes = analysisTimes
    )
    x <- gs_prob(info = avehr$info,
                theta = -log(avehr$AHR),
                upper = upper,
                upar = upar,
                lower = lower,
                lpar = lpar,
                r = r) %>% select(-c("info","theta"))
    return(left_join(x, avehr, by = "Analysis"))
  }

Package: gsdmvn
File: R/gs_power_wlr.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Group sequential design power using weighted log rank test under non-proportional hazards
  #'
  #' @inheritParams gs_design_wlr
  #' @inheritParams gs_power_ahr
  #' @section Specification:
  #' 
  #' \if{latex}{
  #'  \itemize{
  #'    \item Compute information and effect size for Weighted Log-rank test using \code{gs_info_wlr()}.
  #'    \item Compute group sequential bound computation with non-constant effect using \code{gs_power_npe()}.
  #'    \item Combine information and effect size and power and return a
  #'    tibble  with columns Analysis, Bound, Time, Events, Z, Probability, AHR,  theta, info, and info0.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @export
  gs_power_wlr <- function(enrollRates=tibble::tibble(Stratum="All",
                                                      duration=c(2,2,10),
                                                      rate=c(3,6,9)),
                           failRates=tibble::tibble(Stratum="All",
                                                    duration=c(3,100),
                                                    failRate=log(2)/c(9,18),
                                                    hr=c(.9,.6),
                                                    dropoutRate=rep(.001,2)),
                           ratio=1,               # Experimental:Control randomization ratio
                           weight = wlr_weight_fh,
                           approx = "asymptotic",
                           events = c(30, 40, 50), # Targeted events of analysis
                           analysisTimes = NULL,   # Targeted times of analysis
                           binding = FALSE,
                           upper = gs_b,
                           # Default is Lan-DeMets approximation of
                           upar = gsDesign(k=length(events), test.type=1,
                                           n.I=events, maxn.IPlan = max(events),
                                           sfu=sfLDOF, sfupar = NULL)$upper$bound,
                           lower = gs_b,
                           lpar = c(qnorm(.1), rep(-Inf, length(events) - 1)), # Futility only at IA1
                           test_upper = TRUE, test_lower = TRUE,
                           r = 18,
                           tol = 1e-6
  ){
    x <- gs_info_wlr(enrollRates = enrollRates,
                     failRates = failRates,
                     ratio = ratio,
                     events = events,
                     weight = weight,
                     analysisTimes = analysisTimes
    )
    return(gs_power_npe(theta = x$theta, info = x$info, info0 = x$info0, binding = binding,
                        upper=upper, lower=lower, upar = upar, lpar= lpar,
                        test_upper = test_upper, test_lower = test_lower,
                        r = r, tol = tol) %>%
             right_join(x %>% select(-c(info, info0, theta)), by = "Analysis") %>%
             select(c(Analysis, Bound, Time, Events, Z, Probability, AHR, theta, info, info0)) %>%
             arrange(desc(Bound), Analysis)
    )
  }

Package: gsdmvn
File: R/gs_prob.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom stats pnorm
  #' @importFrom dplyr "%>%" summarise filter
  #' @importFrom tibble tibble
  NULL
  #' Group sequential boundary crossing probabilities
  #'
  #' @param theta natural parameter for group sequentia design representing expected drift at time of each analysis
  #' @param upper function to compute upper bound
  #' @param lower function to compare lower bound
  #' @param upar parameter to pass to upper
  #' @param lpar parameter to pass to lower
  #' @param info statistical information at each analysis
  #' @param r  Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @return A `tibble` with a row for each finite bound and analysis containing the following variables:
  #' Analysis analysis number
  #' Bound Upper (efficacy) or Lower (futility)
  #' Z Z-value at bound
  #' Probability probability that this is the first bound crossed under the given input
  #' theta approximate natural parameter value required to cross the bound
  #'
  #' @details
  #' Approximation for \code{theta} is based on Wald test and assumes the observed information is equal to the expected.
  #' @examples
  #' library(dplyr)
  #' # Asymmetric 2-sided design
  #' gs_prob(theta = 0, upar = rep(2.2, 3), lpar = rep(0, 3), 
  #'         upper=gs_b, lower=gs_b,  info = 1:3)
  #' # One-sided design
  #' x <- gs_prob(theta = 0, upar = rep(2.2, 3), lpar = rep(-Inf, 3), 
  #'              upper=gs_b, lower=gs_b,  info = 1:3)
  #' # Without filtering, this shows unneeded lower bound
  #' x
  #' # Filter to just show bounds intended for use
  #' x %>% filter(abs(Z) < Inf)
  #' @export
  gs_prob <- function(theta, upper=gs_b, lower=gs_b, upar, lpar, info, r = 18){
     # deal with R cmd check messages
     Z <- h <- NULL
     K <- length(info)
     Zupper <- upper(upar, info)
     Zlower <- lower(lpar, info)
     if (length(theta) == 1) theta <- rep(theta, K)
     upperProb <- rep(NA, K)
     lowerProb <- rep(NA, K)
     for(k in seq_along(info)){
       if(k==1){
          upperProb[1] <- if(Zupper[1] < Inf) {pnorm(Zupper[1], mean = sqrt(info[1]) * theta[1], lower.tail = FALSE)}else{0}
          lowerProb[1] <- if(Zlower[1] > -Inf){pnorm(Zlower[1], mean = sqrt(info[1]) * theta[1])}else{0}
          g <- h1(r = r, theta = theta[1], I = info[1], a = Zlower[1], b = Zupper[1])
       }else{
          # Cross upper bound
          upperProb[k] <- if(Zupper[k]< Inf){
                hupdate(r = r, theta = theta[k], I = info[k], a = Zupper[k], b = Inf,
                               thetam1 = theta[k - 1], Im1 = info[k - 1], gm1 = g) %>%
                summarise(sum(h)) %>% as.numeric()
             }else{0}
        # Cross lower bound
          lowerProb[k] <- if(Zlower[k] > -Inf){
                hupdate(r = r, theta = theta[k], I = info[k], a = -Inf, b = Zlower[k],
                               thetam1 = theta[k - 1], Im1 = info[k - 1], gm1 = g) %>%
                summarise(sum(h)) %>% as.numeric()
          }else{0}
          # if k < K, update numerical integration for next analy
          if (k < K) g <- hupdate(r = r, theta = theta[k], I = info[k], a = Zlower[k], b = Zupper[k],
                                         thetam1 = theta[k - 1], Im1 = info[k - 1], gm1 = g)
       }
     }
       return(tibble::tibble(
                Analysis = rep(1:K, 2),
                Bound = c(rep("Upper", K), rep("Lower", K)),
                Z= c(Zupper, Zlower),
                Probability = c(cumsum(upperProb),
                                cumsum(lowerProb)),
                theta = rep(theta, 2),
                info = rep(info, 2)) # %>% filter(abs(Z) < Inf)
           )
  }

Package: gsdmvn
File: R/gs_prob_combo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' MaxCombo Group sequential boundary crossing probabilities
  #'
  #' @inheritParams pmvnorm_combo
  #' @param upper_bound a numeric vector of upper bound
  #' @param lower_bound a numeric vector of lower bound
  #' @param analysis an integer vector of the interim analysis index
  #' @param theta a numeric vector of effect size under alternative hypothesis
  #' @param corr a matrix of correlation matrix
  #'
  #' @importFrom mvtnorm GenzBretz
  #'
  gs_prob_combo <- function(upper_bound,
                            lower_bound,
                            analysis,
                            theta,
                            corr,
                            algorithm = GenzBretz(maxpts= 1e5, abseps= 1e-5),
                            ...){
  
    n_analysis <- length(unique(analysis))
  
    p <- c()
    q <- c()
    for(k in 1:n_analysis){
      k_ind <- analysis <= k
  
  
      # Efficacy Bound
      if(k == 1){
        lower <- upper_bound[1]
        upper <- Inf
      }else{
        lower <- c(lower_bound[1:(k-1)], upper_bound[k])
        upper <- c(upper_bound[1:(k-1)], Inf)
      }
  
  
      p[k] <- pmvnorm_combo(lower,
                            upper,
                            group = analysis[k_ind],
                            mean = theta[k_ind],
                            corr = corr[k_ind, k_ind])
  
      # Futility Bound
      if(k == 1){
        lower <- -Inf
        upper <- lower_bound[k]
      }else{
        lower <- c(lower_bound[1:(k-1)], -Inf)
        upper <- c(upper_bound[1:(k-1)], lower_bound[k])
      }
  
      q[k] <- pmvnorm_combo(lower,
                            upper,
                            group = analysis[k_ind],
                            mean  = theta[k_ind],
                            corr  = corr[k_ind, k_ind])
  
    }
  
    data.frame(Bound = rep(c("Upper", "Lower"), each = n_analysis),
               Probability = c(cumsum(p),cumsum(q)))
  
  }
  
  

Package: gsdmvn
File: R/gs_spending_bound.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom dplyr summarize
  #' @importFrom gsDesign gsDesign sfLDOF
  #' @importFrom stats qnorm
  NULL
  #' Derive spending bound for group sequential boundary
  #'
  #' Computes one bound at a time based on spending under given distributional assumptions.
  #' While user specifies \code{gs_spending_bound()} for use with other functions,
  #' it is not intended for use on its own.
  #' Most important user specifications are made through a list provided to functions using \code{gs_spending_bound()}.
  #' Function uses numerical integration and Newton-Raphson iteration to derive an individual bound for a group sequential
  #' design that satisfies a targeted boundary crossing probability.
  #' Algorithm is a simple extension of that in Chapter 19 of Jennison and Turnbull (2000).
  #'
  #' @param k analysis for which bound is to be computed
  #' @param par a list with the following items:
  #' \code{sf} (class spending function),
  #' \code{total_spend} (total spend),
  #' \code{param} (any parameters needed by the spending function \code{sf()}),
  #' \code{timing} (a vector containing values at which spending function is to be evaluated or NULL if information-based spending is used),
  #' \code{max_info} (when \code{timing} is NULL, this can be input as positive number to be used with \code{info} for information fraction at each analysis)
  #' @param hgm1 subdensity grid from h1 (k=2) or hupdate (k>2) for analysis k-1; if k=1, this is not used and may be NULL
  #' @param theta natural parameter used for lower bound only spending;
  #' represents average drift at each time of analysis at least up to analysis k;
  #' upper bound spending is always set under null hypothesis (theta = 0)
  #' @param info statistical information at all analyses, at least up to analysis k
  #' @param efficacy TRUE (default) for efficacy bound, FALSE otherwise
  #' @param test_bound a logical vector of the same length as \code{info} should indicate which analyses will have a bound
  #' @param r  Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param tol Tolerance parameter for convergence (on Z-scale)
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Set the spending time at analysis.
  #'    \item Compute the cumulative spending at analysis.
  #'    \item Compute the incremental spend at each analysis.
  #'    \item Set test_bound a vector of length k > 1 if input as a single value.
  #'    \item Compute spending for current bound.
  #'    \item Iterate to convergence as in gsbound.c from gsDesign.
  #'    \item Compute subdensity for final analysis in rejection region.
  #'    \item Validate the output and return an error message in case of failure.
  #'    \item Return a numeric bound (possibly infinite).
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return returns a numeric bound (possibly infinite) or, upon failure, generates an error message.
  #' @author Keaven Anderson \email{keaven\_anderson@@merck.}
  #' @references Jennison C and Turnbull BW (2000), \emph{Group Sequential
  #' Methods with Applications to Clinical Trials}. Boca Raton: Chapman and Hall.
  #' @export
  gs_spending_bound <- function(k = 1,
                                par = list(sf = gsDesign::sfLDOF,
                                                total_spend = 0.025,
                                                param = NULL,
                                                timing = NULL,
                                                max_info = NULL
                                           ),
                                hgm1 = NULL,
                                theta = .1,
                                info = 1:3,
                                efficacy = TRUE,
                                test_bound = TRUE,
                                r = 18,
                                tol = 1e-6){
    # Set spending time at analyses
    if (!is.null(par$timing)){ timing <- par$timing
    }else{
      if (is.null(par$max_info)){
           timing <- info / (max(info))
      }else timing <- info / par$max_info
    }
    # Cumulative spending at analyses
    spend <- par$sf(alpha = par$total_spend, t = timing, param = par$param)$spend
    # Get incremental spend at each analysis
    old_spend <- 0
    # Make test_bound a vector of length k > 1 if input as a single value
    if (length(test_bound) == 1 && k > 1) test_bound <- rep(test_bound, k)
    # Get incremental spend at each analysis
    for(i in 1:k){
      if (test_bound[i]){ # Check if spending is taken at analysis i
        xx <- spend[i] - old_spend # Cumulative spending minus previous spending
        old_spend <- spend[i] # Reset previous spending
        spend[i] <- xx # Incremental spend at analysis i
      }else spend[i] <- 0 # 0 incremental spend if no testing at analysis i
    }
    # Now just get spending for current bound
    spend <- spend[k]
    # lower bound
    if (!efficacy){
      if (spend <= 0) return(-Inf) # If no spending, return -Inf for bound
      # if theta not a vector, make it one
      if (length(theta) == 1) theta <- rep(theta, length(info))
      # Starting value
      a <- qnorm(spend) + sqrt(info[k]) * theta[k]
      if (k == 1) return(a) # No need for iteration for first interim
      # Extremes for numerical integration
      mu <- theta[k] * sqrt(info[k])
      EXTREMElow <- mu - 3 - 4 * log(r)
      EXTREMEhi <- mu + 3 + 4 * log(r)
      # iterate to convergence as in gsbound.c from gsDesign
      adelta <- 1
      j <- 0
      while(abs(adelta) > tol)
      {  # Get grid for rejection region
        hg <- hupdate(theta = theta[k], I =  info[k], a = -Inf, b = a, thetam1 = theta[k-1], Im1 = info[k-1], gm1 = hgm1, r = r)
        i <- nrow(hg)
        pik <- hg %>% summarise(sum(h)) %>% as.numeric() # pik is for lower bound crossing
  
        # FOLLOWING UPDATE ALGORITHM FROM GSDESIGN::GSBOUND.C
        ##################################################################
        # use 1st order Taylor's series to update boundaries
        # maximum allowed change is 1
        # maximum value allowed is z1[m1]*rtIk to keep within grid points
        adelta <- spend - pik
        dplo <- hg$h[i] / hg$w[i]
        if (adelta > dplo){adelta <- 1
        }else if (adelta < -dplo){adelta <- -1
        }else adelta <- adelta / dplo
        a <- a + adelta
        if (a > EXTREMEhi){a <- EXTREMEhi
        }else if (a < EXTREMElow) a <- EXTREMElow
        #################################################################
  
        if (abs(adelta) < tol) return(a)
        j <- j + 1
        if (j > 20) stop(paste("gs_spending_bound(): bound_update did not converge for lower bound calculation, analysis", k))
      }
  }else{
      # upper bound
      if(spend <= 0) return(Inf)
      # Starting value
      b <- qnorm(spend, lower.tail = FALSE)
      if(k == 1) return(b) # No iteration needed for first bound
      for(iter in 0:20){
        # subdensity for final analysis in rejection region
        hg <- hupdate(theta = 0, I =  info[k], a = b, b = Inf, thetam1 = 0, Im1 = info[k-1], gm1 = hgm1)
        pik <- as.numeric(hg %>% summarise(sum(h))) # Probability of crossing bound
        dpikdb <- hg$h[1] / hg$w[1] # Derivative of bound crossing at b[k]
        b_old <- b
        b <- b - (spend - pik) / dpikdb # Newton-Raphson update
        if (abs(b - b_old) < tol) return(b)
      }
      stop(paste("gs_spending_bound(): bound_update did not converge for upper bound calculation, analysis", k))
    }
  }

Package: gsdmvn
File: R/gs_spending_combo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Derive spending bound for MaxCombo group sequential boundary
  #'
  #' @inheritParams gs_spending_bound
  #' @param ... additional parameters transfered to `par$sf`.
  #'
  #' @examples
  #'
  #' # alpha-spending
  #' par <- list(sf = gsDesign::sfLDOF, total_spend = 0.025)
  #' gs_spending_combo(par, info = 1:3/3)
  #'
  #' # beta-spending
  #' par <- list(sf = gsDesign::sfLDOF, total_spend = 0.2)
  #' gs_spending_combo(par, info = 1:3/3)
  #'
  #' @export
  gs_spending_combo <- function(par = NULL, info = NULL, ...){
    par$sf(par$total_spend, info, ...)$spend
  }

Package: gsdmvn
File: R/gs_utility_combo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #'
  #' @importFrom mvtnorm GenzBretz
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Define the analysis time from input fh_test.
  #'    \item Compute arm0 and arm1 using \code{gs_create_arm()}.
  #'    \item Set a unique test.
  #'    \item Compute the information fraction using \code{gs_info_combo()}.
  #'    \item Compute the correlation between tests.
  #'    \item Compute the correlation between analysis.
  #'    \item Compute the overall correlation.
  #'    \item Extract the sample size from  info.
  #'    \item Compute information restricted to actual analysis.
  #'    \item Compute the effect size.
  #'    \item Return a list of info_all = info, info = info_fh, theta = theta_fh, corr = corr_fh.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  gs_utility_combo <- function(enrollRates,
                               failRates,
                               fh_test,
                               ratio = 1,
                               algorithm = GenzBretz(maxpts= 1e5, abseps= 1e-5),
                               ...){
  
    # Define analysis time
    analysisTimes <- sort(unique(fh_test$analysisTimes))
  
    # Define Arm
    gs_arm <- gs_create_arm(enrollRates, failRates,
                            ratio = ratio,                   # Randomization ratio
                            total_time = max(analysisTimes)) # Total study duration
  
    arm0 <- gs_arm[["arm0"]]
    arm1 <- gs_arm[["arm1"]]
  
    # Unique test
    u_fh_test <- unique(fh_test[, c("test","rho", "gamma", "tau")] )
  
    # Information Fraction
    info <- gs_info_combo(enrollRates, failRates, ratio,
                          analysisTimes = analysisTimes,
                          rho = u_fh_test$rho,
                          gamma = u_fh_test$gamma)
  
    # Correlation between test
    corr_test <- with(u_fh_test,
                      lapply(analysisTimes, function(tmax){
                        cov2cor(gs_sigma2_combo(arm0, arm1, tmax = tmax,
                                                rho = rho, gamma = gamma, tau = tau))
                      })
    )
  
    # Correlation between analysis
    info_split <- split(info, info$test)
    corr_time <- lapply(info_split, function(x){
      corr <- with(x, outer(sqrt(info), sqrt(info), function(x,y) pmin(x,y) / pmax(x,y)))
      rownames(corr) <- analysisTimes
      colnames(corr) <- analysisTimes
      corr
    })
  
    # Overall Correlation
    corr_combo <- diag(1, nrow = nrow(info))
    for(i in 1:nrow(info)){
      for(j in 1:nrow(info)){
        t1 <- as.numeric(info$Analysis[i])
        t2 <- as.numeric(info$Analysis[j])
        if(t1 <= t2){
          test1 <- as.numeric(info$test[i])
          test2 <- as.numeric(info$test[j])
          corr_combo[i,j] <- corr_test[[t1]][test1,test2] * corr_time[[test2]][t1, t2]
          corr_combo[j,i] <- corr_combo[i,j]
        }
      }
    }
  
    # Sample size
    n <- max(info$N)
  
    # Restricted to actual analysis
    info_fh <- merge(info, fh_test, all = TRUE)
    corr_fh <- corr_combo[! is.na(info_fh$gamma), ! is.na(info_fh$gamma)]
    info_fh <- subset(info_fh, ! is.na(gamma))
  
    # Effect size
    theta_fh <- (- info_fh$delta) / sqrt(info_fh$sigma2)
  
    list(info_all = info, info = info_fh, theta = theta_fh, corr = corr_fh)
  
  }

Package: gsdmvn
File: R/gsdmvn.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' gsdmvn: A package for group sequential design under non-proportional hazards (NPH)
  #'
  #' The gsdmvn package will eventually be incorporated into the gsDesign2 package.
  #' This version is for the Regulatory/ASA Biopharmaceutical Subsection training course in September, 2020
  #' The package computes the asymptotic normal distribution for group sequential designs,
  #' generalizing the theory presented by Jennison and Turnbull (2000) to cases with
  #' non-homogeneous treatment effect over time.
  #' The primary application for this is group sequential design under the assumption of non-proportional hazards.
  #' The gsdmvn package has 4 types of functions
  #' 1) support for asymptotic normal distribution computation,
  #' 2) support for group sequential bound derivation, and
  #' 3) support for design and power calculations.
  #' 4) applications to designs for survival analysis under non-proportional hazards assumptions.
  #'
  #' In addition to the above function categeories, vignettes show how to implement
  #' 1) design for a binomial endpoint as an example of how to extend the package to other endpoint types,
  #' 2) the extensive capabilities around group sequential boundary calculations,
  #' including enabling capabilities not in the gsDesign package.
  #'
  #' @section gsdmvn functions:
  #' The primary functions supporting non-proportional hazards in the short course are:
  #' \itemize{
  #' \item gs_design_ahr - derive group sequential design under non-proportional hazards (NPH) for logrank test
  #' \item gs_power_ahr - compute power for a group sequential design under non-proportional hazards for logrank test
  #' }
  #'
  #' Key supportive functions specify bound derivation for designs:
  #' \itemize{
  #' \item gs_b - directly provide bounds for designs
  #' \item gs_spending_bound - provide bounds based on a spending function (e.g., Lan-DeMets O'Brien-Fleming)
  #' }
  #'
  #' Underlying functions to support numerical integration that should not be directly needed by typical users are
  #' \itemize{
  #' \item gridpts - set up grid points and weights for numerical integration for a normal distribution
  #' \item h1 - initialize numerical integration grid points and weights for NPH for first analysis
  #' \item hupdate - update numerical integration grid points and weights for NPH from one interim to the next
  #' \item gs_power_npe - general non-constant-effect size boundary crossing probability calculation for group sequential design
  #' }
  #'
  #' @author Keaven Anderson \email{keaven\_anderson@@merck.}
  #' @references Jennison C and Turnbull BW (2000), \emph{Group Sequential
  #' Methods with Applications to Clinical Trials}. Boca Raton: Chapman and Hall.
  #' @docType package
  #' @name gsdmvn
  #' @keywords design survival nonparametric
  NULL

Package: gsdmvn
File: R/h1.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom stats dnorm pnorm
  #' @importFrom tibble tibble
  NULL
  #' Initialize numerical integration for group sequential design
  #'
  #' Compute grid points for first interim analysis in a group sequential design
  #'
  #' @param r Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param theta Drift parameter for first analysis
  #' @param I Information at first analysis
  #' @param a lower limit of integration (scalar)
  #' @param b upper limit of integration (scalar \code{> a})
  #'
  #' @details Mean for standard normal distribution under consideration is \code{mu = theta * sqrt(I)}
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Compute drift at analysis 1.
  #'    \item Compute deviation from drift.
  #'    \item Compute standard normal density, multiply by grid weight.
  #'    \item Return a tibble of z, w, and h.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return A \code{tibble} with grid points in \code{z}, numerical integration weights in \code{w},
  #' and a normal density with mean \code{mu = theta * sqrt{I}} and variance 1 times the weight in \code{w}.
  #' @export
  #'
  #' @examples
  #' library(dplyr)
  #' # Replicate variance of 1, mean of 35
  #' h1(theta = 5, I = 49) %>% summarise(mu = sum(z * h), var = sum((z - mu)^2 * h))
  #'
  #' # Replicate p-value of .0001 by numerical integration of tail
  #' h1(a = qnorm(.9999)) %>% summarise(p = sum(h))
  h1 <- function(r = 18, theta = 0, I = 1, a = -Inf, b = Inf){
    # fix for binding message
    z <- w <- h <- NULL
    # compute drift at analysis 1
    mu <- theta * sqrt(I);
    g <- gridpts(r, mu, a, b)
    # compute deviation from drift
    x <- g$z - mu
    # compute standard normal density, multiply by grid weight and return
    # values needed for numerical integration
    return(tibble::tibble(z = g$z, w = g$w, h = g$w * dnorm(x)))
  }

Package: gsdmvn
File: R/hupdate.r
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @importFrom stats dnorm
  #' @importFrom tibble tibble
  NULL
  #' Update numerical integration for group sequential design
  #'
  #' Update grid points for numerical integration from one analysis to the next
  #'
  #' @param r Integer, at least 2; default of 18 recommended by Jennison and Turnbull
  #' @param theta Drift parameter for current analysis
  #' @param I Information at current analysis
  #' @param a lower limit of integration (scalar)
  #' @param b upper limit of integration (scalar \code{> a})
  #' @param thetam1  Drift parameter for previous analysis
  #' @param Im1 Information at previous analysis
  #' @param gm1 numerical integration grid from \code{h1()} or previous run of \code{hupdate()}
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Compute the square root of the change in information.
  #'    \item Compute the grid points for group sequential design numerical integration.
  #'    \item Update the integration.
  #'    \item Return a tibble of z, w, and h.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @return A \code{tibble} with grid points in \code{z}, numerical integration weights in \code{w},
  #' and a normal density with mean \code{mu = theta * sqrt{I}} and variance 1 times the weight in \code{w}.
  #'
  #' @examples
  #' library(dplyr)
  #' # 2nd analysis with no interim bound and drift 0 should have mean 0, variance 1
  #' hupdate() %>% summarise(mu = sum(z * h), var = sum((z - mu)^2 * h))
  #' @export
  hupdate <- function(r = 18, theta = 0, I = 2, a = -Inf, b = Inf, thetam1 = 0, Im1 = 1, gm1 = h1()){
    # sqrt of change in information
    rtdelta <- sqrt(I - Im1)
    rtI <- sqrt(I)
    rtIm1 <- sqrt(Im1)
    g <- gridpts(r = r, mu = theta * rtI, a= a, b = b)
    # update integration
    mu <- theta * I - thetam1 * Im1
    h <- rep(0, length(g$z))
    for(i in seq_along(g$z)){
      x <- (g$z[i] * rtI - gm1$z * rtIm1 - mu ) / rtdelta
      h[i] <- sum(gm1$h * dnorm(x))
    }
    h <- h * g$w * rtI / rtdelta
    return(tibble::tibble(z = g$z, w = g$w, h = h))
  }

Package: gsdmvn
File: R/pmvnorm_combo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Multivariate Normal Distribution for Multivariate Maximum Statistics
  #'
  #' Computes the distribution function of the multivariate normal distribution
  #' with maximum statistics for arbitrary limits and correlation matrices
  #'
  #' @inheritParams mvtnorm::pmvnorm
  #' @param group the vector of test statistics group.
  #' @param ... additional parameters transfer to `mvtnorm::pmvnorm`
  #' @details
  #'
  #' Let $Z = {Z_ij}$ be a multivariate normal distribution.
  #' Here i is a group indicator and j is a within group statistics indicator.
  #' Let G_i = max({Z_ij}) for all test within one group.
  #' This program are calculating the probability
  #'
  #'   $$Pr( lower < max(G) < upper )$$
  #'
  #' @importFrom mvtnorm GenzBretz
  #'
  #' @export
  pmvnorm_combo <- function(lower,
                            upper,
                            group,
                            mean,
                            corr,
                            algorithm = GenzBretz(maxpts= 1e5, abseps= 1e-5),
                            ...){
  
    # Number of test in each group
    n_test <- as.numeric(table(group))
  
  
    # Ensure positive definitive of the correlation matrix
    if(! corpcor::is.positive.definite(corr)){
      corr <- corpcor::make.positive.definite(corr)
      corr <- stats::cov2cor(corr)
    }
  
    # One dimension test
    if(length(mean) == 1){
      p <- pnorm(mean, lower) - pnorm(mean, upper)
      return(p)
    }
  
    # One test for all group or lower bound is -Inf.
    if(all(n_test == 1) | all(lower == -Inf) ){
      p <- mvtnorm::pmvnorm(lower = rep(lower, n_test),
                   upper = rep(upper, n_test),
                   mean = mean,
                   corr = corr,
                   sigma = NULL,
                   algorithm = algorithm,
                   ...)
  
      return(p)
  
    # General Algorithm
    }else{
  
      # Re-arrange test based on the order for number of test
      group <- as.numeric(factor(group, order(n_test)))
  
      mean <- mean[order(group)]
      corr <- corr[order(group), order(group)]
      group <- group[order(group)]
  
      n_test <- as.numeric(table(group))
  
  
  
      # Split by number of test == 1
      lower1 <- lower[n_test == 1]
      upper1 <- upper[n_test == 1]
  
      lower2 <- lower[n_test > 1]
      upper2 <- upper[n_test > 1]
  
      # Combination of all possible test
      k <- length(lower2)
      test_ind <- split(matrix(c(1,-1), nrow = k, ncol = 2, byrow = TRUE), 1:k)
      test_ind <- expand.grid(test_ind)
      test <- split(test_ind, 1:nrow(test_ind))
  
      p <- sapply(test, function(x){
        lower_bound <- rep(c(lower1, rep(-Inf, k)), n_test)
        upper_bound <- rep(c(upper1, ifelse(x == 1, upper2, lower2)), n_test)
  
        p_bound <- mvtnorm::pmvnorm(lower = lower_bound,
                           upper = upper_bound,
                           mean = mean,
                           corr = corr,
                           sigma = NULL,
                           algorithm = algorithm,
                           ...)
  
        prod(x) * p_bound
  
      })
  
      return(sum(p))
  
    }
  
  }

Package: gsdmvn
File: R/wlr_weight.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the gsdmvn program.
  #
  #  gsdmvn is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Weight Function of Weighted Log-rank Test
  #'
  #' * `wlr_weight_fh` is Fleming-Harriongton, FH(rho, gamma) weight function.
  #' * `wlr_weight_1`  is constant for log rank test
  #' * `wlr_weight_power` is Gehan-Breslow and Tarone-Ware weight function.
  #'
  #' @param x analysis time
  #' @param arm0 an "arm" object defined in `npsurvSS` package
  #' @param arm1 an "arm" object defined in `npsurvSS` package
  #' @param rho A scalar parameter that controls the type of test
  #' @param gamma A scalar parameter that controls the type of test
  #' @param tau A scalar parameter of the cut-off time for modest weighted log rank test
  #' @param power A scalar parameter that controls the power of the weight function
  #' @section Specification:
  #' \if{latex}{
  #'  \itemize{
  #'    \item Compute the sample size via the sum of arm sizes.
  #'    \item Compute the proportion of size in the two arms.
  #'    \item If the input tau is specified, define time up to the cut off time tau.
  #'    \item Compute the CDF using the proportion of the size in the two arms and \code{npsruvSS::psurv()}.
  #'    \item Return the Fleming-Harriongton weights for weighted Log-rank test.
  #'   }
  #' }
  #' \if{html}{The contents of this section are shown in PDF user manual only.}
  #'
  #' @name wlr_weight
  
  #' @rdname wlr_weight
  #' @export
  wlr_weight_fh <- function(x, arm0, arm1, rho = 0, gamma = 0, tau = NULL) {
  
    n   <- arm0$size + arm1$size
    p1 <- arm1$size / n
    p0 <- 1 - p1
  
    if(! is.null(tau)){
      # Define time up to cut-off time tau
      if(tau > 0){x <- pmin(x, tau)}
    }
  
    # CDF
    esurv <- p0 * npsurvSS::psurv(x, arm0) + p1 * npsurvSS::psurv(x, arm1)
    (1-esurv)^rho * esurv^gamma
  
  }
  
  #' @rdname wlr_weight
  #' @export
  wlr_weight_1 <- function(x, arm0, arm1){
    1
  }
  
  #' @rdname wlr_weight
  #' @export
  wlr_weight_n <- function(x, arm0, arm1, power = 1){
  
    n   <- arm0$size + arm1$size
    p1 <- arm1$size / n
    p0 <- 1 - p1
    tmax <- arm0$total_time
  
    (n * (p0 * prob_risk(arm0, x, tmax) + p1 * prob_risk(arm1, x,tmax)))^power
  }

Package: gsdmvn
File: man/ahr_blinded.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/ahr_blinded.R
  \name{ahr_blinded}
  \alias{ahr_blinded}
  \title{Blinded estimation of average hazard ratio}
  \usage{
  ahr_blinded(
    Srv = survival::Surv(time = simtrial::Ex1delayedEffect$month, event =
      simtrial::Ex1delayedEffect$evntd),
    intervals = array(3, 3),
    hr = c(1, 0.6),
    ratio = 1
  )
  }
  \arguments{
  \item{Srv}{input survival object (see \code{Surv}); note that only 0=censored, 1=event for \code{Surv}}
  
  \item{intervals}{Vector containing positive values indicating interval lengths where the
  exponential rates are assumed.
  Note that a final infinite interval is added if any events occur after the final interval
  specified.}
  
  \item{hr}{vector of hazard ratios assumed for each interval}
  
  \item{ratio}{ratio of experimental to control randomization.}
  }
  \value{
  A \code{tibble} with one row containing
  `AHR` blinded average hazard ratio based on assumed period-specific hazard ratios input in `failRates`
  and observed events in the corresponding intervals
  `Events` total observed number of events, `info` statistical information based on Schoenfeld approximation,
  and info0 (information under related null hypothesis) for each value of `totalDuration` input;
  if `simple=FALSE`, `Stratum` and `t` (beginning of each constant HR period) are also returned
  and `HR` is returned instead of `AHR`
  }
  \description{
  Based on blinded data and assumed hazard ratios in different intervals, compute
  a blinded estimate of average hazard ratio (AHR) and corresponding estimate of statistical information.
  This function is intended for use in computing futility bounds based on spending assuming
  the input hazard ratio (hr) values for intervals specified here.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if input hr is a numeric vector.
     \item Validate if input hr is non-negative.
     \item Simulate piece-wise exponential survival estimation with the inputs survival object Srv
     and intervals.
     \item Save the length of  hr and events to an object, and if the length of hr is shorter than
     the intervals, add replicates of the last element of hr and the corresponding numbers of events
     to hr.
     \item Compute the blinded estimation of average hazard ratio.
     \item Compute adjustment for information.
     \item Return a tibble of the sum of events, average hazard raito, blinded average hazard
     ratio, and the information.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  \dontrun{
  library(simtrial)
  library(survival)
  ahr_blinded(Srv = Surv(time = simtrial::Ex2delayedEffect$month,
                         event = simtrial::Ex2delayedEffect$evntd),
              intervals = c(4,100),
              hr = c(1, .55),
              ratio = 1)
  }
  
  }

Package: gsdmvn
File: man/gridpts.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gridpts.r
  \name{gridpts}
  \alias{gridpts}
  \title{Grid points for group sequential design numerical integration}
  \usage{
  gridpts(r = 18, mu = 0, a = -Inf, b = Inf)
  }
  \arguments{
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{mu}{Mean of normal distribution (scalar) under consideration}
  
  \item{a}{lower limit of integration (scalar)}
  
  \item{b}{upper limit of integration (scalar \code{> a})}
  }
  \value{
  A \code{tibble} with grid points in \code{z} and numerical integration weights in \code{w}
  }
  \description{
  Points and weights for Simpson's rule numerical integration from
  p 349 - 350 of Jennison and Turnbull book.
  This is not used for arbitrary integration, but for the canonical form of Jennison and Turnbull.
  mu is computed elsewhere as drift parameter times sqrt of information.
  Since this is a lower-level routine, no checking of input is done; calling routines should
  ensure that input is correct.
  Lower limit of integration can be \code{-Inf} and upper limit of integration can be \code{Inf}
  }
  \details{
  Jennison and Turnbull (p 350) claim accuracy of \code{10E-6} with \code{r=16}.
  The numerical integration grid spreads out at the tail to enable accurate tail probability calcuations.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Define odd numbered grid points for real line.
     \item Trim points outside of [a, b] and include those points.
     \item If extreme, include only 1 point where density will be essentially 0.
     \item Define even numbered grid points between the odd ones.
     \item Compute weights for odd numbered grid points.
     \item Combine odd- and even-numbered grid points with their corresponding weights.
     \item Return a tibble of with grid points in z and numerical integration weights in z.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(dplyr)
  
  # approximate variance of standard normal (i.e., 1)
  gridpts() \%>\% summarise(var = sum(z^2 * w * dnorm(z)))
  
  # approximate probability above .95 quantile (i.e., .05)
  gridpts(a = qnorm(.95), b = Inf) \%>\% summarise(p05 = sum(w * dnorm(z)))
  }

Package: gsdmvn
File: man/gs_b.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_b.r
  \name{gs_b}
  \alias{gs_b}
  \title{gs_b: Default boundary generation}
  \usage{
  gs_b(par = NULL, info = NULL, k = NULL, ...)
  }
  \arguments{
  \item{par}{For \code{gs_b()}, this is just Z-values for the boundaries; can include infinite values}
  
  \item{info}{Information at analyses; not used in this function; present as it is a standard parameter for other boundary computation routines}
  
  \item{k}{is NULL (default), return \code{par}, else return \code{par[k]}}
  
  \item{...}{further arguments passed to or from other methods}
  }
  \value{
  returns the vector input \code{par} if \code{k} is NULL, otherwise, \code{par[k]}
  }
  \description{
  \code{gs_b()} is the simplest version of a function to be used with the \code{upper} and \code{lower}
  arguments in \code{gs_prob()},
  \code{gs_power_nph} and \code{gs_design_nph()};
  it simply returns the vector input in the input vector \code{Z} or, if \code{k} is specified \code{par[k]j} is returned.
  Note that if bounds need to change with changing information at analyses, \code{gs_b()} should not be used.
  For instance, for spending function bounds use
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if the input k is null as default.
     \itemize{
       \item If the input k is null as default, return the whole vector of Z-values of the boundaries.
       \item If the input k is not null, return the corresponding boundary in the vector of Z-values.
       }
     \item Return a vector of boundaries.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  # Simple: enter a vector of length 3 for bound
  gs_b(par = 4:2)
  
  # 2nd element of par
  gs_b(4:2, k = 2)
  
  # Generate an efficacy bound using a spending function
  # Use Lan-DeMets spending approximation of O'Brien-Fleming bound
  # as 50\%, 75\% and 100\% of final spending
  # Information fraction
  IF <- c(.5, .75, 1)
  gs_b(par = gsDesign::gsDesign(alpha = .025, k= length(IF), 
       test.type = 1, sfu = gsDesign::sfLDOF, 
       timing = IF)$upper$bound)
  }

Package: gsdmvn
File: man/gs_bound.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_bound.R
  \name{gs_bound}
  \alias{gs_bound}
  \title{Lower and Upper Bound of Group Sequential Design}
  \usage{
  gs_bound(
    alpha,
    beta,
    theta,
    corr,
    analysis = 1:length(alpha),
    theta0 = rep(0, length(analysis)),
    binding_lower_bound = FALSE,
    algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),
    alpha_bound = FALSE,
    beta_bound = FALSE,
    ...
  )
  }
  \arguments{
  \item{alpha}{a numeric vector of cumulative allocated alpha in each interim analysis}
  
  \item{beta}{a numeric vector of cumulative allocated beta in each interim analysis}
  
  \item{theta}{a numeric vector of effect size under alternative.}
  
  \item{corr}{a matrix of correlation matrix}
  
  \item{analysis}{a numeric vector of interim analysis indicator. Default is 1:length(alpha).}
  
  \item{theta0}{a numeric vector of effect size under null hypothesis. Default is 0.}
  
  \item{binding_lower_bound}{a logical value to indicate binding lower bound.}
  
  \item{algorithm}{ an object of class \code{\link[mvtnorm]{GenzBretz}},
                      \code{\link[mvtnorm]{Miwa}} or \code{\link[mvtnorm]{TVPACK}}
                      specifying both the algorithm to be used as well as 
                      the associated hyper parameters.}
  
  \item{alpha_bound}{logical value to indicate if alpha is Type I error or upper bound. Default is FALSE.}
  
  \item{beta_bound}{logical value to indicate if beta is Type II error or lower bound. Default is FALSE.}
  
  \item{...}{additional parameters transfer to `mvtnorm::pmvnorm`}
  }
  \description{
  Lower and Upper Bound of Group Sequential Design
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Create a vector of allocated alpha in each interim analysis from the cumulative allocated alpha.
     \item Create a vector of allocated beta in each interim analysis from the cumulative allocated beta.
     \item Extract the number of analysis.
     \item Find the upper and lower bound by solving multivariate normal distribution using \code{pmvnorm_combo}
     \item
     \item Return a data frame of upper and lower boundaries of group sequential design.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(gsDesign)
  
  x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,
                         beta = 0.2 , astar = 0 , timing = c( 1 ) ,
                         sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,
                         sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,
                         hr = 0.6 , hr0 = 1 , eta = 0.01 ,
                         gamma = c( 10 ) ,
                         R = c( 12 ) , S = NULL ,
                         T = 36 , minfup = 24 , ratio = 1 )
  
  cbind(x$lower$bound, x$upper$bound)
  
  gsdmvn:::gs_bound(alpha = sfLDOF(0.025, 1:3/3)$spend,
           beta = sfLDOF(0.2, 1:3/3)$spend,
           analysis = 1:3,
           theta = x$theta[2] * sqrt(x$n.I),
           corr = outer(1:3, 1:3, function(x,y) pmin(x,y) / pmax(x,y)))
  
  }

Package: gsdmvn
File: man/gs_create_arm.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_create_arm.R
  \name{gs_create_arm}
  \alias{gs_create_arm}
  \title{Create "npsurvSS" arm object}
  \usage{
  gs_create_arm(enrollRates, failRates, ratio, total_time = 1e+06)
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio}
  
  \item{total_time}{total analysis time}
  }
  \description{
  Create "npsurvSS" arm object
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if there is only one stratum.
     \item Calculate the accrual duration.
     \item calculate the accrual intervals.
     \item Calculate the accrual parameter as the proportion of enrollment rate*duration.
     \item Set cure proportion to zero.
     \item set survival intervals and shape.
     \item Set fail rate in failRates to the Weibull scale parameter for the survival distribution in the arm 0.
     \item Set the multiplication of hazard ratio and fail rate to the Weibull scale parameter
     for the survival distribution in the arm 1.
     \item Set the shape parameter to one as the exponential distribution for
     shape parameter for the loss to follow-up distribution
     \item Set the scale parameter to one as the scale parameter for the loss to follow-up
      distribution since the exponential distribution is supported only
     \item Create arm 0 using \code{npsurvSS::create_arm()} using the parameters for arm 0.
     \item Create arm 1 using \code{npsurvSS::create_arm()} using the parameters for arm 1.
     \item Set the class of the two arms.
     \item Return a list of the two arms.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  

Package: gsdmvn
File: man/gs_design_ahr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_design_ahr.r
  \name{gs_design_ahr}
  \alias{gs_design_ahr}
  \title{Group sequential design using average hazard ratio under non-proportional hazards}
  \usage{
  gs_design_ahr(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    alpha = 0.025,
    beta = 0.1,
    IF = NULL,
    analysisTimes = 36,
    binding = FALSE,
    upper = gs_b,
    upar = gsDesign(k = 3, test.type = 1, n.I = c(0.25, 0.75, 1), sfu = sfLDOF, sfupar =
      NULL)$upper$bound,
    lower = gs_b,
    lpar = c(qnorm(0.1), -Inf, -Inf),
    h1_spending = TRUE,
    test_upper = TRUE,
    test_lower = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{alpha}{One-sided Type I error}
  
  \item{beta}{Type II error}
  
  \item{IF}{Targeted information fraction at each analysis}
  
  \item{analysisTimes}{Minimum time of analysis}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{h1_spending}{Indicator that lower bound to be set by spending under alternate hypothesis (input \code{failRates})
  if spending is used for lower bound}
  
  \item{test_upper}{indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound}
  
  \item{test_lower}{indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  lower bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for boundary convergence (on Z-scale)}
  }
  \value{
  a \code{tibble} with columns Analysis, Bound, Z, Probability, theta, Time, AHR, Events
  }
  \description{
  Group sequential design using average hazard ratio under non-proportional hazards
  }
  \details{
  Need to be added
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if input analysisTimes is a positive number or positive increasing sequence.
     \item Validate if input IF is a positive number or positive increasing sequence
     on (0, 1] with final value of 1.
     \item Validate if input IF and analysisTimes  have the same length if both have length > 1.
     \item Get information at input analysisTimes
     \itemize{
       \item Use \code{gs_info_ahr()} to get the information and effect size based on AHR approximation.
       \item Extract the final event.
       \item Check if input If needed for (any) interim analysis timing.
     }
     \item Add the analysis column to the information at input analysisTimes.
     \item Add the sample size column to the information at input analysisTimes using \code{eAccrual()}.
     \item Get sample size and bounds using \code{gs_design_npe()} and save them to bounds.
     \item Add Time, Events, AHR, N that have already been calculated to the bounds.
     \item Return a list of design enrollment, failure rates, and bounds.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(gsDesign)
  library(gsDesign2)
  library(dplyr)
  # call with defaults
  gs_design_ahr()
  
  # Single analysis
  gs_design_ahr(analysisTimes = 40)
  
  # Multiple analysisTimes
  gs_design_ahr(analysisTimes = c(12,24,36))
  
  # Specified information fraction
  gs_design_ahr(IF = c(.25,.75,1), analysisTimes = 36)
  
  # multiple analysis times & IF
  # driven by times
  gs_design_ahr(IF = c(.25,.75,1), analysisTimes = c(12,25,36))
  # driven by IF
  gs_design_ahr(IF = c(1/3, .8, 1), analysisTimes = c(12,25,36))
  
  # 2-sided symmetric design with O'Brien-Fleming spending
  gs_design_ahr(analysisTimes = c(12, 24, 36),
                binding = TRUE,
                upper = gs_spending_bound,
                upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                            param = NULL, timing = NULL),
                lower = gs_spending_bound,
                lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                            param = NULL, timing = NULL),
                h1_spending = FALSE)
  
  # 2-sided asymmetric design with O'Brien-Fleming upper spending
  # Pocock lower spending under H1 (NPH)
  gs_design_ahr(analysisTimes = c(12, 24, 36),
                binding = TRUE,
                upper = gs_spending_bound,
                upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                            param = NULL, timing = NULL),
                lower = gs_spending_bound,
                lpar = list(sf = gsDesign::sfLDPocock, total_spend = 0.1, 
                            param = NULL, timing = NULL),
                h1_spending = TRUE)
  }

Package: gsdmvn
File: man/gs_design_combo.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_design_combo.R
  \name{gs_design_combo}
  \alias{gs_design_combo}
  \title{Group sequential design using MaxCombo test under non-proportional hazards}
  \usage{
  gs_design_combo(
    enrollRates,
    failRates,
    fh_test,
    ratio = 1,
    alpha = 0.025,
    beta = 0.2,
    binding = FALSE,
    upper = gs_b,
    upar = c(3, 2, 1),
    lower = gs_b,
    lpar = c(-1, 0, 1),
    algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),
    n_upper_bound = 1000,
    ...
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{fh_test}{a data frame to summarize the test in each analysis.
  Refer examples for its data structure.}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{alpha}{One-sided Type I error}
  
  \item{beta}{Type II error}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{algorithm}{ an object of class \code{\link[mvtnorm]{GenzBretz}},
                      \code{\link[mvtnorm]{Miwa}} or \code{\link[mvtnorm]{TVPACK}}
                      specifying both the algorithm to be used as well as 
                      the associated hyper parameters.}
  
  \item{n_upper_bound}{a numeric value of upper limit of sample size}
  
  \item{...}{additional parameters transfer to `mvtnorm::pmvnorm`}
  }
  \description{
  Group sequential design using MaxCombo test under non-proportional hazards
  }
  \examples{
  \dontrun{
  
  # The example is slow to run
  
  library(dplyr)
  library(mvtnorm)
  library(gsDesign)
  
  enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500/12)
  
  failRates <- tibble::tibble(Stratum = "All",
                              duration = c(4, 100),
                              failRate = log(2) / 15,  # median survival 15 month
                              hr = c(1, .6),
                              dropoutRate = 0.001)
  
  fh_test <- rbind( data.frame(rho = 0, gamma = 0, tau = -1,
                               test = 1,
                               Analysis = 1:3,
                               analysisTimes = c(12, 24, 36)),
                    data.frame(rho = c(0, 0.5), gamma = 0.5, tau = -1,
                               test = 2:3,
                               Analysis = 3, analysisTimes = 36)
  )
  
  x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,
                         beta = 0.2 , astar = 0 , timing = c( 1 ) ,
                         sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,
                         sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,
                         hr = 0.6 , hr0 = 1 , eta = 0.01 ,
                         gamma = c( 10 ) ,
                         R = c( 12 ) , S = NULL ,
                         T = 36 , minfup = 24 , ratio = 1 )
  
  # User defined boundary
  gs_design_combo(enrollRates,
                  failRates,
                  fh_test,
                  alpha = 0.025,
                  beta = 0.2,
                  ratio = 1,
                  binding = FALSE,                 # test.type = 4 non-binding futility bound
                  upar = x$upper$bound,
                  lpar = x$lower$bound)
  
  # Boundary derived by spending function
  gs_design_combo(enrollRates,
                  failRates,
                  fh_test,
                  alpha = 0.025,
                  beta = 0.2,
                  ratio = 1,
                  binding = FALSE,                 # test.type = 4 non-binding futility bound
                  upper = gs_spending_combo,
                  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),   # alpha spending
                  lower = gs_spending_combo,
                  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2),     # beta spending
  )
  }
  
  
  }

Package: gsdmvn
File: man/gs_design_npe.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_design_npe.r
  \name{gs_design_npe}
  \alias{gs_design_npe}
  \title{Group sequential design computation with non-constant effect and information}
  \usage{
  gs_design_npe(
    theta = 0.1,
    theta1 = NULL,
    info = 1,
    info0 = NULL,
    info1 = NULL,
    alpha = 0.025,
    beta = 0.1,
    binding = FALSE,
    upper = gs_b,
    lower = gs_b,
    upar = qnorm(0.975),
    lpar = -Inf,
    test_upper = TRUE,
    test_lower = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{theta}{natural parameter for group sequential design representing expected incremental drift at all analyses;
  used for power calculation}
  
  \item{theta1}{natural parameter used for lower bound spending; if \code{NULL}, this will be set to \code{theta}
  which yields the usual beta-spending. If set to 0, spending is 2-sided under null hypothesis.}
  
  \item{info}{proportionate statistical information at all analyses for input \code{theta}}
  
  \item{info0}{proportionate statistical information under null hypothesis, if different than alternative;
  impacts null hypothesis bound calculation}
  
  \item{info1}{proportionate statistical information under alternate hypothesis;
  impacts null hypothesis bound calculation}
  
  \item{alpha}{One-sided Type I error}
  
  \item{beta}{Type II error}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{function to compute upper bound}
  
  \item{lower}{function to compare lower bound}
  
  \item{upar}{parameter to pass to function provided in \code{upper}}
  
  \item{lpar}{Parameter passed to function provided in \code{lower}}
  
  \item{test_upper}{indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound}
  
  \item{test_lower}{indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  lower bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for boundary convergence (on Z-scale)}
  }
  \value{
  a \code{tibble} with columns Analysis, Bound, Z, Probability,  theta, info, info0
  }
  \description{
  \code{gs_design_npe()} derives group sequential design size, bounds and boundary crossing probabilities based on proportionate
  information and effect size at analyses.
  It allows a non-constant treatment effect over time, but also can be applied for the usual homogeneous effect size designs.
  It requires treatment effect and proportionate statistical information at each analysis as well as a method of deriving bounds, such as spending.
  The routine enables two things not available in the gsDesign package: 1) non-constant effect, 2) more flexibility in boundary selection.
  For many applications, the non-proportional-hazards design function \code{gs_design_nph()} will be used; it calls this function.
  Initial bound types supported are 1) spending bounds, 2) fixed bounds, and 3) Haybittle-Peto-like bounds.
  The requirement is to have a boundary update method that can each bound without knowledge of future bounds.
  As an example, bounds based on conditional power that require knowledge of all future bounds are not supported by this routine;
  a more limited conditional power method will be demonstrated.
  Boundary family designs Wang-Tsiatis designs including the original (non-spending-function-based) O'Brien-Fleming and Pocock designs
  are not supported by \code{gs_power_npe()}.
  }
  \details{
  The inputs \code{info} and \code{info0} should be vectors of the same length with increasing positive numbers.
  The design returned will change these by some constant scale factor to ensure the design has power \code{1 - beta}.
  The bound specifications in \code{upper, lower, upar, lpar} will be used to ensure Type I error and other boundary properties are as specified.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if input info is a numeric vector  or NULL, if non-NULL validate if it
     is strictly increasing and positive.
     \item Validate if input info0 is a numeric vector or NULL, if non-NULL validate if it
      is strictly increasing and positive.
     \item Validate if input info1 is a numeric vector or NULL, if non-NULL validate if it
     is strictly increasing and positive.
     \item Validate if input theta is a real vector and has the same length as info.
     \item Validate if input theta1 is a real vector and has the same length as info.
     \item Validate if input test_upper and test_lower are logical and have the same length as info.
     \item Validate if input test_upper value is TRUE.
     \item Validate if input alpha and beta are positive and of length one.
     \item Validate if input alpha and beta are from the unit interval and alpha is smaller than beta.
     \item Initialize bounds, numerical integration grids, boundary crossing probabilities.
     \item Compute fixed sample size for desired power and Type I error.
     \item Find an interval for information inflation to give correct power using \code{gs_power_npe()}.
     \item
     \item If there is no interim analysis, return a tibble including Analysis time, upper bound, Z-value,
     Probability of crossing bound, theta, info0 and info1.
     \item If the desing is a group sequential design, return a tibble of Analysis,
      Bound, Z, Probability,  theta, info, info0.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  
  library(gsDesign)
  library(dplyr)
  # Single analysis
  # Lachin book p 71 difference of proportions example
  pc <- .28 # Control response rate
  pe <- .40 # Experimental response rate
  p0 <- (pc + pe) / 2 # Ave response rate under H0
  # Information per increment of 1 in sample size
  info0 <- 1 / (p0 * (1 - p0) * 4)
  info1 <- 1 / (pc * (1 - pc) * 2 + pe * (1 - pe) * 2)
  # Result should round up to next even number = 652
  # Divide information needed under H1 by information per patient added
  gs_design_npe(theta = pe - pc, info = info1, info0 = info0)$info[1] / info1
  
  # Fixed bound
  design <-
  gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 80, 
                info0 = (1:3) * 80, info1 = (1:3) * 80,
                upper = gs_b, upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,
                lower = gs_b, lpar = c(-1, 0, 0))
  design
  
  # Same fixed bounds, null hypothesis
  gs_power_npe(theta = rep(0,3), info = design$info0[1:3], 
              upar = design$Z[1:3], lpar = design$Z[4:6])
  
  # Same upper bound; this represents non-binding Type I error and will total 0.025
  gs_power_npe(theta = rep(0,3), info = design$info0[1:3], 
               upar = design$Z[1:3], lpar = rep(-Inf,3)) \%>\% 
    filter(Bound=="Upper")
  
  # Spending bound examples
  
  # Design with futility only at analysis 1; efficacy only at analyses 2, 3
  # Spending bound for efficacy; fixed bound for futility
  # NOTE: test_upper and test_lower DO NOT WORK with gs_b; must explicitly make bounds infinite
  # test_upper and test_lower DO WORK with gs_spending_bound
  design <-
  gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 40, info0 = (1:3) * 40,
                upper = gs_spending_bound,
                upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                            param = NULL, timing = NULL),
                lower = gs_b, lpar = c(-1, -Inf, -Inf),
                test_upper = c(FALSE, TRUE, TRUE))
  design
  
  # Spending function bounds
  # 2-sided asymmetric bounds
  # Lower spending based on non-zero effect
  gs_design_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                           param = NULL, timing = NULL),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1, 
                           param = -1, timing = NULL))
  
  # Two-sided symmetric spend, O'Brien-Fleming spending
  # Typically, 2-sided bounds are binding
  xx <- gs_design_npe(theta = c(.1, .2, .3), theta1 = rep(0, 3), info = (1:3) * 40,
                      binding = TRUE,
                      upper = gs_spending_bound,
                      upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                                  param = NULL, timing = NULL),
                      lower = gs_spending_bound,
                      lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, 
                                  param = NULL, timing = NULL))
  xx
  
  # Re-use these bounds under alternate hypothesis
  # Always use binding = TRUE for power calculations
  upar <- (xx \%>\% filter(Bound=="Upper"))$Z
  gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
               binding = TRUE,
               upar = upar,
               lpar = -upar)
  
  }
  \author{
  Keaven Anderson \email{keaven\_anderson@merck.}
  }

Package: gsdmvn
File: man/gs_design_nph.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_design_nph.r
  \name{gs_design_nph}
  \alias{gs_design_nph}
  \title{Fixed and group sequential design under non-proportional hazards}
  \usage{
  gs_design_nph(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    alpha = 0.025,
    beta = 0.1,
    analysisTimes = 30,
    IF = c(0.25, 0.75, 1),
    upper = gs_b,
    upar = gsDesign::gsDesign(k = 3, test.type = 1, n.I = c(0.25, 0.75, 1), maxn.IPlan =
      1, sfu = sfLDOF, sfupar = NULL)$upper$bound,
    lower = gs_b,
    lpar = c(qnorm(0.1), rep(-Inf, length(IF) - 1)),
    r = 18
  )
  }
  \arguments{
  \item{enrollRates}{Piecewise constant enrollment rates by stratum and time period.}
  
  \item{failRates}{Piecewise constant control group failure rates, duration for each piecewise constant period,
  hazard ratio for experimental vs control, and dropout rates by stratum and time period.}
  
  \item{ratio}{Experimental:Control randomization ratio}
  
  \item{alpha}{One-sided Type I error}
  
  \item{beta}{Type II error}
  
  \item{analysisTimes}{Final calendar time if beta is not NULL}
  
  \item{IF}{information fraction at planned analyses}
  
  \item{upper}{Function to produce upper bound}
  
  \item{upar}{Parameters to pass to upper}
  
  \item{lower}{Function to produce lower bound}
  
  \item{lpar}{Parameters to pass to lower}
  
  \item{r}{Control for grid size; normally leave at default of \code{r=18}}
  }
  \value{
  A list with 3 tibbles: 1) \code{enrollRates} with \code{enrollRates$rate} adjusted by sample size calculation and adding \code{N} with
  cumulative enrollment at end of each enrollment rate period,
  2) \code{failRates} as input,
  3) code{bounds} with a row for each bound and each analysis; rows contain the variables \code{Analysis} with analysis number, \code{Z} with Z-value bound,
  \code{Probability} with cumulative probability of crossing bound at each analysis under the alternate hypothesis input,
  \code{theta} standardized effect size at each analysis, \code{info} cumulative statistical information for \code{theta} at each analysis,
  \code{Time} expected timing of analysis,
  \code{avehr} expected average hazard ratio at time of analysis,
  \code{Events} expected events an time of analysis under alternate hypothesis,
  \code{info0} information under null hypothesis with same expected total events under alternate hypothesis, and
  \code{N} expected enrollment at time of analysis.
  }
  \description{
  \code{gs_design_nph()} is a flexible routine to provide a sample size or power for a fixed or
  group sequential design under
  various non-proportional hazards assumptions for either single or multiple strata studies.
  The piecewise exponential distribution allows a simple method to specify a distribtuion
  and enrollment pattern
  where the enrollment, failure and dropout rates changes over time.
  }
  \examples{
  library(dplyr)
  
  # Design
  library(dplyr)
  
  x <- gs_design_nph()
  # Notice cumulative power is 0.9 (90\%)
  x
  # Check Type I error, non-binding; should be .025 (2.5\%)
  gs_power_nph(enrollRates = x$enrollRates,
             failRates = x$failRates \%>\% mutate(hr = 1),
             events = (x$bounds \%>\% filter(Bound == "Upper"))$Events,
             upar = (x$bounds \%>\% filter(Bound == "Upper"))$Z,
             lpar = rep(-Inf,3),
             upper = gs_b,
             lower = gs_b
             ) \%>\% filter(abs(Z) < Inf)
  # Power under proportional hazards, HR = 0.75
  gs_power_nph(enrollRates = x$enrollRates,
             failRates = x$failRates \%>\% mutate(hr = .75),
             events = (x$bounds \%>\% filter(Bound == "Upper"))$Events,
             upar = (x$bounds \%>\% filter(Bound == "Upper"))$Z,
             lpar = rep(-Inf,3),
             upper = gs_b,
             lower = gs_b
             ) \%>\% filter(abs(Z) < Inf)
  
  }

Package: gsdmvn
File: man/gs_design_wlr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_design_wlr.r
  \name{gs_design_wlr}
  \alias{gs_design_wlr}
  \title{Group sequential design using weighted log-rank test under non-proportional hazards}
  \usage{
  gs_design_wlr(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    weight = wlr_weight_fh,
    approx = "asymptotic",
    alpha = 0.025,
    beta = 0.1,
    IF = NULL,
    analysisTimes = 36,
    binding = FALSE,
    upper = gs_b,
    upar = gsDesign(k = 3, test.type = 1, n.I = c(0.25, 0.75, 1), sfu = sfLDOF, sfupar =
      NULL)$upper$bound,
    lower = gs_b,
    lpar = c(qnorm(0.1), -Inf, -Inf),
    h1_spending = TRUE,
    test_upper = TRUE,
    test_lower = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{weight}{weight of weighted log rank test
  * "1"=unweighted,
  * "n"=Gehan-Breslow,
  * "sqrtN"=Tarone-Ware,
  * "FH_p[a]_q[b]"= Fleming-Harrington with p=a and q=b}
  
  \item{approx}{approximate estimation method for Z statistics
  * "event driven" = only work under propotional hazard model with log rank test
  * "asymptotic"}
  
  \item{alpha}{One-sided Type I error}
  
  \item{beta}{Type II error}
  
  \item{IF}{Targeted information fraction at each analysis}
  
  \item{analysisTimes}{Minimum time of analysis}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{h1_spending}{Indicator that lower bound to be set by spending under alternate hypothesis (input \code{failRates})
  if spending is used for lower bound}
  
  \item{test_upper}{indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound}
  
  \item{test_lower}{indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  lower bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for boundary convergence (on Z-scale)}
  }
  \description{
  Group sequential design using weighted log-rank test under non-proportional hazards
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if input analysisTimes is a positive number or a positive increasing sequence.
     \item Validate if input IF is a positive number or positive increasing sequence on (0, 1] with final value of 1.
     \item Validate if inputs IF and analysisTimes  have the same length if both have length > 1.
     \item Compute information at input analysisTimes using \code{gs_info_wlr()}.
     \item Compute sample size and bounds using \code{gs_design_npe()}.
     \item Return a list of design enrollment, failure rates, and bounds.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(dplyr)
  library(mvtnorm)
  library(gsDesign)
  
  enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500/12)
  
  failRates <- tibble::tibble(Stratum = "All",
                              duration = c(4, 100),
                              failRate = log(2) / 15,  # median survival 15 month
                              hr = c(1, .6),
                              dropoutRate = 0.001)
  
  
  x <- gsDesign::gsSurv( k = 3 , test.type = 4 , alpha = 0.025 ,
                         beta = 0.2 , astar = 0 , timing = c( 1 ) ,
                         sfu = sfLDOF , sfupar = c( 0 ) , sfl = sfLDOF ,
                         sflpar = c( 0 ) , lambdaC = c( 0.1 ) ,
                         hr = 0.6 , hr0 = 1 , eta = 0.01 ,
                         gamma = c( 10 ) ,
                         R = c( 12 ) , S = NULL ,
                         T = 36 , minfup = 24 , ratio = 1 )
  
  # User defined boundary
  gs_design_wlr(enrollRates = enrollRates, failRates = failRates,
               ratio = 1, alpha = 0.025, beta = 0.2,
               weight = function(x, arm0, arm1){
                   gsdmvn:::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)
               },
               upar = x$upper$bound,
               lpar = x$lower$bound,
               analysisTimes = c(12, 24, 36))
  
  # Boundary derived by spending function
  gs_design_wlr(enrollRates = enrollRates, failRates = failRates,
               ratio = 1, alpha = 0.025, beta = 0.2,
               weight = function(x, arm0, arm1){
                    gsdmvn:::wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0.5)
               },
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.2),
               analysisTimes = c(12, 24, 36))
  }

Package: gsdmvn
File: man/gs_info_ahr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_info_ahr.r
  \name{gs_info_ahr}
  \alias{gs_info_ahr}
  \title{Information and effect size based on AHR approximation}
  \usage{
  gs_info_ahr(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    events = NULL,
    analysisTimes = NULL
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio}
  
  \item{events}{Targeted minimum events at each analysis}
  
  \item{analysisTimes}{Targeted minimum study duration at each analysis}
  }
  \value{
  a \code{tibble} with columns \code{Analysis, Time, AHR, Events, theta, info, info0.}
  \code{info, info0} contains statistical information under H1, H0, respectively.
  For analysis \code{k}, \code{Time[k]} is the maximum of \code{analysisTimes[k]} and the expected time
  required to accrue the targeted \code{events[k]}.
  \code{AHR} is expected average hazard ratio at each analysis.
  }
  \description{
  Based on piecewise enrollment rate, failure rate, and dropout rates computes
  approximate information and effect size using an average hazard ratio model.
  }
  \details{
  The \code{AHR()} function computes statistical information at targeted event times.
  The \code{tEvents()} function is used to get events and average HR at targeted \code{analysisTimes}.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if input events is a numeric value vector or a vector with increasing values.
     \item Validate if input analysisTime is a numeric value vector or a vector with increasing values.
     \item Validate if inputs events and analysisTime have the same length if they are both specified.
     \item Compute average hazard ratio:
     \itemize{
       \item If analysisTime is specified, calculate average hazard ratio using \code{gsDesign2::AHR()}.
       \item If events is specified, calculate average hazard ratio using \code{gsDesign2::tEvents()}.
     }
     \item Return a tibble of Analysis, Time, AHR, Events, theta, info, info0.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(gsDesign)
  library(gsDesign2)
  
  # Only put in targeted events
  gs_info_ahr(events = c(30, 40, 50))
  # Only put in targeted analysis times
  gs_info_ahr(analysisTimes = c(18, 27, 36))
  # Some analysis times after time at which targeted events accrue
  # Check that both Time >= input analysisTime and Events >= input events
  gs_info_ahr(events = c(30, 40, 50), analysisTimes = c(16, 19, 26))
  gs_info_ahr(events = c(30, 40, 50), analysisTimes = c(14, 20, 24))
  }

Package: gsdmvn
File: man/gs_info_wlr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_info_wlr.R
  \name{gs_info_wlr}
  \alias{gs_info_wlr}
  \title{Information and effect size for Weighted Log-rank test}
  \usage{
  gs_info_wlr(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    events = NULL,
    analysisTimes = NULL,
    weight = wlr_weight_fh,
    approx = "asymptotic"
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio}
  
  \item{events}{Targeted minimum events at each analysis}
  
  \item{analysisTimes}{Targeted minimum study duration at each analysis}
  
  \item{weight}{weight of weighted log rank test
  * "1"=unweighted,
  * "n"=Gehan-Breslow,
  * "sqrtN"=Tarone-Ware,
  * "FH_p[a]_q[b]"= Fleming-Harrington with p=a and q=b}
  
  \item{approx}{approximate estimation method for Z statistics
  * "event driven" = only work under propotional hazard model with log rank test
  * "asymptotic"}
  }
  \value{
  a \code{tibble} with columns \code{Analysis, Time, N, Events, AHR, delta, sigma2, theta, info, info0.}
  \code{info, info0} contains statistical information under H1, H0, respectively.
  For analysis \code{k}, \code{Time[k]} is the maximum of \code{analysisTimes[k]} and the expected time
  required to accrue the targeted \code{events[k]}.
  \code{AHR} is expected average hazard ratio at each analysis.
  }
  \description{
  Based on piecewise enrollment rate, failure rate, and dropout rates computes
  approximate information and effect size using an average hazard ratio model.
  }
  \details{
  The \code{AHR()} function computes statistical information at targeted event times.
  The \code{tEvents()} function is used to get events and average HR at targeted \code{analysisTimes}.
  }

Package: gsdmvn
File: man/gs_power_ahr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_power_ahr.r
  \name{gs_power_ahr}
  \alias{gs_power_ahr}
  \title{Group sequential design power using average hazard ratio under non-proportional hazards}
  \usage{
  gs_power_ahr(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    events = c(30, 40, 50),
    analysisTimes = NULL,
    binding = FALSE,
    upper = gs_b,
    upar = gsDesign(k = length(events), test.type = 1, n.I = events, maxn.IPlan =
      max(events), sfu = sfLDOF, sfupar = NULL)$upper$bound,
    lower = gs_b,
    lpar = c(qnorm(0.1), rep(-Inf, length(events) - 1)),
    test_upper = TRUE,
    test_lower = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{events}{Targeted events at each analysis}
  
  \item{analysisTimes}{Minimum time of analysis}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{test_upper}{indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound}
  
  \item{test_lower}{indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  lower bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for boundary convergence (on Z-scale)}
  }
  \value{
  a \code{tibble} with columns \code{Analysis, Bound, Z, Probability, theta, Time, AHR, Events}.
  Contains a row for each analysis and each bound.
  }
  \description{
  Group sequential design power using average hazard ratio under non-proportional hazards
  }
  \details{
  Bound satisfy input upper bound specification in \code{upper, upar} and lower bound specification in \code{lower, lpar}.
  The \code{AHR()} function computes statistical information at targeted event times.
  The \code{tEvents()} function is used to get events and average HR at targeted \code{analysisTimes}.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Calculate information and effect size based on AHR approximation using \code{gs_info_ahr()}.
     \item Return a tibble of with columns Analysis, Bound, Z, Probability, theta,
      Time, AHR, Events and  contains a row for each analysis and each bound.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(gsDesign2)
  library(dplyr)
  
  gs_power_ahr() \%>\% filter(abs(Z) < Inf)
  
  # 2-sided symmetric O'Brien-Fleming spending bound
  # NOT CURRENTLY WORKING
  gs_power_ahr(analysisTimes = c(12, 24, 36),
                binding = TRUE,
                upper = gs_spending_bound,
                upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
                lower = gs_spending_bound,
                lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL))
  
  }

Package: gsdmvn
File: man/gs_power_combo.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_power_combo.R
  \name{gs_power_combo}
  \alias{gs_power_combo}
  \title{Group sequential design power using MaxCombo test under non-proportional hazards}
  \usage{
  gs_power_combo(
    enrollRates,
    failRates,
    fh_test,
    ratio = 1,
    binding = FALSE,
    upper = gs_b,
    upar = c(3, 2, 1),
    lower = gs_b,
    lpar = c(-1, 0, 1),
    algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),
    ...
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{fh_test}{a data frame to summarize the test in each analysis.
  Refer examples for its data structure.}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{algorithm}{ an object of class \code{\link[mvtnorm]{GenzBretz}},
                      \code{\link[mvtnorm]{Miwa}} or \code{\link[mvtnorm]{TVPACK}}
                      specifying both the algorithm to be used as well as 
                      the associated hyper parameters.}
  
  \item{...}{additional parameters transfer to `mvtnorm::pmvnorm`}
  }
  \description{
  Group sequential design power using MaxCombo test under non-proportional hazards
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Validate if lower and upper bounds have been specified.
     \item Extract info, info_fh, theta_fh and corr_fh from utility.
     \item Extract sample size via the maximum sample size of info.
     \item Calculate information fraction either for fixed or group sequential design.
     \item Compute spending function using \code{gs_bound()}.
     \item Compute probability of crossing bounds under the null and alternative
      hypotheses using \code{gs_prob_combo()}.
     \item Export required information for boundary and crossing probability
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(dplyr)
  library(mvtnorm)
  library(gsDesign)
  
  enrollRates <- tibble::tibble(Stratum = "All", duration = 12, rate = 500/12)
  
  failRates <- tibble::tibble(Stratum = "All",
                              duration = c(4, 100),
                              failRate = log(2) / 15,  # median survival 15 month
                              hr = c(1, .6),
                              dropoutRate = 0.001)
  
  fh_test <- rbind( data.frame(rho = 0, gamma = 0, tau = -1,
                               test = 1,
                               Analysis = 1:3,
                               analysisTimes = c(12, 24, 36)),
                    data.frame(rho = c(0, 0.5), gamma = 0.5, tau = -1,
                               test = 2:3,
                               Analysis = 3, analysisTimes = 36)
  )
  
  # User defined bound
  gs_power_combo(enrollRates, failRates, fh_test, upar = c(3,2,1), lpar = c(-1, 0, 1))
  
  # Minimal Information Fraction derived bound
  gs_power_combo(enrollRates, failRates, fh_test,
                 upper = gs_spending_combo,
                 upar  = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
                 lower = gs_spending_combo,
                 lpar  = list(sf = gsDesign::sfLDOF, total_spend = 0.2))
  
  }

Package: gsdmvn
File: man/gs_power_npe.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_power_npe.r
  \name{gs_power_npe}
  \alias{gs_power_npe}
  \title{Group sequential bound computation with non-constant effect}
  \usage{
  gs_power_npe(
    theta = 0.1,
    theta1 = NULL,
    info = 1,
    info1 = NULL,
    info0 = NULL,
    binding = FALSE,
    upper = gs_b,
    lower = gs_b,
    upar = qnorm(0.975),
    lpar = -Inf,
    test_upper = TRUE,
    test_lower = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{theta}{natural parameter for group sequential design representing
  expected incremental drift at all analyses; used for power calculation}
  
  \item{theta1}{natural parameter for alternate hypothesis, if needed for lower bound computation}
  
  \item{info}{statistical information at all analyses for input \code{theta}}
  
  \item{info1}{statistical information under hypothesis used for futility bound calculation if different from
  \code{info}; impacts futility hypothesis bound calculation}
  
  \item{info0}{statistical information under null hypothesis, if different than \code{info};
  impacts null hypothesis bound calculation}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{function to compute upper bound}
  
  \item{lower}{function to compare lower bound}
  
  \item{upar}{parameter to pass to upper}
  
  \item{lpar}{parameter to pass to lower}
  
  \item{test_upper}{indicator of which analyses should include an upper (efficacy) bound;
  single value of TRUE (default)  indicates all analyses; otherwise,
  a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound}
  
  \item{test_lower}{indicator of which analyses should include a lower bound;
  single value of TRUE (default) indicates all analyses;
  single value FALSE indicated no lower bound; otherwise,
  a logical vector of the same length as \code{info} should indicate which analyses will have a lower bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for boundary convergence (on Z-scale)}
  }
  \description{
  \code{gs_power_npe()} derives group sequential bounds and boundary crossing probabilities for a design.
  It allows a non-constant treatment effect over time, but also can be applied for the usual homogeneous effect size designs.
  It requires treatment effect and statistical information at each analysis as well as a method of deriving bounds, such as spending.
  The routine enables two things not available in the gsDesign package: 1) non-constant effect, 2) more flexibility in boundary selection.
  For many applications, the non-proportional-hazards design function \code{gs_design_nph()} will be used; it calls this function.
  Initial bound types supported are 1) spending bounds, 2) fixed bounds, and 3) Haybittle-Peto-like bounds.
  The requirement is to have a boundary update method that can each bound without knowledge of future bounds.
  As an example, bounds based on conditional power that require knowledge of all future bounds are not supported by this routine;
  a more limited conditional power method will be demonstrated.
  Boundary family designs Wang-Tsiatis designs including the original (non-spending-function-based) O'Brien-Fleming and Pocock designs
  are not supported by \code{gs_power_npe()}.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Extract the length of input info as the number of interim analysis.
     \item Validate if input info0 is NULL, so set it equal to info.
     \item Validate if input info1 is NULL, so set it equal to info.
     \item Validate if the length of inputs info, info0, and info1 are the same.
     \item Validate if input theta is a scalar, so replicate the value for all k interim analysis.
     \item Validate if input theta1 is NULL and if it is a scalar. If it is NULL,
     set it equal to input theta. If it is a scalar, replicate the value for all k interim analysis.
     \item Validate if input test_upper is a scalar, so replicate the value for all k interim analysis.
     \item Validate if input test_lower is a scalar, so replicate the value for all k interim analysis.
     \item Define vector a to be -Inf with length equal to the number of interim analysis.
     \item Define vector b to be Inf with length equal to the number of interim analysis.
     \item Define hgm1_0 and hgm1 to be NULL.
     \item Define upperProb and lowerProb to be vectors of NA with length of the number of interim analysis.
     \item Update lower and upper bounds using \code{gs_b()}.
     \item If there are no interim analysis, compute proabilities of crossing upper and lower bounds
     using \code{h1()}.
     \item Compute cross upper and lower bound probabilities using \code{hupdate()} and \code{h1()}.
     \item Return a tibble of analysis number, Bounds, Z-values, Probability of crossing bounds,
     theta, theta1, info, info0, and info1
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  
  library(gsDesign)
  library(dplyr)
  
  # Default (single analysis; Type I error controlled)
  gs_power_npe(theta=0) \%>\% filter(Bound=="Upper")
  
  # Fixed bound
  gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40, info0 = (1:3) * 40,
                upper = gs_b, upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,
                lower = gs_b, lpar = c(-1, 0, 0))
  
  # Same fixed efficacy bounds, no futility bound (i.e., non-binding bound), null hypothesis
  gs_power_npe(theta = rep(0,3), info = (1:3) * 40,
  upar = gsDesign::gsDesign(k=3,sfu=gsDesign::sfLDOF)$upper$bound,
  lpar = rep(-Inf, 3)) \%>\% filter(Bound=="Upper")
  
  # Fixed bound with futility only at analysis 1; efficacy only at analyses 2, 3
  gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40, 
               upar = c(Inf, 3, 2), lpar = c(qnorm(.1), -Inf, -Inf))
  
  # Spending function bounds
  # Lower spending based on non-zero effect
  gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1, param = -1, timing = NULL))
  
  # Same bounds, but power under different theta
  gs_power_npe(theta = c(.15, .25, .35), theta1 = c(.1, .2, .3), info = (1:3) * 40,
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfHSD, total_spend = 0.1, param = -1, timing = NULL))
  
  # Two-sided symmetric spend, O'Brien-Fleming spending
  # Typically, 2-sided bounds are binding
  xx <- gs_power_npe(theta = rep(0, 3), theta1 = rep(0, 3), info = (1:3) * 40,
                     upper = gs_spending_bound,
                     binding = TRUE,
                     upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
                     lower = gs_spending_bound,
                     lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL))
  xx
  
  # Re-use these bounds under alternate hypothesis
  # Always use binding = TRUE for power calculations
  upar <- (xx \%>\% filter(Bound=="Upper"))$Z
  gs_power_npe(theta = c(.1, .2, .3), info = (1:3) * 40,
               binding = TRUE,
               upar = upar,
               lpar = -upar)
  
  }
  \author{
  Keaven Anderson \email{keaven\_anderson@merck.}
  }

Package: gsdmvn
File: man/gs_power_nph.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_power_nph.r
  \name{gs_power_nph}
  \alias{gs_power_nph}
  \title{Group sequential design power under non-proportional hazards}
  \usage{
  gs_power_nph(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    events = c(30, 40, 50),
    analysisTimes = NULL,
    maxEvents = 45,
    upper = gs_b,
    upar = gsDesign(k = length(events), test.type = 1, n.I = events, maxn.IPlan =
      maxEvents, sfu = sfLDOF, sfupar = NULL)$upper$bound,
    lower = gs_b,
    lpar = c(qnorm(0.1), rep(-Inf, 2)),
    r = 18
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{events}{Targeted events at each analysis}
  
  \item{analysisTimes}{Not yet implemented}
  
  \item{maxEvents}{Final planned events}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{r}{Control for grid size; normally leave at default of \code{r=18}}
  }
  \value{
  a \code{tibble} with columns Analysis, Bound, Z, Probability, theta, Time, avehr, Events
  }
  \description{
  Group sequential design power under non-proportional hazards
  }
  \details{
  Need to be added
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Compute average hazard ratio using \code{gs_info_ahr()}.
     \item Calculate the probability of crossing bounds using \code{gs_prob()}.
     \item Combine the probability of crossing bounds and average hazard ratio and return a
     tibble  with columns Analysis, Bound, Z, Probability, theta, Time, avehr, and Events.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(gsDesign)
  library(gsDesign2)
  library(dplyr)
  
  gs_power_nph() \%>\% filter(abs(Z) < Inf)
  }

Package: gsdmvn
File: man/gs_power_wlr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_power_wlr.r
  \name{gs_power_wlr}
  \alias{gs_power_wlr}
  \title{Group sequential design power using weighted log rank test under non-proportional hazards}
  \usage{
  gs_power_wlr(
    enrollRates = tibble::tibble(Stratum = "All", duration = c(2, 2, 10), rate = c(3, 6,
      9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    ratio = 1,
    weight = wlr_weight_fh,
    approx = "asymptotic",
    events = c(30, 40, 50),
    analysisTimes = NULL,
    binding = FALSE,
    upper = gs_b,
    upar = gsDesign(k = length(events), test.type = 1, n.I = events, maxn.IPlan =
      max(events), sfu = sfLDOF, sfupar = NULL)$upper$bound,
    lower = gs_b,
    lpar = c(qnorm(0.1), rep(-Inf, length(events) - 1)),
    test_upper = TRUE,
    test_lower = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{enrollRates}{enrollment rates}
  
  \item{failRates}{failure and dropout rates}
  
  \item{ratio}{Experimental:Control randomization ratio (not yet implemented)}
  
  \item{weight}{weight of weighted log rank test
  * "1"=unweighted,
  * "n"=Gehan-Breslow,
  * "sqrtN"=Tarone-Ware,
  * "FH_p[a]_q[b]"= Fleming-Harrington with p=a and q=b}
  
  \item{approx}{approximate estimation method for Z statistics
  * "event driven" = only work under propotional hazard model with log rank test
  * "asymptotic"}
  
  \item{events}{Targeted events at each analysis}
  
  \item{analysisTimes}{Minimum time of analysis}
  
  \item{binding}{indicator of whether futility bound is binding; default of FALSE is recommended}
  
  \item{upper}{Function to compute upper bound}
  
  \item{upar}{Parameter passed to \code{upper()}}
  
  \item{lower}{Function to compute lower bound}
  
  \item{lpar}{Parameter passed to \code{lower()}}
  
  \item{test_upper}{indicator of which analyses should include an upper (efficacy) bound; single value of TRUE (default) indicates all analyses;
  otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have an efficacy bound}
  
  \item{test_lower}{indicator of which analyses should include an lower bound; single value of TRUE (default) indicates all analyses;
  single value FALSE indicated no lower bound; otherwise, a logical vector of the same length as \code{info} should indicate which analyses will have a
  lower bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for boundary convergence (on Z-scale)}
  }
  \description{
  Group sequential design power using weighted log rank test under non-proportional hazards
  }
  \section{Specification}{
  
  
  \if{latex}{
   \itemize{
     \item Compute information and effect size for Weighted Log-rank test using \code{gs_info_wlr()}.
     \item Compute group sequential bound computation with non-constant effect using \code{gs_power_npe()}.
     \item Combine information and effect size and power and return a
     tibble  with columns Analysis, Bound, Time, Events, Z, Probability, AHR,  theta, info, and info0.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  

Package: gsdmvn
File: man/gs_prob.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_prob.r
  \name{gs_prob}
  \alias{gs_prob}
  \title{Group sequential boundary crossing probabilities}
  \usage{
  gs_prob(theta, upper = gs_b, lower = gs_b, upar, lpar, info, r = 18)
  }
  \arguments{
  \item{theta}{natural parameter for group sequentia design representing expected drift at time of each analysis}
  
  \item{upper}{function to compute upper bound}
  
  \item{lower}{function to compare lower bound}
  
  \item{upar}{parameter to pass to upper}
  
  \item{lpar}{parameter to pass to lower}
  
  \item{info}{statistical information at each analysis}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  }
  \value{
  A `tibble` with a row for each finite bound and analysis containing the following variables:
  Analysis analysis number
  Bound Upper (efficacy) or Lower (futility)
  Z Z-value at bound
  Probability probability that this is the first bound crossed under the given input
  theta approximate natural parameter value required to cross the bound
  }
  \description{
  Group sequential boundary crossing probabilities
  }
  \details{
  Approximation for \code{theta} is based on Wald test and assumes the observed information is equal to the expected.
  }
  \examples{
  library(dplyr)
  # Asymmetric 2-sided design
  gs_prob(theta = 0, upar = rep(2.2, 3), lpar = rep(0, 3), 
          upper=gs_b, lower=gs_b,  info = 1:3)
  # One-sided design
  x <- gs_prob(theta = 0, upar = rep(2.2, 3), lpar = rep(-Inf, 3), 
               upper=gs_b, lower=gs_b,  info = 1:3)
  # Without filtering, this shows unneeded lower bound
  x
  # Filter to just show bounds intended for use
  x \%>\% filter(abs(Z) < Inf)
  }

Package: gsdmvn
File: man/gs_prob_combo.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_prob_combo.R
  \name{gs_prob_combo}
  \alias{gs_prob_combo}
  \title{MaxCombo Group sequential boundary crossing probabilities}
  \usage{
  gs_prob_combo(
    upper_bound,
    lower_bound,
    analysis,
    theta,
    corr,
    algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),
    ...
  )
  }
  \arguments{
  \item{upper_bound}{a numeric vector of upper bound}
  
  \item{lower_bound}{a numeric vector of lower bound}
  
  \item{analysis}{an integer vector of the interim analysis index}
  
  \item{theta}{a numeric vector of effect size under alternative hypothesis}
  
  \item{corr}{a matrix of correlation matrix}
  
  \item{algorithm}{ an object of class \code{\link[mvtnorm]{GenzBretz}},
                      \code{\link[mvtnorm]{Miwa}} or \code{\link[mvtnorm]{TVPACK}}
                      specifying both the algorithm to be used as well as 
                      the associated hyper parameters.}
  
  \item{...}{additional parameters transfer to `mvtnorm::pmvnorm`}
  }
  \description{
  MaxCombo Group sequential boundary crossing probabilities
  }

Package: gsdmvn
File: man/gs_spending_bound.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_spending_bound.r
  \name{gs_spending_bound}
  \alias{gs_spending_bound}
  \title{Derive spending bound for group sequential boundary}
  \usage{
  gs_spending_bound(
    k = 1,
    par = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL,
      max_info = NULL),
    hgm1 = NULL,
    theta = 0.1,
    info = 1:3,
    efficacy = TRUE,
    test_bound = TRUE,
    r = 18,
    tol = 1e-06
  )
  }
  \arguments{
  \item{k}{analysis for which bound is to be computed}
  
  \item{par}{a list with the following items:
  \code{sf} (class spending function),
  \code{total_spend} (total spend),
  \code{param} (any parameters needed by the spending function \code{sf()}),
  \code{timing} (a vector containing values at which spending function is to be evaluated or NULL if information-based spending is used),
  \code{max_info} (when \code{timing} is NULL, this can be input as positive number to be used with \code{info} for information fraction at each analysis)}
  
  \item{hgm1}{subdensity grid from h1 (k=2) or hupdate (k>2) for analysis k-1; if k=1, this is not used and may be NULL}
  
  \item{theta}{natural parameter used for lower bound only spending;
  represents average drift at each time of analysis at least up to analysis k;
  upper bound spending is always set under null hypothesis (theta = 0)}
  
  \item{info}{statistical information at all analyses, at least up to analysis k}
  
  \item{efficacy}{TRUE (default) for efficacy bound, FALSE otherwise}
  
  \item{test_bound}{a logical vector of the same length as \code{info} should indicate which analyses will have a bound}
  
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{tol}{Tolerance parameter for convergence (on Z-scale)}
  }
  \value{
  returns a numeric bound (possibly infinite) or, upon failure, generates an error message.
  }
  \description{
  Computes one bound at a time based on spending under given distributional assumptions.
  While user specifies \code{gs_spending_bound()} for use with other functions,
  it is not intended for use on its own.
  Most important user specifications are made through a list provided to functions using \code{gs_spending_bound()}.
  Function uses numerical integration and Newton-Raphson iteration to derive an individual bound for a group sequential
  design that satisfies a targeted boundary crossing probability.
  Algorithm is a simple extension of that in Chapter 19 of Jennison and Turnbull (2000).
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Set the spending time at analysis.
     \item Compute the cumulative spending at analysis.
     \item Compute the incremental spend at each analysis.
     \item Set test_bound a vector of length k > 1 if input as a single value.
     \item Compute spending for current bound.
     \item Iterate to convergence as in gsbound.c from gsDesign.
     \item Compute subdensity for final analysis in rejection region.
     \item Validate the output and return an error message in case of failure.
     \item Return a numeric bound (possibly infinite).
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \references{
  Jennison C and Turnbull BW (2000), \emph{Group Sequential
  Methods with Applications to Clinical Trials}. Boca Raton: Chapman and Hall.
  }
  \author{
  Keaven Anderson \email{keaven\_anderson@merck.}
  }

Package: gsdmvn
File: man/gs_spending_combo.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gs_spending_combo.R
  \name{gs_spending_combo}
  \alias{gs_spending_combo}
  \title{Derive spending bound for MaxCombo group sequential boundary}
  \usage{
  gs_spending_combo(par = NULL, info = NULL, ...)
  }
  \arguments{
  \item{par}{a list with the following items:
  \code{sf} (class spending function),
  \code{total_spend} (total spend),
  \code{param} (any parameters needed by the spending function \code{sf()}),
  \code{timing} (a vector containing values at which spending function is to be evaluated or NULL if information-based spending is used),
  \code{max_info} (when \code{timing} is NULL, this can be input as positive number to be used with \code{info} for information fraction at each analysis)}
  
  \item{info}{statistical information at all analyses, at least up to analysis k}
  
  \item{...}{additional parameters transfered to `par$sf`.}
  }
  \description{
  Derive spending bound for MaxCombo group sequential boundary
  }
  \examples{
  
  # alpha-spending
  par <- list(sf = gsDesign::sfLDOF, total_spend = 0.025)
  gs_spending_combo(par, info = 1:3/3)
  
  # beta-spending
  par <- list(sf = gsDesign::sfLDOF, total_spend = 0.2)
  gs_spending_combo(par, info = 1:3/3)
  
  }

Package: gsdmvn
File: man/gsdmvn.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/gsdmvn.r
  \docType{package}
  \name{gsdmvn}
  \alias{gsdmvn}
  \title{gsdmvn: A package for group sequential design under non-proportional hazards (NPH)}
  \description{
  The gsdmvn package will eventually be incorporated into the gsDesign2 package.
  This version is for the Regulatory/ASA Biopharmaceutical Subsection training course in September, 2020
  The package computes the asymptotic normal distribution for group sequential designs,
  generalizing the theory presented by Jennison and Turnbull (2000) to cases with
  non-homogeneous treatment effect over time.
  The primary application for this is group sequential design under the assumption of non-proportional hazards.
  The gsdmvn package has 4 types of functions
  1) support for asymptotic normal distribution computation,
  2) support for group sequential bound derivation, and
  3) support for design and power calculations.
  4) applications to designs for survival analysis under non-proportional hazards assumptions.
  }
  \details{
  In addition to the above function categeories, vignettes show how to implement
  1) design for a binomial endpoint as an example of how to extend the package to other endpoint types,
  2) the extensive capabilities around group sequential boundary calculations,
  including enabling capabilities not in the gsDesign package.
  }
  \section{gsdmvn functions}{
  
  The primary functions supporting non-proportional hazards in the short course are:
  \itemize{
  \item gs_design_ahr - derive group sequential design under non-proportional hazards (NPH) for logrank test
  \item gs_power_ahr - compute power for a group sequential design under non-proportional hazards for logrank test
  }
  
  Key supportive functions specify bound derivation for designs:
  \itemize{
  \item gs_b - directly provide bounds for designs
  \item gs_spending_bound - provide bounds based on a spending function (e.g., Lan-DeMets O'Brien-Fleming)
  }
  
  Underlying functions to support numerical integration that should not be directly needed by typical users are
  \itemize{
  \item gridpts - set up grid points and weights for numerical integration for a normal distribution
  \item h1 - initialize numerical integration grid points and weights for NPH for first analysis
  \item hupdate - update numerical integration grid points and weights for NPH from one interim to the next
  \item gs_power_npe - general non-constant-effect size boundary crossing probability calculation for group sequential design
  }
  }
  
  \references{
  Jennison C and Turnbull BW (2000), \emph{Group Sequential
  Methods with Applications to Clinical Trials}. Boca Raton: Chapman and Hall.
  }
  \author{
  Keaven Anderson \email{keaven\_anderson@merck.}
  }
  \keyword{design}
  \keyword{nonparametric}
  \keyword{survival}

Package: gsdmvn
File: man/h1.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/h1.r
  \name{h1}
  \alias{h1}
  \title{Initialize numerical integration for group sequential design}
  \usage{
  h1(r = 18, theta = 0, I = 1, a = -Inf, b = Inf)
  }
  \arguments{
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{theta}{Drift parameter for first analysis}
  
  \item{I}{Information at first analysis}
  
  \item{a}{lower limit of integration (scalar)}
  
  \item{b}{upper limit of integration (scalar \code{> a})}
  }
  \value{
  A \code{tibble} with grid points in \code{z}, numerical integration weights in \code{w},
  and a normal density with mean \code{mu = theta * sqrt{I}} and variance 1 times the weight in \code{w}.
  }
  \description{
  Compute grid points for first interim analysis in a group sequential design
  }
  \details{
  Mean for standard normal distribution under consideration is \code{mu = theta * sqrt(I)}
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Compute drift at analysis 1.
     \item Compute deviation from drift.
     \item Compute standard normal density, multiply by grid weight.
     \item Return a tibble of z, w, and h.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(dplyr)
  # Replicate variance of 1, mean of 35
  h1(theta = 5, I = 49) \%>\% summarise(mu = sum(z * h), var = sum((z - mu)^2 * h))
  
  # Replicate p-value of .0001 by numerical integration of tail
  h1(a = qnorm(.9999)) \%>\% summarise(p = sum(h))
  }

Package: gsdmvn
File: man/hupdate.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/hupdate.r
  \name{hupdate}
  \alias{hupdate}
  \title{Update numerical integration for group sequential design}
  \usage{
  hupdate(
    r = 18,
    theta = 0,
    I = 2,
    a = -Inf,
    b = Inf,
    thetam1 = 0,
    Im1 = 1,
    gm1 = h1()
  )
  }
  \arguments{
  \item{r}{Integer, at least 2; default of 18 recommended by Jennison and Turnbull}
  
  \item{theta}{Drift parameter for current analysis}
  
  \item{I}{Information at current analysis}
  
  \item{a}{lower limit of integration (scalar)}
  
  \item{b}{upper limit of integration (scalar \code{> a})}
  
  \item{thetam1}{Drift parameter for previous analysis}
  
  \item{Im1}{Information at previous analysis}
  
  \item{gm1}{numerical integration grid from \code{h1()} or previous run of \code{hupdate()}}
  }
  \value{
  A \code{tibble} with grid points in \code{z}, numerical integration weights in \code{w},
  and a normal density with mean \code{mu = theta * sqrt{I}} and variance 1 times the weight in \code{w}.
  }
  \description{
  Update grid points for numerical integration from one analysis to the next
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Compute the square root of the change in information.
     \item Compute the grid points for group sequential design numerical integration.
     \item Update the integration.
     \item Return a tibble of z, w, and h.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  
  \examples{
  library(dplyr)
  # 2nd analysis with no interim bound and drift 0 should have mean 0, variance 1
  hupdate() \%>\% summarise(mu = sum(z * h), var = sum((z - mu)^2 * h))
  }

Package: gsdmvn
File: man/pmvnorm_combo.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/pmvnorm_combo.R
  \name{pmvnorm_combo}
  \alias{pmvnorm_combo}
  \title{Multivariate Normal Distribution for Multivariate Maximum Statistics}
  \usage{
  pmvnorm_combo(
    lower,
    upper,
    group,
    mean,
    corr,
    algorithm = GenzBretz(maxpts = 1e+05, abseps = 1e-05),
    ...
  )
  }
  \arguments{
  \item{lower}{ the vector of lower limits of length n.}
  
  \item{upper}{ the vector of upper limits of length n.}
  
  \item{group}{the vector of test statistics group.}
  
  \item{mean}{ the mean vector of length n.}
  
  \item{corr}{ the correlation matrix of dimension n.}
  
  \item{algorithm}{ an object of class \code{\link[mvtnorm]{GenzBretz}},
                      \code{\link[mvtnorm]{Miwa}} or \code{\link[mvtnorm]{TVPACK}}
                      specifying both the algorithm to be used as well as 
                      the associated hyper parameters.}
  
  \item{...}{additional parameters transfer to `mvtnorm::pmvnorm`}
  }
  \description{
  Computes the distribution function of the multivariate normal distribution
  with maximum statistics for arbitrary limits and correlation matrices
  }
  \details{
  Let $Z = {Z_ij}$ be a multivariate normal distribution.
  Here i is a group indicator and j is a within group statistics indicator.
  Let G_i = max({Z_ij}) for all test within one group.
  This program are calculating the probability
  
    $$Pr( lower < max(G) < upper )$$
  }

Package: gsdmvn
File: man/wlr_weight.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/wlr_weight.R
  \name{wlr_weight}
  \alias{wlr_weight}
  \alias{wlr_weight_fh}
  \alias{wlr_weight_1}
  \alias{wlr_weight_n}
  \title{Weight Function of Weighted Log-rank Test}
  \usage{
  wlr_weight_fh(x, arm0, arm1, rho = 0, gamma = 0, tau = NULL)
  
  wlr_weight_1(x, arm0, arm1)
  
  wlr_weight_n(x, arm0, arm1, power = 1)
  }
  \arguments{
  \item{x}{analysis time}
  
  \item{arm0}{an "arm" object defined in `npsurvSS` package}
  
  \item{arm1}{an "arm" object defined in `npsurvSS` package}
  
  \item{rho}{A scalar parameter that controls the type of test}
  
  \item{gamma}{A scalar parameter that controls the type of test}
  
  \item{tau}{A scalar parameter of the cut-off time for modest weighted log rank test}
  
  \item{power}{A scalar parameter that controls the power of the weight function}
  }
  \description{
  * `wlr_weight_fh` is Fleming-Harriongton, FH(rho, gamma) weight function.
  * `wlr_weight_1`  is constant for log rank test
  * `wlr_weight_power` is Gehan-Breslow and Tarone-Ware weight function.
  }
  \section{Specification}{
  
  \if{latex}{
   \itemize{
     \item Compute the sample size via the sum of arm sizes.
     \item Compute the proportion of size in the two arms.
     \item If the input tau is specified, define time up to the cut off time tau.
     \item Compute the CDF using the proportion of the size in the two arms and \code{npsruvSS::psurv()}.
     \item Return the Fleming-Harriongton weights for weighted Log-rank test.
    }
  }
  \if{html}{The contents of this section are shown in PDF user manual only.}
  }
  

Package: gsdmvn
File: vignettes/DesignWithAverageHazardRatio.Rmd
Format: text
Content:
  ---
  title: "Design Using Average Hazard Ratio"
  author: "Keaven M. Anderson"
  date: "12/21/2020"
  output:
    html_document:
      code_folding: hide
      toc: true
      toc_depth: 2
  bibliography: gsDesign.bib
  vignette: >
    %\VignetteIndexEntry{Design Using Average Hazard Ratio}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r setup, include=FALSE, message=FALSE, warning=FALSE}
  knitr::opts_chunk$set(echo = TRUE)
  ```
  
  ```{r packages, message=FALSE, warning=FALSE}
  # packages used
  library(gsDesign)
  library(gsDesign2)
  library(gsdmvn)
  library(ggplot2)
  library(dplyr)
  library(knitr)
  library(kableExtra)
  library(gt)
  library(tidyr)
  ```
  
  ## Introduction
  
  @JTBook
  
  ## Scenarios
  
  Expected enrollment duration is 24 months with piecewise constant enrollment rates escalating every 2 months until month 6 where enrollment is assumed to have reached steady state.
  For alternate scenarios, enrollment will occur at a faster than planned rate.
  For both scenarios, we assume for initial illustrations that the total enrollment is 100 subjects.
  
  ```{r}
  # 6 month ramp-up of enrollment, 24 months enrollment time target
  enroll24 <- tibble::tibble(Stratum = rep("All",4), 
                             duration = c(rep(2,3), 18),
                             rate = 1:4)
  # Set rates to enroll 100 subjects
  N <- sum(enroll24$duration * enroll24$rate)
  enroll24$rate <- enroll24$rate * 100 / N
  
  # Enroll in 16 months, same ramp-up
  enroll16 <- tibble::tibble(Stratum = rep("All",4), 
                             duration = c(rep(2,3), 12),
                             rate = 1:4)
  # Set rates to enroll 100 subjects
  N <- sum(enroll16$duration * enroll16$rate)
  enroll16$rate <- enroll16$rate * 100 / N
  # Put these in a single tibble by scenario
  # We will use 16 month enrollment for delayed effect and crossing hazards
  # scenarios
  enrollRates <- rbind(enroll24 %>% mutate(Scenario = "PH"),
                       enroll24 %>% mutate(Scenario = "Delayed effect 1"),
                       enroll16 %>% mutate(Scenario = "Delayed effect 2"),
                       enroll16 %>% mutate(Scenario = "Crossing")
  )
  ```
  
  We will consider the following failure rate assumptions:
  
  - PH: Proportional hazards is assumed.
      - Control group has exponential failure rate with a median of 12 months.
      - Constant hazard ratio of 0.7 (experimental/control).
  - Delayed effect
      - Control group has exponential failure rate with a median of 10 months.
      - Hazard ratio of 1 for 6 months followed by a hazard ratio of 0.575.
  - Crossing hazards
      - Control group has exponential failure rate with a median of 10 months.
      - Hazard ratio of 1.5 for 4 months followed by a hazard ratio of 0.5.
      
  Survival curves for these 3 scenarios are shown below:
  
  ```{r}
  Month <- c(0,4,6,44)
  duration <- Month - c(0,Month[1:3])
  control_rate <- log(2) / c(rep(16,4), rep(14, 4), rep(14, 4))
  s <- tibble::tibble(Scenario = c(rep("PH",4), rep("Delayed effect", 4), rep("Crossing", 4)),
                      Treatment = rep("Control", 12),
                      Month = rep(Month, 3),
                      duration = rep(duration, 3),
                      rate = control_rate,
                      hr = c(rep(.7, 4), c(1, 1, 1, .575), c(1.5,1.5, .5, .5)),
  )
  s <- rbind(s,
             s %>% mutate(Treatment = "Experimental",
                          rate = rate * hr)
  ) %>%
    group_by(Scenario, Treatment) %>%
    mutate(Survival = exp(-cumsum(duration * rate)))
  ggplot(s, aes(x = Month, y = Survival, col = Scenario, lty = Treatment)) + 
    geom_line() +
    scale_y_log10(breaks = (1:10) /10, lim=c(.1,1))+
    scale_x_continuous(breaks = seq(0,42, 6))
  ```
      
  ## Average Hazard Ratio
  
  ```{r}
  # Durations to be used in common for all failure rate scenarios
  dur <- c(4,2,100)
  # Exponential failure, proportional hazards
  failRates <- rbind(tibble(Scenario = "PH", Stratum = "All", 
                            duration = dur, failRate = log(2) / 14,
                            hr = 0.7, dropoutRate = .001),
                     tibble(Scenario = "Delayed effect 1", Stratum = "All", 
                            duration = dur, failRate = log(2) / 11,
                            hr = c(1, .6, .6), dropoutRate = .001),
                     tibble(Scenario = "Delayed effect 2", Stratum = "All", 
                            duration = dur, failRate = log(2) / 11,
                            hr = c(1, 1, .7), dropoutRate = .001),
                     tibble(Scenario = "Crossing", Stratum = "All", 
                            duration = dur, failRate = log(2) / 11,
                            hr = c(1.5, .6,.6), dropoutRate = .001)
  )
  hr <- NULL
  for(g in c("PH", "Delayed effect 1", "Delayed effect 2", "Crossing")){
    hr <- 
      rbind(hr,
          AHR(enrollRates = enrollRates %>% filter(Scenario == g), 
              failRates = failRates %>% filter(Scenario == g),
              totalDuration = c(.001, seq(4, 44, 4))
          ) %>%
          mutate(Scenario = g)
    )
  }
  ```
  
  ```{r} 
  ggplot(hr, aes(x=Time, y=AHR, col = Scenario)) + geom_line() + scale_x_continuous(breaks = seq(0, 42, 6))
  ```
  
  ```{r}
  ggplot(hr, aes(x=Time, y=`Events`, col = Scenario)) + geom_line() + scale_x_continuous(breaks = seq(0, 42, 6))
  ```
  
  
  ## Sample Size and Events by Scenario
  
  ### Fixed design using AHR and logrank
  
  We power a fixed design at 90% with 2.5% one-sided Type I error under the different scenarios under consideration.
  
  ```{r}
  ss_ahr_fixed <- NULL
  for(g in c("PH", "Delayed effect 1","Delayed effect 2", "Crossing")){
      ss_ahr_fixed <- 
        rbind(ss_ahr_fixed,
        gs_design_ahr(enrollRates = enrollRates %>% filter(Scenario == g),
                      failRates = failRates %>% filter(Scenario == g),
                      analysisTimes = 36,
                      upper = gs_b, upar = qnorm(.975),
                      lower = gs_b, lpar = -Inf,
                      alpha = .025,
                      beta = .1
                      )$bounds %>% mutate(Scenario = g)
        )
  }
  ss_ahr_fixed %>% select(Time, N, Events, AHR, Scenario) %>% 
    gt() %>% fmt_number(columns=1:3,decimals = 0) %>% fmt_number(columns = 4, decimals = 3)  %>%
    tab_header(title = "Sample Size and Events Required by Scenario",
               subtitle = "36 Month Trial duration, 2.5% One-sided Type 1 Error, 90% Power")
  ```
  
  Assuming delayed effect 1 is the primary scenario for which we wish to protect power, how long should the trial be to optimize the tradeoffs between sample size, AHR and events required?
  We will inform this tradeoff by looking sizing the trial for different assumed trial durations with the same failure rates and assumed relative enrollment rates.
  The counts of events required is perhaps the most interesting here in that a 24 month trial requires almost twice the events to be powered at 90% compared to a trial of 42 months duration.
  For further study, we will consider the 36 month trial duration as a reasonable tradeoff between time, sample size and power under a presumed delayed effect of 4 months followed by a hazard ratio of 0.6 thereafter. 
  
  ```{r}
  ss_ahr_fixed <- NULL
  g <- "Delayed effect 1"
  for(trialEnd in c(24,30,36,42)){
      ss_ahr_fixed <- 
        rbind(ss_ahr_fixed,
        gs_design_ahr(enrollRates = enrollRates %>% filter(Scenario == g),
                      failRates = failRates %>% filter(Scenario == g),
                      analysisTimes = trialEnd,
                      upper = gs_b, upar = qnorm(.975),
                      lower = gs_b, lpar = -Inf,
                      alpha = .025,
                      beta = .1
                      )$bounds %>% mutate(Scenario = g)
        )
  }
  ss_ahr_fixed %>% select(Time, N, Events, AHR, Scenario) %>% 
    gt() %>% fmt_number(columns=1:3,decimals = 0) %>% fmt_number(columns = 4, decimals = 3) %>%
    tab_header(title = "Sample Size and Events Required by Trial Duration",
               subtitle = "Delayed Effect of 4 Months, HR = 0.6 Thereafter; 90% Power")
  ```
  
  
  ### Alternate hypothesis mapping
  
  
  Experimental version of `AHR()`.
  
  ```{r}
  AHRx <- function(enrollRates=tibble::tibble(Stratum="All",
                                             duration=c(2,2,10),
                                             rate=c(3,6,9)),
                  failRates=tibble::tibble(Stratum="All",
                                           duration=c(3,100),
                                           failRate=log(2)/c(9,18),
                                           hr=c(.9,.6),
                                           dropoutRate=rep(.001,2)),
                  totalDuration=30,
                  ratio=1,
                  simple=TRUE
  ){
    # check input values
    # check input enrollment rate assumptions
    if(max(names(enrollRates)=="Stratum") != 1){stop("gsDesign2: enrollRates column names in `AHR()` must contain stratum")}
    if(max(names(enrollRates)=="duration") != 1){stop("gsDesign2: enrollRates column names in `AHR()` must contain duration")}
    if(max(names(enrollRates)=="rate") != 1){stop("gsDesign2: enrollRates column names in `AHR()' must contain rate")}
  
    # check input failure rate assumptions
    if(max(names(failRates)=="Stratum") != 1){stop("gsDesign2: failRates column names in `AHR()` must contain stratum")}
    if(max(names(failRates)=="duration") != 1){stop("gsDesign2: failRates column names in `AHR()` must contain duration")}
    if(max(names(failRates)=="failRate") != 1){stop("gsDesign2: failRates column names in `AHR()` must contain failRate")}
    if(max(names(failRates)=="hr") != 1){stop("gsDesign2: failRates column names in `AHR()` must contain hr")}
    if(max(names(failRates)=="dropoutRate") != 1){stop("gsDesign2: failRates column names in `AHR()` must contain dropoutRate")}
  
    # check input trial durations
    if(!is.numeric(totalDuration)){stop("gsDesign2: totalDuration in `AHR()` must be a non-empty vector of positive numbers")}
    if(!is.vector(totalDuration) > 0){stop("gsDesign2: totalDuration in `AHR()` must be a non-empty vector of positive numbers")}
    if(!min(totalDuration) > 0){stop("gsDesign2: totalDuration in `AHR()` must be greater than zero")}
    strata <- names(table(enrollRates$Stratum))
    strata2 <- names(table(failRates$Stratum))
    length(strata) == length(strata2)
    for(s in strata){
      if(max(strata2==s) != 1){stop("gsDesign2: Strata in `AHR()` must be the same in enrollRates and failRates")}
    }
    # check input simple is logical
    if(!is.logical(simple)){stop("gsDesign2: simple in `AHR()` must be logical")}
  
    # compute proportion in each group
    Qe <- ratio / (1 + ratio)
    Qc <- 1 - Qe
  
    # compute expected events by treatment group, stratum and time period
    rval <- NULL
    for(td in totalDuration){
      events <- NULL
      for(s in strata){
        # subset to stratum
        enroll <- enrollRates %>% filter(Stratum==s)
        fail <- failRates %>% filter(Stratum==s)
        # Control events
        enrollc <- enroll %>% mutate(rate=rate*Qc)
        control <- eEvents_df(enrollRates=enrollc,failRates=fail,totalDuration=td,simple=FALSE)
        # Experimental events
        enrolle <- enroll %>% mutate(rate=rate*Qe)
        fre <- fail %>% mutate(failRate=failRate*hr)
        experimental <- eEvents_df(enrollRates=enrolle,failRates=fre,totalDuration=td,simple=FALSE)
        # Combine control and experimental; by period recompute HR, events, information
        events <-
          rbind(control %>% mutate(Treatment="Control"),
                experimental %>% mutate(Treatment="Experimental")) %>%
          arrange(t, Treatment) %>% ungroup() %>% group_by(t) %>%
          summarize(Stratum = s, info = (sum(1 / Events))^(-1),
                    Events = sum(Events), HR = last(failRate) / first(failRate)
          ) %>%
          rbind(events)
      }
      rval <- rbind(rval,
                    events %>%
                      mutate(Time=td, lnhr = log(HR), info0 = Events * Qc * Qe) %>%
  # NEXT 2 lines are the only changes from AHR()
                      ungroup() %>% group_by(Stratum, t) %>%
                      summarize(Time = td, Events = sum(Events), HR=first(HR), lnhr=first(lnhr), info0 = sum(info0), info = sum(info)) %>% ungroup()
      )
    }
  
    if(!simple) return(rval %>% select(c("Time", "Stratum", "t", "HR", "Events", "info", "info0")) %>% 
                         group_by(Time, Stratum) %>% arrange(t, .by_group = TRUE))
    return(rval %>%
             group_by(Time) %>%
             summarize(AHR = exp(sum(log(HR)*Events)/sum(Events)),
                       Events = sum(Events),
                       info = sum(info),
                       info0 = sum(info0))
    )
  }
  ```
  
  
  Under the different scenarios of interest, we can examine the expected number of events in time periods of interest.
  
  
  ```{r}
  events_by_time_period <- NULL
  for(g in c("PH", "Delayed effect 1","Delayed effect 2", "Crossing")){
      events_by_time_period <- 
        rbind(events_by_time_period,
        AHRx(enrollRates = enrollRates %>% filter(Scenario == g),
            failRates = failRates %>% filter(Scenario == g),
            totalDuration = c(12, 20, 28, 36), simple = FALSE) %>% mutate(Scenario = g)
        )
  }
  events_by_time_period %>% gt()
  ```
  
  Recall that our alternate hypothesis assumes no treatment effect (HR=1) for 4 months and then HR = 0.6 thereafter.
  For any of the above scenarios, if we wish to base a futility bound on this assumption plus the above number of events in the first 4 months and after 4 months, then we can compute the average hazard ratio under the alternate hazard ratio for each scenario at 20 months as follows.
  You can see that an interim futility spending bound based on the alternate hypothesis can depend fairly heavily on enrollment and the control failure rate.
  Note also that at the time of interim analysis, the alternate hypothesis AHR can be computed in this same fashion based on observed events by time period.
  Note that this can be quite different than the scenario HR; e.g., for PH, we assume HR=0.7 throughout, but for the futility bound comparison, we compute blinded AHR that decreases with each analysis under the alternate hypothesis.
  
  
  ```{r}
  # Time periods for each scenario were 0-4, 4-6, and 6+
  # Thus H1 has HR as follows
  hr1 <- tibble(t = c(0, 4, 6), hr1 = c(1, .6, .6))
  ahr_by_analysis <-
    events_by_time_period %>% 
    full_join(hr1) %>%
    group_by(Scenario, Time) %>%
    summarize(AHR1 = exp(sum(Events * log(hr1))/ sum(Events)))
  ahr_by_analysis %>% 
    pivot_wider(names_from = Scenario, values_from = AHR1) %>% 
    gt() %>% fmt_number(columns=2:5, decimals = 3)
  ```
  
  
  ## Group sequential design
  
  Here we assume the design is under a delayed effect model where the delay is not too long and the long-term average hazard ratio benefit is strong.
  proportional hazards scenario, but we look at power under the alternate scenarios.
  We will plan a 36 month group sequential design under the delayed effect 1 scenario.
  Interim analyses are planned after 12, 20, and 28 months.
  
  ```{r}
  analysisTimes <- c(12, 20, 28, 36)
  upar <- list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL, theta=0)
  lpar <- list(sf = gsDesign::sfHSD, total_spend = .1, param = -2, timing = NULL, theta=NULL)
  NPHasymmetric <- gs_design_ahr(enrollRates = enrollRates,
                                 failRates = failRates,
                                 ratio = 1, alpha = .025, beta = 0.1,
                                 # Information fraction not required (but available!)
                                 analysisTimes = analysisTimes,
                                 # Function to enable spending bound
                                 upper = gs_spending_bound, lower = gs_spending_bound,
                                 # Spending function and parameters used
                                 upar = upar, lpar = lpar
  )
  NPHasymmetric$bounds
  ```
  
  By scenario, we now wish to compute the adjusted expected futility bounds and the power implied.
  
  ```{r}
  xx <- NULL
  lparx <- lpar
  for(g in c("PH", "Delayed effect 1","Delayed effect 2", "Crossing")){
    AHR1 <- (filter(ahr_by_analysis, Scenario == g))$AHR1
    lparx$theta1 <- -log(AHR1)
    yy <- gs_power_ahr(enrollRates = enrollRates %>% filter(Scenario == g),
                 failRates = failRates %>% filter(Scenario == g), 
                 events = NULL,
                 analysisTimes = c(12,20,28,36),
                 upper = gs_spending_bound,
                 upar = upar,
                 lower = gs_spending_bound,
                 lpar = lparx)
    xx <- rbind(xx, yy %>% mutate(Scenario = g))
  }
  ```
  
  
  
  
  ### Weighted logrank
  
  ```{r, eval = FALSE}
  ss_FH05_fixed <- NULL
  g <- "Delayed effect"
  # for(g in c("PH", "Delayed effect", "Crossing")){
  #     ss_FH05_fixed <- 
  #       rbind(ss_FH05_fixed,
      gs_arm <- gsdmvn:::gs_create_arm(enrollRates, failRates %>% filter(Scenario == g), 
                                       ratio = 1,       # Randomization ratio
                                       total_time = 44) # Total study duration
      arm0 <- gs_arm[["arm0"]]
      arm1 <- gs_arm[["arm1"]]
      npsurvSS::size_two_arm(arm0, arm1, power = 0.9, alpha = 0.025, 
                             test = list(test="weighted logrank", weight = "FH_p.5_q0"))
      
      npsurvSS::size_two_arm(arm0, arm1, power = 0.9, alpha = 0.025, 
                         test = list(test="rmst difference", 
                         # Milestone allow user to define RMST cutpoint
                         milestone = 44)) # 42 selected since better than 40 or 44
      gsdmvn:::gs_design_wlr(enrollRates = enrollRates, 
                             failRates = failRates %>% filter(Scenario == g), 
                             weight = function(x, arm0, arm1){
                                        gsdmvn:::wlr_weight_fh(x, arm0, arm1, 
                                                               rho = 0, gamma = 0.5, tau = 4)},
                             alpha = .025, beta = .1,
                             upar = qnorm(.975),
                             lpar = -Inf,
                             analysisTimes = 44)$bounds %>% filter(Bound =="Upper")
  ```
  
  ```{r, eval = FALSE}
  # Ignore tau or (tau can be -1) 
  gsdmvn:::gs_design_wlr(enrollRates = enrollRates, 
                         failRates = failRates %>% filter(Scenario == g), 
                         weight = function(x, arm0, arm1){
                                    gsdmvn:::wlr_weight_fh(x, arm0, arm1, 
                                                           rho = 0, gamma = 0.5)},
                         alpha = .025, beta = .1,
                         upar = qnorm(.975),
                         lpar = -Inf,
                         analysisTimes = 44)$bounds %>% filter(Bound =="Upper")
  ```
  
  
  ```{r, eval = FALSE}
  # MaxCombo
     MC_test <- data.frame(rho = c(0, 0, .5), gamma = c(0, .5, .5), tau = -1,
                        test = 1:3,
                        Analysis = 1,
                        analysisTimes = 44)
     gs_design_combo(enrollRates,
                 failRates %>% filter(Scenario == g),
                 MC_test,
                 alpha = 0.025,
                 beta = 0.1,
                 ratio = 1,
                 binding = FALSE,
                 upper = gs_spending_combo,
                 upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
                 lower = gs_spending_combo,
                 lpar = list(sf = gsDesign::sfLDOF, total_spend = .1)
                 )
  }
  ```
  
  ## References

Package: gsdmvn
File: vignettes/DesignWithSpending.Rmd
Format: text
Content:
  ---
  title: "Trial design with spending under NPH"
  output:
    html_vignette
  vignette: >
    %\VignetteIndexEntry{Trial design with spending under NPH}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  library(gsdmvn)
  library(dplyr)
  library(tibble)
  library(gsDesign)
  library(gsDesign2)
  ```
  
  
  ## Overview
  
  This vignette covers how to implement designs for trials with spending assuming non-proportional hazards.
  We are primarily concerned with practical issues of implementation rather than design strategies, but we will not ignore design strategy.
  
  ## Scenario for consideration
  
  Here we set up enrollment, failure and dropout rates along with assumptions for enrollment duration and times of analyses.
  
  
  ```{r}
  analysisTimes <- c(18, 24, 30, 36)
  enrollRates <- tibble::tibble(Stratum = "All",
                                duration = c(2, 2, 2, 6),
                                rate = c(8, 12, 16, 24))
  failRates <- tibble::tibble(Stratum = "All",
                              duration=c(3,100),
                              failRate=log(2)/c(8,14),
                              hr=c(.9,.6),
                              dropoutRate=.001
                             )
  ```
  
  ## Deriving power for a given sample size
  
  We derive statistical information at targeted analysis times.
  
  ```{r}
  xx <- gsDesign2::AHR(enrollRates = enrollRates, failRates = failRates, totalDuration = analysisTimes)
  Events <- ceiling(xx$Events)
  yy <- gs_info_ahr(enrollRates = enrollRates, failRates = failRates, events = Events)
  ```
  
  Now we can examine power using `gs_power_npe()`:
  
  ```{r}
  timing <- yy$info0/max(yy$info0)
  d <- gsDesign::gsDesign(k=length(timing), test.type=2, sfu= sfLDOF, alpha = .025, timing = timing)
  zz <- gs_power_npe(theta = yy$theta, info = yy$info, info0 = yy$info0, 
               upper = gs_b, lower = gs_b,
               upar = d$upper$bound,
               lpar = d$lower$bound
  )
  zz
  ```
  
  
  ## Deriving sample size to power a trial
  
  If we were using a fixed design, we would approximate the sample size as follows:
  
  ```{r}
  K <- 4
  minx <- ((qnorm(.025) / sqrt(zz$info0[K]) + qnorm(.1) / sqrt(zz$info[K])) / zz$theta[K])^2
  minx
  ```
  
  If we inflate the enrollment rates by `minx` and use a fixed design, we will see this achieves the targeted power.
  
  ```{r}
  gs_power_npe(theta = yy$theta[K], info = yy$info[K] * minx, info0 = yy$info0[K] * minx, 
               upar = qnorm(.975), lpar = -Inf) %>% 
    filter(Bound == "Upper")
  ```
  
  The power for a group sequential design with the same final sample size is a bit lower:
  
  ```{r}
  zz <- gs_power_npe(theta = yy$theta, info = yy$info * minx, info0 = yy$info0 * minx, 
               upper = gs_spending_bound, lower = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL)
  )
  zz
  ```
  
  If we inflate this a bit we will be overpowered.
  
  ```{r}
  zz <- gs_power_npe(theta = yy$theta, info = yy$info * minx * 1.2, info0 = yy$info0 * minx * 1.2, 
               upper = gs_spending_bound, lower = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL)
  )
  zz
  ```
  
  Now we use `gs_design_npe()` to inflate the information proportionately to power the trial.
  
  ```{r}
  theta <- yy$theta
  info <- yy$info
  info0 <- yy$info0
  upper = gs_spending_bound
  lower = gs_spending_bound
  upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL)
  lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL)
  alpha = .025
  beta = .1
  binding = FALSE
  test_upper = TRUE
  test_lower = TRUE
  r = 18
  tol = 1e-06
  
  zz <- gs_design_npe(theta = yy$theta, info = yy$info, info0 = yy$info0, 
               upper = gs_spending_bound, lower = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL),
               lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025, param = NULL, timing = NULL)
  )
  zz
  ```

Package: gsdmvn
File: vignettes/gsDesign.bib
Format: text
Content:
  @article{Haybittle,
    title={Repeated assessment of results in clinical trials of cancer treatment},
    author={Haybittle, JL},
    journal={The British journal of radiology},
    volume={44},
    number={526},
    pages={793--797},
    year={1971},
    publisher={The British Institute of Radiology}
  }
  
  @BOOK{JTBook,
     author = "Jennison, Christopher and Turnbull, Bruce W.",
     title = "Group Sequential Methods with Applications to Clinical Trials",
     publisher = "Chapman and Hall/CRC",
     address = "Boca Raton, FL",
     year = "2000"}
  
  @ARTICLE{KimTsiatis,
  	author = "Kim, Kyungmann  and Anastasios A. Tsiatis",
  	title = "Study duration for clinical trials with survival response and early stopping rule.",
  	journal = "Biometrics",
  	volume = "46",
  	year = "1990",
  	pages = "81-92"}
  
  @book{LachinBook,
    title={Biostatistical methods: the assessment of relative risks},
    author={Lachin, John M},
    volume={509},
    year={2009},
    publisher={John Wiley \& Sons}
  }
  
  @ARTICLE{LachinFoulkes,
  	author = "Lachin, John M. and Foulkes, Mary A.",
  	title = "Evaluation of sample size and power for analyses of survival with allowance for nonuniform patient entry, losses to follow-up, noncompliance, and stratification.",
  	journal = "Biometrics",
  	volume = "42",
  	year = "1986",
  	pages = "507-519"}
  
  @ARTICLE{LanDeMets,
  	author = "Lan, K.~K.~G. and DeMets, David L.",
  	title = "Discrete sequential boundaries for clinical trials.",
  	journal = "Biometrika",
  	volume = "70",
  	year = "1983",
  	pages = "659-663"}
  
  @ARTICLE{LanDeMets1989,
  	author = "Lan, K.~K.~G. and DeMets, David L.",
  	title = "Group sequential procedures: Calendar versus information time.",
  	journal = "Statistics in Medicine",
  	volume = "8",
  	year = "1989",
  	pages = "1191-1198",
  	DOI="10.1002/sim.4780081003"}
  
  @article{Luo2019,
    title={Design and monitoring of survival trials in complex scenarios},
    author={Luo, Xiaodong and Mao, Xuezhou and Chen, Xun and Qiu, Junshan and Bai, Steven and Quan, Hui},
    journal={Statistics in medicine},
    volume={38},
    number={2},
    pages={192--209},
    year={2019},
    publisher={Wiley Online Library}
  }
  @ARTICLE{MaurerBretz2013,
    author="Willi Maurer and Frank Bretz",
    title="Multiple testing in group sequential trials using graphical approaches",
    journal="Statistics in Biopharmaceutical Research",
    volume="5",
    year="2013",
    pages="311-320",
    DOI="10.1080/19466315.2013.807748"}
  
  @article{Peto,
    title={Design and analysis of randomized clinical trials requiring prolonged observation of each patient. I. Introduction and design},
    author={Peto, Richard and Pike, MCetal and Armitage, Pet and Breslow, NE and Cox, DR and Howard, Sf V and Mantel, N and McPherson, K and Peto, J and Smith, PG},
    journal={British journal of cancer},
    volume={34},
    number={6},
    pages={585--612},
    year={1976},
    publisher={Nature Publishing Group}
  }
  
  @BOOK{PLWBook,
    author = "Michael A. Proschan and  K. K. Gordon Lan and Janet Turk Wittes",
    title = "Statistical Monitoring of Clinical Trials. A Unified Approach.",
    publisher = "Springer",
    address = "New York, NY",
    year = "2006"}
  
  @ARTICLE{Tsiatis,
  	author="Anastasios A. Tsiatis",
  	title="Repeated significance testing for a general class of statistics use in censored survival analysis.",
  	journal="Journal of the American Statistical Association",
  	volume="77",
  	pages="855-861",
  	year="1982"}
  
  @ARTICLE{Schemper2009,
    title="The estimation of average hazard ratios by weighted Cox regression",
    author="Schemper, Michael and Wakounig, Samo and Heinze, Georg",
    journal="Statistics in medicine",
    volume="28",
    number="19",
    pages="2473--2489",
    year="2009"
  }
  
  @ARTICLE{Kalbfleisch1981,
    title={Estimation of the average hazard ratio},
    author={Kalbfleisch, John D and Prentice, Ross L},
    journal={Biometrika},
    volume={68},
    number={1},
    pages={105--112},
    year={1981}
  }
  
  @UNPUBLISHED{NPHWGDesign,
    title={Robust design and analysis of clinical trials with non-proportional hazards: a straw man guidance from a cross-pharma working group},
    author={Satrajit Roychoudhury and Keaven M. Anderson and Jiabu Ye and Pralay Mukhopadhyay},
    ee= {https://arxiv.org/abs/1908.07112},
    year={2020}
  }
  
  @UNPUBLISHED{NPHWGSimulation,
    title={Alternative analysis methods for time to event endpoints under non-proportional hazards: a comparative analysis},
    author={Ray S. Lin and Ji Lin and Satrajit Roychoudhury and Keaven M. Anderson and Tianle Hu and Bo Huang and Larry F Leon and Jason J.Z. Liao and Rong Liu and Xiaodong Luo and Pralay Mukhopadhyay and Rui Qin and Kay Tatsuoka and Xuejing Wang and Yang Wang and Jian Zhu and Tai-Tsang Chen and Renee Iacona},
    note={Submitted for publication},
    year={2019}
  }
  
  @article{Karrison2016,
    title={Versatile tests for comparing survival curves based on weighted log-rank statistics},
    author={Karrison, Theodore G},
    journal={The Stata Journal},
    volume={16},
    number={3},
    pages={678--690},
    year={2016},
    publisher={SAGE Publications Sage CA: Los Angeles, CA}
  }
  
  @article{FH1982,
    title={A class of rank test procedures for censored survival data},
    author={Harrington, David P and Fleming, Thomas R},
    journal={Biometrika},
    volume={69},
    number={3},
    pages={553--566},
    year={1982},
    publisher={Oxford University Press}
  }
  
  @book{FH2011,
    title={Counting processes and survival analysis},
    author={Fleming, Thomas R and Harrington, David P},
    volume={169},
    year={2011},
    publisher={John Wiley \& Sons}
  }
  
  @article{Lee2007,
    title={On the versatility of the combination of the weighted log-rank statistics},
    author={Lee, Seung-Hwan},
    journal={Computational Statistics \& Data Analysis},
    volume={51},
    number={12},
    pages={6557--6564},
    year={2007},
    publisher={Elsevier}
  }
  
  @article{Schoenfeld1981,
    title={The asymptotic properties of nonparametric tests for comparing survival distributions},
    author={Schoenfeld, David},
    journal={Biometrika},
    volume={68},
    number={1},
    pages={316--319},
    year={1981},
    publisher={Oxford University Press}
  }
  @manual{RHubWhitepaper,
   title={A Risk-based Approach for Assessing R package Accuracy within a Validated Infrastructure: White Paper Summary},
   author={Andy Nicholls},
   url={https://www.pharmar.org/blog/2020/01/30/2020-05-07-a-risk-based-approach-for-assessing-r-package-accuracy-within-a-validated-infrastructure-white-paper-summary/},
   year={2020}
  }
  @manual{RFoundation,
    title={R: Regulatory Compliance and Validation Issues A Guidance Document for the Use of R in Regulated Clinical Trial Environments},
    author={"The R Foundation for Statistical Computing},
    url={https://www.r-project.org/doc/R-FDA.pdf},
    year={2018}
  }
  @manual{FDAsoftware,
    title={Statistical Software Clarifying Statement},
    author={Food and Drug Administration},
    url={https://www.fda.gov/media/109552/download},
    year={2015}
  }
  @article{Yung2019Bcs,
    title={Sample size and power for the weighted log-rank test and Kaplan-Meier based tests with allowance for nonproportional hazards},
    author={Yung, Godwin and Liu, Yi},
    journal={Biometrics},
    year={2019},
    publisher={Wiley Online Library}
  }
  @Manual{npsurvss,
    title = {npsurvSS: Sample Size and Power Calculation for Common Non-Parametric Tests in Survival Analysis},
    author = {Godwin Yung and Yi Liu},
    year = {2019},
    url = {https://cran.r-project.org/web/packages/npsurvSS}
  }
  @article{Luo2019SIM,
    title={Design and monitoring of survival trials in complex scenarios},
    author={Luo, Xiaodong and Mao, Xuezhou and Chen, Xun and Qiu, Junshan and Bai, Steven and Quan, Hui},
    journal={Statistics in medicine},
    volume={38},
    number={2},
    pages={192--209},
    year={2019},
    publisher={Wiley Online Library}
  }
  @Manual{PWEALL,
    title = {PWEALL: Design and Monitoring of Survival Trials Accounting for Complex Situations},
    author = {Xiaodong Luo and Xuezhou Mao and Xun Chen and Hui Quan},
    year = {2019},
    url = {https://cran.r-project.org/web/packages/PWEALL}
  }
   @Manual{gsDesign,
      title = {gsDesign: Group Sequential Design},
      author = {Keaven Anderson},
      year = {2020},
      note = {R package version 3.1.1},
      url = {https://CRAN.R-project.org/package=gsDesign}
    }
  @article{Liu2008,
    title={On adaptive extensions of group sequential trials for clinical investigations},
    author={Liu, Qing and Anderson, Keaven M},
    journal={Journal of the American Statistical Association},
    volume={103},
    number={484},
    pages={1621--1630},
    year={2008},
    publisher={Taylor \& Francis}
  }
  @BOOK{EAST,
     author = "Cytel, Inc.",
     title = "EAST 5",
     publisher = "Cytel, Inc.",
     address = "Cambridge, MA",
     year = "2007"}
  @inproceedings{Gillen2013,
    title={Designing, monitoring, and analyzing group sequential clinical trials using the RCTdesign Package for R},
    author={Gillen, Daniel L and Emerson, Scott S},
    booktitle={Proceedings of the Fourth Seattle Symposium in Biostatistics: Clinical Trials},
    pages={177--208},
    year={2013},
    organization={Springer}
  }
  @Manual{RCTdesign,
    title = {RCTdesign.org : Methods and Software for Clinical Trials},
    author = {Scott S. Emerson and Daniel L. Gillen and John M. Kittelson and Sarah C. Emerson and Gregory P. Levin},
    year = {2020},
    url = {http://rctdesign.org/}
  }
  @article{Yung2019Bcs,
    title={Sample size and power for the weighted log-rank test and Kaplan-Meier based tests with allowance for nonproportional hazard, Godwin and Liu, Yi},
    journal={Biometricss},
    author={Yung},
    year={2019},
    publisher={Wiley Online Library}
  }
  @Manual{npsurvss,
    title = {npsurvSS: Sample Size and Power Calculation for Common Non-Parametric Tests in Survival Analysis},
    author = {Godwin Yung and Yi Liu},
    year = {2019},
    url = {https://cran.r-project.org/web/packages/npsurvSS}
  }
  @article{Luo2019SIM,
    title={Design and monitoring of survival trials in complex scenarios},
    author={Luo, Xiaodong and Mao, Xuezhou and Chen, Xun and Qiu, Junshan and Bai, Steven and Quan, Hui},
    journal={Statistics in medicine},
    volume={38},
    number={2},
    pages={192--209},
    year={2019},
    publisher={Wiley Online Library}
  }
  @Manual{PWEALL,
    title = {PWEALL: Design and Monitoring of Survival Trials Accounting for Complex Situations},
    author = {Xiaodong Luo and Xuezhou Mao and Xun Chen and Hui Quan},
    year = {2019},
    url = {https://cran.r-project.org/web/packages/PWEALL}
  }
  @misc{rpact,
    title={rpact: Confirmatory adaptive clinical trial design and analysis (R package version 2.0. 2)},
    author={Wassmer, G and Pahlke, F},
    year={2019}
  }
  @article{Magirr2019,
    title={Modestly weighted logrank tests},
    author={Magirr, Dominic and Burman, Carl-Fredrik},
    journal={Statistics in medicine},
    volume={38},
    number={20},
    pages={3782--3790},
    year={2019},
    publisher={Wiley Online Library}
  }
  @Manual{survRM2,
    title = {survRM2: Comparing Restricted Mean Survival Time},
    author = {	Hajime Uno and Lu Tian and Miki Horiguchi and Angel Cronin and Chakib Battioui and James Bell},
    year = {2020},
    url = {https://cran.r-project.org/web/packages/survRM2}
  }
  @article{FHO,
    title={Designs for group sequential tests},
    author={Fleming, Thomas R and Harrington, David P and O'Brien, Peter C},
    journal={Controlled clinical trials},
    volume={5},
    number={4},
    pages={348--361},
    year={1984},
    publisher={Elsevier}
  }
  @article{Mukhopadhyay2020,
    title={Statistical and practical considerations in designing of immuno-oncology trials},
    author={Mukhopadhyay, Pralay and Huang, Wenmei and Metcalfe, Paul and {\"O}hrn, Fredrik and Jenner, Mary and Stone, Andrew},
    journal={Journal of Biopharmaceutical Statistics},
    pages={1--17},
    year={2020},
    publisher={Taylor \& Francis}
  }
  @article{NPHWG2021Design,
    title={Robust design and analysis of clinical trials with non-proportional hazards: a straw man guidance from a cross-pharma working group},
    author={Roychoudhury, Satrajit and Anderson, Keaven M and Ye, Jiabu and Mukhopadhyay, Pralay},
    journal={Statistics in Biopharmaceutical Research},
    pages={1--37},
    year={2021},
    publisher={Taylor \& Francis}
  }
  @article{NPHWG2020sim,
    title={Alternative analysis methods for time to event endpoints under nonproportional hazards: a comparative analysis},
    author={Lin, Ray S and Lin, Ji and Roychoudhury, Satrajit and Anderson, Keaven M and Hu, Tianle and Huang, Bo and Leon, Larry F and Liao, Jason JZ and Liu, Rong and Luo, Xiaodong and others},
    journal={Statistics in Biopharmaceutical Research},
    volume={12},
    number={2},
    pages={187--198},
    year={2020},
    publisher={Taylor \& Francis}
  }
  @article{FPG,
    title={Monitoring pairwise comparisons in multi-armed clinical trials},
    author={Follmann, Dean A and Proschan, Michael A and Geller, Nancy L},
    journal={Biometrics},
    pages={325--336},
    year={1994},
    publisher={JSTOR}
  }
  @article{AGZS2021unified,
    title={A unified framework for weighted parametric group sequential design (WPGSD)},
    author={Anderson, Keaven M and Guo, Zifang and Zhao, Jing and Sun, Linda Z},
    journal={arXiv preprint arXiv:2103.10537},
    year={2021}
  }
  @article{CAPTURE,
    title={Randomised placebo-controlled trial of abciximab before and during coronary intervention in refractory unstable angina: the CAPTURE study},
    author={Capture Investigators and others},
    journal={Lancet},
    volume={349},
    pages={1429--1435},
    year={1997}
  }
  @article{CCmyth,
    title={The myth of continuity-corrected sample size formulae},
    author={Gordon, Ian and Watson, Ray},
    journal={Biometrics},
    pages={71--76},
    year={1996},
    publisher={JSTOR}
  }
  @article{FarringtonManning,
    title={Test statistics and sample size formulae for comparative binomial trials with null hypothesis of non-zero risk difference or non-unity relative risk},
    author={Farrington, Conor P and Manning, Godfrey},
    journal={Statistics in medicine},
    volume={9},
    number={12},
    pages={1447--1454},
    year={1990},
    publisher={Wiley Online Library}
  }
  @article{Chan2003,
    title={Confidence Interval and Hypothesis Testing"},
    author={Chan, Ivan SF and Mehrotra, Devan V},
    journal={Encyclopedia of Biopharmaceutical Statistics},
    pages={231--234},
    year={2003},
    publisher={Informa Health Care}
  }
  @article{Chan2002,
    title={Power and sample size determination for noninferiority trials using an exact method},
    author={Chan, Ivan SF},
    journal={Journal of biopharmaceutical statistics},
    volume={12},
    number={4},
    pages={457--469},
    year={2002},
    publisher={Taylor \& Francis}
  }

Package: gsdmvn
File: vignettes/NPEbackground.Rmd
Format: text
Content:
  ---
  title: "Non-Proportional Effect Size in Group Sequential Design"
  output:
    html_document:
      toc: true
      toc_depth: 2
      toc_float: true
      theme: flatly
      highlight: tango
  bibliography: gsDesign.bib
  vignette: |
    %\VignetteIndexEntry{Non-Proportional Effect Size in Group Sequential Design}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  library(tibble)
  library(dplyr)
  library(knitr)
  library(gsdmvn)
  ```
  
  # Overview
  
  
  The acronym NPES is short for non-proportional effect size. 
  While it is motivated primarily by a use for when designing a time-to-event trial under non-proportional hazards (NPH), we have simplified and generalized the concept here. The model is likely to be useful for rank-based survival tests beyond the logrank test that will be considered initially by @Tsiatis.
  It could also be useful in other situations where treatment effect may vary over time in a trial for some reason.
  We generalize the framework of Chapter 2 of @PLWBook to incorporate the possibility of the treatment effect changing during the course of a trial in some systematic way.
  This vignettes addresses distribution theory and initial technical issues around computing
  
  - boundary crossing probabilities
  - bounds satisfying targeted boundary crossing probabilities
  
  This is then applied to generalize computational algorithms provided in Chapter 19 of @JTBook that are used to compute boundary crossing probabilities as well as boundaries for group sequential designs.
  Additional specifics around boundary computation, power and sample size are provided in a separate vignette.
  
  # The probability model
  
  ## The continuous model and E-process
  
  We consider a simple example here to motivate distribution theory that is quite general and applies across many situations.
  For instance @PLWBook immediately suggest paired observations, time-to-event and binary outcomes as endpoints where the theory is applicable.
  
  We assume for a given integer $N>0$ that $X_{i}$ are independent, $i=1,2,\ldots$.
  For some integer $K\le N$ we assume we will perform analysis $K$ times after $0<n_1<n_2,\ldots ,n_K = N$ observations are available for analysis.
  Note that we have not confined $n\le N$, but $N$ can be considered the final planned sample size.
  @PLWBook refer to the estimation or E-process which we extend here to
  
  $$\hat{\theta}_k = \frac{\sum_{i=1}^{n_k} X_{i}}{n_k}\equiv \bar X_{k}.$$
  While @PLWBook have used $\delta$ instead of $\theta$ in our notation, we stick more closely to the notation of @JTBook where $\theta$ is used.
  For our example, we see $\hat{\theta}_k\equiv\bar X_k$ represents the sample average at analysis $k$, $1\le k\le K$.
  With a survival endpoint, $\hat\theta_k$ would typically represent a Cox model coefficient representing the logarithm of the hazard ratio for experimental vs control treatment and $n_k$ would represent the planned number of events at analysis $k$, $1\le k\le K.$
  Denoting $t_k=n_k/N$, we assume that for some real-valued function $\theta(t)$ for $t\ge 0$ we have for $1\le k\le K$
   
  $$E(\hat{\theta}_k) =\theta(t_k) =E(\bar X_k).$$
  In the models of @PLWBook and @JTBook we would have $\theta(t)$ equal to some constant $\theta$.
  We assume further that for $i=1,2,\ldots$
  $$\hbox{Var}(X_{i})=1.$$
  The sample average variance under this assumption is for $1\le k\le K$
  
  $$\hbox{Var}(\hat\theta(t_k))=\hbox{Var}(\bar X_k) =  1/ n_k.$$
  The statistical information for the estimate $\hat\theta(t_k)$ for $1\le k\le K$ for this case is
  $$ \mathcal{I}_k \equiv \frac{1}{\hbox{Var}(\hat\theta(t_k))} = n_k.$$
  We now see that $t_k$, $1\le k\le K$ is the so-called information fraction at analysis $k$ in that
  $t_k=\mathcal{I}_k/\mathcal{I}_K.$
  
  ## Z-process
  
  The Z-process is commonly used (e.g., @JTBook) and will be used below to extend the computational algorithm in Chapter 19 of @JTBook by defining equivalently in the first and second lines below for $k=1,\ldots,K$
  
  $$Z_{k} = \frac{\hat\theta_k}{\sqrt{\hbox{Var}(\hat\theta_k)}}= \sqrt{\mathcal{I}_k}\hat\theta_k= \sqrt{n_k}\bar X_k.$$
  
  The variance for $1\le k\le K$ is 
  $$\hbox{Var}(Z_k) = 1$$
  and the expected value is
  
  $$E(Z_{k})= \sqrt{\mathcal{I}_k}\theta(t_{k})= \sqrt{n_k}E(\bar X_k) .$$
  
  ## B-process
  
  B-values are mnemonic for Brownian motion.
  For $1\le k\le K$ we define
  $$B_{k}=\sqrt{t_k}Z_k$$
  which implies
  $$ E(B_{k}) = \sqrt{t_{k}\mathcal{I}_k}\theta(t_k) = t_k \sqrt{\mathcal{I}_K} \theta(t_k) = \mathcal{I}_k\theta(t_k)/\sqrt{\mathcal{I}_K}$$
  and
  $$\hbox{Var}(B_k) = t_k.$$
  
  For our example, we have
  
  $$B_k=\frac{1}{\sqrt N}\sum_{i=1}^{n_k}X_i.$$
  It can be useful to think of $B_k$ as a sum of independent random variables.
  
  ## Summary of E-, Z- and B-processes
  
  ```{r, message=FALSE, warning=FALSE, echo=FALSE}
  mt <- tibble::tribble(~Statistic,    ~Example,   ~"Expected value", ~Variance,
    "$\\hat\\theta_k$", "$\\bar X_k$", "$\\theta(t_k)$",  "$\\mathcal{I}_k^{-1}$",
    "$Z_k=\\sqrt{\\mathcal{I}_k}\\hat\\theta_k$","$\\sqrt{n_k}\\bar X_k$", "$\\sqrt{\\mathcal{I}_k}\\theta(t_k)$",  "$1$",
    "$B_k=\\sqrt{t_k}Z_k$","$\\sum_{i=1}^{n_k}X_i/\\sqrt N$", "$t_k\\sqrt{\\mathcal{I}_K}\\theta(t_k)=\\mathcal{I}_k\\theta(t_k)/\\sqrt{\\mathcal{I}_K}$",  "$t_k$"
  )
  mt %>% kable(escape=FALSE)
  ```
  
  ## Conditional independence, covariance and canonical form
  
  We assume independent increments in the B-process.
  That is, for $1\le j < k\le K$
  $$\tag{1} B_k - B_j \sim \hbox{Normal} (\sqrt{\mathcal{I}_K}(t_k\theta(t_k)- t_j\theta(t_j)), t_k-t_j)$$
  independent of $B_1,\ldots,B_j$. 
  As noted above, for a given $1\le k\le K$ we have for our example
  $$B_j=\sum_{i=1}^{n_j}X_i / \sqrt N.$$
  Because of independence of the sequence $X_i$, $i=1,2,\ldots$, we  immediately have for $1\le j\le k\le K$
  $$\hbox{Cov}(B_j,B_k) = \hbox{Var}(B_j) = t_j.$$
  This leads further to
  $$\hbox{Corr}(B_j,B_k)=\frac{t_j}{\sqrt{t_jt_k}}=\sqrt{t_j/t_k}=\hbox{Corr}(Z_j,Z_k)=\hbox{Cov}(Z_j,Z_k)$$
  which is the covariance structure in the so-called *canonical form* of @JTBook.
  For our example, we have
  $$B_k=\frac{1}{\sqrt N}\sum_{i=1}^{n_k}X_i$$
  and 
  $$B_k-B_j=\frac{1}{\sqrt N}\sum_{i=n_j + 1}^{n_k}X_i$$
  and the covariance is obvious.
  We assume independent increments in the B-process that will be demonstrated for the simple example above.
  That is, for $1\le j < k\le K$
  $$\tag{1} B_k - B_j \sim \hbox{Normal} (\mathcal{I}_k\theta(t_k)- \mathcal{I}_j\theta(t_j), t_k-t_j)$$
  independent of $B_1,\ldots,B_j$. 
  For a given $1\le j\le k\le K$ we have for our example
  $$B_j=\sum_{i=1}^{n_j}X_i / (\sqrt N\sigma).$$
  Because of independence of the sequence $X_i$, $i=1,2,\ldots$, we  immediately have for $1\le j\le k\le K$
  $$\hbox{Cov}(B_j,B_k) = \hbox{Var}(B_j) = t_j/t_k =\mathcal{I}_j/\mathcal{I}_k.$$
  This leads to
  $$\mathcal{I}_j/\mathcal{I}_k=\sqrt{t_j/t_k}=\hbox{Corr}(B_j,B_k)=\hbox{Corr}(Z_j,Z_k)=\hbox{Cov}(Z_j,Z_k)$$
  which is the covariance structure in the so-called *canonical form* of @JTBook.
  The independence of $B_j$ and 
  $$B_k-B_j=\sum_{i=n_j + 1}^{n_k}X_i/(\sqrt N\sigma)$$
  is obvious for this example. 
  
  # Test bounds and crossing probabilities
  
  In this section we define notation for bounds and boundary crossing probabilities for a group sequential design.
  We also define an algorithm for computing bounds based on a targeted boundary crossing probability at each analysis.
  The notation will be used elsewhere for defining one- and two-sided group sequential hypothesis testing.
  A value of $\theta(t)>0$ will reflect a positive benefit. 
  
  For $k=1,2,\ldots,K-1$, interim cutoffs $-\infty \le a_k< b_k\le \infty$ are set; final cutoffs $-\infty \le a_K\leq b_K <\infty$ are also set.
  An infinite efficacy bound at an analysis means that bound cannot be crossed at that analysis.
  Thus, $3K$ parameters define a group sequential design: $a_k$, $b_k$, and $\mathcal{I}_k$, $k=1,2,\ldots,K$. 
  
  ## Notation for boundary crossing probabilities
  
  We now apply the above distributional assumptions to compute boundary crossing probabilities.
  We use a shorthand notation in this section to have $\theta$ represent $\theta()$ and $\theta=0$ to represent $\theta(t)\equiv 0$ for all $t$.
  We denote the probability of crossing the upper boundary at analysis $k$ without previously crossing a bound by
  
  $$\alpha_{k}(\theta)=P_{\theta}(\{Z_{k}\geq b_{k}\}\cap_{j=1}^{i-1}\{a_{j}\le Z_{j}< b_{j}\}),$$
  $k=1,2,\ldots,K.$
  
  
  Next, we consider analogous notation for the lower bound. For $k=1,2,\ldots,K$
  denote the probability of crossing a lower bound at analysis $k$ without previously crossing any bound by
  
  $$\beta_{k}(\theta)=P_{\theta}((Z_{k}< a_{k}\}\cap_{j=1}^{k-1}\{ a_{j}\le Z_{j}< b_{j}\}).$$
  For symmetric testing for analysis $k$ we would have $a_k= - b_k$, $\beta_k(0)=\alpha_k(0),$ $k=1,2,\ldots,K$. 
  The total lower boundary crossing probability for a trial is denoted by 
  $$\beta(\theta)\equiv\sum_{k=1}^{K}\beta_{k}(\theta).$$
  Note that we can also set $a_k= -\infty$ for any or all analyses if a lower bound is not desired, $k=1,2,\ldots,K$.
  For $k<K$, we can set $b_k=\infty$ where an upper bound is not desired.
  Obviously, for each $k$, we want either $a_k>-\infty$ or $b_k<\infty$.
  
  ## Recursive algorithms
  
  We now provide a small update to the algorithm of Chapter 19 of @JTBook to do the numerical integration required to compute the boundary crossing probabilites of the previous section and also identifying group sequential boundaries satisfying desired characteristics. 
  The key to these calculations is the conditional power identitity in equation (1) above which allows building recursive numerical integration identities to enable simple, efficient numerical integration.
  
  We define 
  
  $$g_1(z;\theta) = \frac{d}{dz}P(Z_1\le z) = \phi\left(z - \sqrt{\mathcal{I}_1}\theta(t_1)\right)\tag{2}$$
  
  and for $k=2,3,\ldots K$ we recursively define the subdensity function 
  
  $$\begin{align}
  g_k(z; \theta) &= \frac{d}{dz}P_\theta(\{Z_k\le z\}\cap_{j=1}^{k-1}\{a_j\le Z_j<b_j\}) \\
   &=\int_{a_{k-1}}^{b_{k-1}}\frac{d}{dz}P_\theta(\{Z_k\le z |Z_{k-1}=z_{k-1}\})g_{k-1}(z_{k-1}; \theta)dz_{k-1}\\
   &=\int_{a_{k-1}}^{b_{k-1}}f_k(z_{k-1},z;\theta)g_{k-1}(z_{k-1}; \theta)dz_{k-1}.\tag{3}
   \end{align}
  $$
  The bottom line notation here is the same as on p. 347 in @JTBook.
  However, $f_k()$ here takes a slightly different form.
  
  $$\begin{align}
  f_k(z_{k-1},z;\theta) &=\frac{d}{dz}P_\theta(\{Z_k\le z |Z_{k-1}=z_{k-1}\})\\
   &=\frac{d}{dz}P_\theta(B_k - B_{k-1} \le z\sqrt{t_k}-z_{k-1}\sqrt{t_{k-1}})\\
   &=\frac{d}{dz}\Phi\left(\frac{z\sqrt{t_k}-z_{k-1}\sqrt{t_{k-1}}-\sqrt{\mathcal{I}_K}(t_k\theta(t_k)- t_{k-1}\theta(t_{k-1}))}{\sqrt{t_k-t_{k-1}}}\right)\\
   &=\frac{\sqrt{t_k}}{\sqrt{t_k-t_{k-1}}}\phi\left(\frac{z\sqrt{t_k}-z_{k-1}\sqrt{t_{k-1}}-\sqrt{\mathcal{I}_K}(t_k\theta(t_k)- t_{k-1}\theta(t_{k-1}))}{\sqrt{t_k-t_{k-1}}}\right)\\
   &=\frac{\sqrt{\mathcal{I}_k}}{\sqrt{\mathcal{I}_k-\mathcal{I}_{k-1}}}\phi\left(\frac{z\sqrt{\mathcal{I}_k}-z_{k-1}\sqrt{\mathcal{I}_{k-1}}-(\mathcal{I}_k\theta(t_k)- \mathcal{I}_{k-1}\theta(t_{k-1}))}{\sqrt{\mathcal{I}_k-\mathcal{I}_{k-1}}}\right).\tag{3}
  \end{align}$$
  
  We have worked towards this last line due to its comparability to equation (19.4) on p. 347 of @JTBook which assumes $\theta(t_k)=\theta$ for some constant $\theta$; we re-write that equation slightly here as:
  
  $$f_k(z_{k-1},z;\theta) = \frac{\sqrt{\mathcal{I}_k}}{\sqrt{\mathcal{I}_k-\mathcal{I}_{k-1}}}\phi\left(\frac{z\sqrt{\mathcal{I}_k}-z_{k-1}\sqrt{\mathcal{I}_{k-1}}-\theta(\mathcal{I}_k- \mathcal{I}_{k-1})}{\sqrt{\mathcal{I}_k-\mathcal{I}_{k-1}}}\right).\tag{4}$$
  This is really the only difference in the computational algorithm for boundary crossing probabilities from the @JTBook algorithm.
  Using the above recursive approach we can compute for $k=1,2,\ldots,K$
  
  $$\alpha_{k}(\theta)=\int_{b_k}^\infty g_k(z;\theta)dz\tag{5}$$
  and
  $$\beta_{k}(\theta)=\int_{-\infty}^{a_k} g_k(z;\theta)dz.\tag{6}$$
  
  ## Deriving spending boundaries
  
  We can now derive boundaries satisfying given boundary crossing probabilities using equations (2-6) above.
  Suppose for we have specified $b_1,\ldots,b_{k-1}$ and $a_1,\ldots,a_{k-1}$ and now wish to derive $a_k$ and $b_k$ such that equations (5) and (6) hold.
  We write the upper bound as a function of the probability of crossing we wish to derive.
  
  $$\pi_k(b;\theta) = \int_b^\infty g_k(z;\theta)dz$$ 
  $$\pi_k^\prime(b;\theta) =\frac{d}{db}\pi_k(b;\theta)= -g_k(b; \theta).\tag{7}$$
  If we have a value $\pi_k(b^{(i)};\theta)$ we can use a first order Taylor's series expansion to approximate
  
  $$\pi_k(b;\theta)\approx \pi_k(b^{(i)};\theta)+(b-b^{(i)})\pi_k^\prime(b^{(i)};\theta)$$
  We set $b^{(i+1)}$ such that
  
  $$\alpha_k(\theta)=\pi_k(b^{(i)}; \theta) + (b^{(i+1)}-b^{(i)})\pi^\prime(b^{(i)};\theta).$$
  
  Solving for $b^{(i+1)}$ we have 
  
  $$b^{(i+i)} = b^{(i)} + \frac{\alpha_k(\theta) - \pi_k(b^{(i)};\theta)}{\pi_k^\prime(b^{(i)}; \theta)}= b^{(i)} - \frac{\alpha_k(\theta) - \pi_k(b^{(i)};\theta)}{g_k(b^{(i)}; \theta)}\tag{8}$$
  and iterate until $|b^{(i+1)}-b^{(i)}|<\epsilon$ for some tolerance level $\epsilon>0$ and $\pi_k(b^{(i+1)};\theta)-\alpha_k(\theta)$ is suitably small.
  A simple starting value for any $k$ is 
  
  $$b^{(0)} = \Phi^{-1}(1- \alpha_k(\theta)) + \sqrt{\mathcal{I}_k}\theta(t_k).\tag{9}$$
  Normally, $b_k$ will be calculated with $\theta(t_k)=0$ for $k=1,2,\ldots,K$ which simplifies the above.
  However, $a_k$ computed analogously will often use a non-zero $\theta$ to enable so-called $\beta$-spending.
  
  # Numerical integration
  
  The numerical integration required to compute boundary probabilities and derive boundaries is the same as that defined in section 19.3 of @JTBook. The single change is the replacement of the non-proportional effect size assumption of equation (3) above replacing the equivalent of equation (4) used for a constant effect size as in @JTBook.
  
  ## Demonstrating calculations
  
  We walk through how to perform the basic calculations above.
  The basic scenario will have one interim analysis in addition to the final analysis.
  We will target Type I error $\alpha=0.025$ and Type II error $\beta = 0.1$, the latter corresponding to a target of 90% power.
  We will assume a power spending function with $\rho=2$ for both bounds.
  That is, for information fraction $t$, the cumulative spending will be $\alpha \time t^2$ for the upper bound and $\beta \times t^2$ for the lower bound.
  Statistical information will be 1 for the first analysis and 4 for the final analysis, leading to information fraction $t_1= 1/4, t_2=1$ for the interim and final, respectively.
  We assume $\theta_1 = .5$, $\theta_3=1.5$.
  
  - Set up overall study parameters
  
  ```{r}
  # Information for both null and alternative
  info <- c(1, 4)
  # information fraction
  timing <- info / max(info) 
  # Type I error
  alpha <- 0.025
  # Type II error (1 - power)
  beta <- 0.1
  # Cumulative alpha-spending at IA, Final
  alphaspend <- alpha * timing^2
  # Cumulative beta-spending at IA, Final
  betaspend <- beta * timing^2
  # Average treatment effect at analyses
  theta <- c(1, 3)/2
  ```
  
  - Calculate interim bounds
  
  ```{r}
  # Upper bound under null hypothesis
  b1 <- qnorm(alphaspend[1], lower.tail = FALSE)
  # Lower bound under alternate hypothesis
  a1 <- qnorm(betaspend[1], mean = sqrt(info[1]) * theta[1])
  # Compare probability of crossing vs target for bounds:
  cat("Upper bound =", b1, "Target spend=", alphaspend[1],
      "Actual spend=", pnorm(b1, lower.tail=FALSE))
  ```
  ```{r}
  # Lower bound under alternate hypothesis
  a1 <- qnorm(betaspend[1], mean = sqrt(info[1]) * theta[1])
  # Compare probability of crossing vs target for bounds:
  cat("Lower bound =", a1, "Target spend=", betaspend[1],
      "Actual spend=", pnorm(a1, mean = sqrt(info[1]) * theta[1]))
  ```
  
  - Set up numerical integration grid for next (final) analysis
  
  We set up a table for numerical integration over the continuation region which we can subsequently use to compute boundary crossing probabilities for bounds at the second interim analysis.
  We begin with the null hypothesis. 
  The columns in the resulting table are
      - `z` - $Z$-values for the grid; recall that each interim test statistic is normally distributed with variance 1
      - `w` - weights for numerical integration
      - `h` - weights `w` times the normal density that can be used for numerical integration; we will demonstrate use below 
  
  ```{r}
  # Set up grid over continuation region
  # Null hypothesis
  grid1_0 <- h1(theta = 0, I = info[1], a = a1, b = b1)
  grid1_0 %>% head()
  ```
  The probability of not crossing a bound under the null hypothesis is computed as follows:
  
  ```{r}
  probH0continue <- grid1_0 %>% summarize(sum(h)) %>% as.numeric()
  cat("Probability of continuing trial under null hypothesis\n",
      " Using numerical integration:", probH0continue, 
      "\n  Using normal cdf:", pnorm(b1) - pnorm(a1), "\n")
  ```
  
  We now set up numerical integration grid under the alternate hypothesis and the compute continuation probability.
  
  ```{r}
  grid1_1 <- h1(theta = theta[1], I = info[1], a = a1, b = b1)
  probH1continue <- grid1_1 %>% summarize(sum(h)) %>% as.numeric()
  h1mean <- sqrt(info[1]) * theta[1]
  cat("Probability of continuing trial under alternate hypothesis\n",
      " Using numerical integration:", probH1continue, 
      "\n  Using normal cdf:", pnorm(b1, mean = h1mean) - pnorm(a1, h1mean), "\n")
  ```
  
  - Compute initial iteration for analysis 2 bounds
  
  The initial estimate of the second analysis bounds are computed the same way as the actual first analysis bounds.
  
  ```{r}
  # Upper bound under null hypothesis
  # incremental spend
  spend0 <- alphaspend[2] - alphaspend[1]
  # H0 bound at 2nd analysis; 1st approximation
  b2_0 <- qnorm(spend0, lower.tail = FALSE)
  # Lower bound under alternate hypothesis
  spend1 <- betaspend[2] - betaspend[1]
  a2_0 <- qnorm(spend1, mean = sqrt(info[2]) * theta[2])
  cat("Initial bound approximation for 2nd analysis\n (",
      a2_0, ", ", b2_0,")\n", sep="")
  ```
  
  - Compute actual boundary crossing probabilities with initial approximations
  
  To get actual boundary crossing probabilities at the second analysis, we update our numerical integration grids.
  Under the null hypothesis, we need to update to the interval above `b2_0`.
  
  ```{r}
  # Upper rejection region grid under H0
  grid2_0 <- hupdate(theta = 0, I = info[2], a = b2_0, b = Inf, Im1 = info[1], gm1 = grid1_0)
  pupper_0 <- grid2_0 %>% summarize(sum(h)) %>% as.numeric()
  cat("Upper spending at analysis 2\n Target:", spend0, "\n Using initial bound approximation:",
      pupper_0,"\n")
  ```
  
  To get a first order Taylor's series approximation to update this bound, we need the derivative of the above probability with respect to the Z-value cutoff. This was given above as the subdensity computed in the grid.
  As before, the grid contains the numerical integration weight in `w` and that weight times the subdensity in `h`. Thus, to get the subdensity at the bound, which is the estimated derivative in the boundary crossing probability, we compute:
  
  ```{r}
  # First point in grid is at bound
  # Compute derivative
  dpdb2 <- grid2_0$h[1] / grid2_0$w[1]
  # Compute difference between target and actual bound crossing probability
  pdiff <- spend0 - pupper_0
  # Taylor's series update
  b2_1 <- b2_0 - pdiff / dpdb2
  # Compute boundary crossing probability at updated bound
  cat("Original bound approximation:", b2_0, 
      "\nUpdated bound approximation:", b2_1
      )
  grid2_0 <- hupdate(theta = 0, I = info[2], a = b2_1, b = Inf, Im1 = info[1], gm1 = grid1_0)
  pupper_1 <- grid2_0 %>% summarize(sum(h)) %>% as.numeric()
  cat("\nOriginal boundary crossing probability:", pupper_0, 
      "\nUpdated boundary crossing probability:", pupper_1,
      "\nTarget:", spend0, "\n"
      )
  ```
  
  We see that the Taylor's series update has gotten us substantially closer to the targeted boundary probability.
  We now update the lower bound in an analogous fashion.
  
  ```{r}
  # Lower rejection region grid under H1
  grid2_1 <- hupdate(theta = theta[2], I = info[2], a = -Inf, b = a2_0, 
                     thetam1 = theta[1], Im1 = info[1], gm1 = grid1_1)
  plower_0 <- grid2_1 %>% summarize(sum(h)) %>% as.numeric()
  # Last point in grid is at bound
  # Compute derivative
  indx <- nrow(grid2_1)
  dpda2 <- grid2_1$h[indx] / grid2_1$w[indx]
  # Compute difference between target and actual bound crossing probability
  pdiff <- spend1 - plower_0
  # Taylor's series update
  a2_1 <- a2_0 + pdiff / dpda2
  # Compute boundary crossing probability at updated bound
  cat("Original bound approximation:", a2_0, 
      "\nUpdated bound approximation:", a2_1
      )
  grid2_1 <- hupdate(theta = theta[2], I = info[2], a = -Inf, b = a2_1, 
                     thetam1 = theta[1], Im1 = info[1], gm1 = grid1_1)
  plower_1 <- grid2_1 %>% summarize(sum(h)) %>% as.numeric()
  cat("\nOriginal boundary crossing probability:", plower_0, 
      "\nUpdated boundary crossing probability:", plower_1,
      "\nTarget:", spend1, "\n"
      )
  ```
  
  - Confirm with `gs_power_npe()`
  
  ```{r, message = FALSE}
  gs_power_npe(theta = theta, theta1 = theta, info = info, binding = TRUE,
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfPower, total_spend = 0.025, param = 2),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfPower, total_spend = 0.1, param = 2)
  )
  ```
  
  
  # References

Package: gsdmvn
File: vignettes/NPEbounds.Rmd
Format: text
Content:
  ---
  title: "Computing Bounds Under Non-Constant Treatment Effect"
  author: "Keaven Anderson"
  date: "`r Sys.Date()`"
  output: rmarkdown::html_vignette
  bibliography: gsDesign.bib
  vignette: >
    %\VignetteIndexEntry{Computing Bounds Under Non-Constant Treatment Effect}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
  ---
  
  ```{r setup, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  ```
  
  # Overview
  
  We consider one- and two-sided hypothesis testing using a group sequential design with possibly non-constant treatment effect.
  That can be useful for situations such as an assumed non-proportional hazards model or using weighted logrank tests for a time-to-event endpoint. 
  Asymptotic distributional assumptions for this document have been laid out in the vignette *Non-Proportional Effect Size in Group Sequential Design.*
  In general, we assume $K\ge 1$ analyses with statistical information $\mathcal{I}_k$ and information fraction $t_k=\mathcal{I}_k/\mathcal{I}_k$ at analysis $k$, $1\le k\le K$.
  We denote the null hypothesis $H_{0}$: $\theta(t)=0$ and an alternate hypothesis $H_1$: $\theta(t)=\theta_1(t)$ for $t> 0$ where $t$ represents the information fraction for a study.
  While a study is planned to stop at information fraction $t=1$, we define $\theta(t)$ for $t>0$ since a trial can overrun its planned statistical information at the final analysis. 
  As before, we use a shorthand notation in to have $\theta$ represent $\theta()$, $\theta=0$ to represent 
  $\theta(t)\equiv 0$ for all $t$ and $\theta_1$ to represent $\theta_i(t_k)$, the effect size at analysis $k$, $1\le k\le K$.
  
  For our purposes, $H_0$ will represent no treatment difference, but it could represent a non-inferiority hypothesis.
  Recall that we assume $K$ analyses and bounds $-\infty \le a_k< b_k<\le \infty$ for $1\le k < K$ and $-\infty \le a_K\le b_K<\infty$. 
  We denote the probability of crossing the upper boundary at analysis $k$ without previously crossing a bound by
  
  $$\alpha_{k}(\theta)=P_{\theta}(\{Z_{k}\geq b_{k}\}\cap_{j=1}^{i-1}\{a_{j}\le Z_{j}< b_{j}\}),$$
  $k=1,2,\ldots,K.$
  The total probability of crossing an upper bound prior to crossing a lower bound is denoted by 
  
  $$\alpha(\theta)\equiv\sum_{k=1}^K\alpha_k(\theta).$$
  We denote the probability of crossing a lower bound at analysis $k$ without previously crossing any bound by
  
  $$\beta_{k}(\theta)=P_{\theta}((Z_{k}< a_{k}\}\cap_{j=1}^{k-1}\{ a_{j}\le Z_{j}< b_{j}\}).$$
  
  Efficacy bounds $b_k$, $1\le k\le K$, for a group sequential design will be derived to control Type I at some level $\alpha=\alpha(0)$.
  
  Lower bounds $a_k$, $1\le k\le K$ may be used to control boundary crossing probabilities under either the null hypothesis (2-sided testing), the alternate hypothesis or some other hypothesis (futility testing).
  
  Thus, we may consider up to 3 values of $\theta(t)$: 
  
  - under the null hypothesis $\theta_0(t)=0$ for computing efficacy bounds, 
  - under a value $\theta_1(t)$ for computing lower bounds, and
  - under a value $\theta_a(t)$ for computing sample size or power.
  
  We refer to the information under these 3 assumptions as $\mathcal{I}^{(0)}(t)$, $\mathcal{I}^{(1)}(t)$, and $\mathcal{I}^{(a)}(t)$, respectively. Often we will assume
  $\mathcal{I}(t)=\mathcal{I}^{(0)}(t)=\mathcal{I}^{(1)}(t)=\mathcal{I}^{(a)}(t).$
  
  We note that information may differ under different values of $\theta(t)$. 
  For fixed designs, \cite{LachinBook} computes sample size based on different variances under the null and alternate hypothesis.
  
  
  # Two-sided testing and design
  
  We denote an alternative $H_{1}$: $\theta(t)=\theta_1(t)$; we will always assume $H_1$ for power calculations and sometimes will use $H_1$ for controlling lower boundary $a_k$ crossing probabilities. 
  A value of $\theta(t)>0$ will reflect a positive benefit. 
  We will not restrict the alternate hypothesis to $\theta_1(t)>0$ for all $t$.
  The value of $\theta(t)$ will be referred to as the (standardized) treatment effect at information fraction $t$.
  
  We assume there is interest in stopping early if there is good evidence to reject one hypothesis in favor of the other. 
  
  If $a_k= -\infty$ at analysis $k$ for some $1\le k\le K$ then the alternate hypothesis cannot be rejected at analysis $k$; i.e., there is no futility bound at analysis $k$. 
  For $k=1,2,\ldots,K$, the trial is stopped at analysis $k$ to reject $H_0$ if $a_j<Z_j< b_j$, $j=1,2,\dots,i-1$ and $Z_k\geq b_k$. 
  If the trial continues until stage $k$ without crossing a bound and $Z_k\leq a_k$ then $H_1$ is rejected in favor of $H_0$, $k=1,2,\ldots,K$. 
  Note that if $a_K< b_K$ there is the possibility of completing the trial without rejecting $H_0$ or $H_1$. 
  
  
  ## Haybittle-Peto and spending bounds
  
  The recursive algorithm of the previous section allows computation of both spending bounds and Haybittle-Peto bounds.
  For a Haybittle-Peto efficacy bound, one would normally set $b_k=\Phi^{-1}(1-\epsilon)$ for $k=1,2,\ldots,K-1$ and some small $\epsilon>0$ such as $\epsilon= 0.001$ which yields $b_k=3.09$.
  While the original proposal was to use $b_K=\Phi^{-1}(1-\alpha)$ at the final analysis, to fully control one-sided Type I error at level $\alpha$ we suggest computing the final bound $b_K$ using the above algorithm so that $\alpha(0)=\alpha$.
  
  Bounds computed with spending $\alpha_k(0)$ at analysis $k$ can be computed by using equation (9) for $b_1$.
  Then for $k=2,\ldots,K$ the algorithm of the previous section is used.
  As noted by @JTBook, $b_1,\ldots,b_K$ if determined under the null hypothesis depend only on $t_k$ and $\alpha_k(0)$ with no dependence on $\mathcal{I}_k$, $k=1,\ldots,K$.
  When computing bounds based on $\beta_k(\theta)$, $k=1,\ldots,K$,  where some $\theta(t_k)\neq 0$ we have an additional dependency with $a_k$ depending not only on $t_k$ and $b_k$, $k=1,\ldots,K$, but also on the final total information $\mathcal{I}_K$.
  Thus, a spending bound under something other than the null hypothesis needs to be recomputed each time $\mathcal{I}_K$ changes, whereas it only needs to be computed once when $\theta(t_k)=0$, $k=1,\ldots,K$.
  
  ## Bounds based on boundary families
  
  Assume constants $b_1^*,\ldots,b_K^*$ and a total targeted one-sided Type I error $\alpha$.
  We wish to find $C_u$ as a function of $t_1,\ldots t_K$ such that if $b_k=C_ub_k^*$ then $\alpha(0)=\alpha.$
  Thus, the problem is to solve for $C_u$. If $a_k$, $k=1,2,\ldots,K$ are fixed then this is a simple root finding problem.
  Since one normally normally uses non-binding efficacy bounds, it will normally be the case that $a_k=-\infty$, $k=1,\ldots,K$ for this problem.
  
  Now we assume constants $a_k^*$ and wish to find $C_l$ such that if $a_k=C_la_k^*+\theta(t_k)\sqrt{\mathcal{I}_k}$ for $k=1,\ldots,K$ then
  $\beta(\theta)=\beta$. If we use the constant upper bounds from the previous paragraph, finding $C_l$ is a simple root-finding problem.
  
  For 2-sided symmetric bounds with $a_k=-b_k$, $k=1,\ldots,K$, we only need to solve for $C_u$ and again use simple root finding. 
  
  At this point, we do not solve for this type of bound for asymmetric upper and lower bounds.
  
  # Sample size
  
  For sample size, we assume $t_k$, and $\theta(t_k)$ $1,\ldots,K$ are fixed. 
  We assume $\beta(\theta)$ is decreasing as $\mathcal{I}$ is decreasing.
  This will automatically be the case when $\theta(t_k)>0$, $k=1,\ldots,K$ and for many other cases.
  Thus, the information required is done by a search for $\mathcal{I_K}$ that yields $\alpha(\theta)$ yields the targeted power.
  
  # References

Package: gsdmvn
File: vignettes/PowerEvaluationWithSpending.Rmd
Format: text
Content:
  ---
  title: "Power Evaluation with Spending bounds"
  subtitle: "Binomial Trial Example"
  output:
    html_vignette
  bibliography: gsDesign.bib
  vignette: >
    %\VignetteIndexEntry{Power Evaluation with Spending bounds}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  library(gsdmvn)
  ```
  
  
  ## Overview
  
  This vignette covers how to compute power or Type I error for a design derived with a spending bound. 
  We will write this with a general non-constant treatment effect using `gs_design_npe()` to derived the design under one parameter setting and computing power under another setting.
  We will use a trial with a binary endpoint to enable a full illustration.
  
  
  ## Scenario for consideration
  
  We consider a scenario largely based on the CAPTURE study @CAPTURE where the primary endpoint was a composite of death, acute myocardial infarction or the need for recurrent percutaneous intervention within 30 days of randomization.
  That is, we consider a 2-arm trial with an experimental arm and a control arm.
  The primary endpoint for the trial is a binary indicator for each participant if they have a failed outcome. 
  For this case, we consider the parameter $\theta = p_1 - p _2$ where $p_1$ denotes the probability that a trial participant in the control group experiences a failure and $p_2$ represents the same probability for a trial participant in the experimental group. 
  We will assume $K=3$ analyses after 350, 700, and 1400 patients have been observed with equal randomization between the treatment groups. 
  The study was designed with approximately 80% power (Type II error $\beta = 1 - 0.8 = 0.2$) and 2.5% one-sided Type I error ($\alpha = 0.025$) to detect a reduction from a 15% event rate ($p_1 = 0.15$) in the control group to 10% ($p_2 = 0.1$) in the experimental group.
  The parameter of interest is $\theta = p_1 - p_2$. We denote the alternate hypothesis as 
  $$H_1: \theta = \theta_1= p_1^1 - p_2^1 = 0.15 - 0.10 = 0.05$$ and the null hypothesis
  $$H_0: \theta = \theta_0 = 0 = p_1^0 - p_2^0$$ where $p^0_1 = p^0_2= (p_1^1+p_2^1)/2 = 0.125$ 
  as laid out in @LachinBook. 
  
  
  
  ```{r}
  library(gsdmvn)
  p0 <- 0.15 # assumed failure rate in control group
  p1 <- 0.10 # assumed failure rate in experimental group
  alpha <- 0.025 # Design Type I error
  beta <- 0.2 # Design Type II error for 80% power
  ```
  
  We note that had we considered a success outcome such as objective response in an oncology study, we would let $p_1$ denote the experimental group and $p_2$ the control group response rate.
  Thus, we always set up the notation so the $p_1>p_2$ represents superiority for the experimental group.
  
  ## Notation and Statistical Testing
  
  We let $X_{ij}\sim \hbox{Bernoulli}(p_i),$ $j=1,2,\ldots,n_{ik}$ denote independent random variables in the control ($i=1$) and experimental ($i=2$) groups through analysis $k=1,2,\ldots,K$ where $n_{i1} < n_{i2}<\ldots<n_K$.
  We let $Y_{ik}=\sum_{j=1}^{n_k}X_{ij}$.
  At analysis $k=1,2,\ldots$ and for $j=1,2$ we denote the proportion of failures in group $i$ at analysis $k$
  
  $$ \hat{p}_{ik}=Y_{ik}/n_{ik}.$$
  Estimating under null hypothesis $H_0: p_1=p_2\equiv  p_0$ we estimate
  
  $$ \hat{p}_{0k}=\frac{Y_{1k}+ Y_{2k}}{n_{1k}+ n_{2k}}=
  \frac{n_{1k}\hat p_{1k} + n_{2k}\hat p_{2k}}{n_{1k} + n_{2k}}. $$
  We note
  
  $$\hbox{Var}(\hat p_{ik})=\frac{p_{i}(1-p_i)}{n_{ik}},$$
  and its consistent estimatator
  $$\widehat{\hbox{Var}}(\hat p_{ik})=\frac{\hat p_{ik}(1-\hat p_{ik})}{n_{ik}},$$
  $i=1,2$, $k=1,2,\ldots,K$.
  Letting $\hat\theta_k=\hat p_{1k}-\hat p_{2k},$ we also have
  
  $$\sigma^2_k\equiv \hbox{Var}(\hat\theta_i)=\frac{p_1(1-p_1)}{n_{1k}}+\frac{p_2(1-p_2)}{n_{2k}},$$
  its consistent estimator 
  $$\hat\sigma^2_k=\frac{\hat p_{1k}(1-\hat p_{1k})}{n_{1k}}+\frac{\hat p_{2k}(1-\hat p_{2k})}{n_{2k}},$$
  and the corresponding null hypothesis estimator
  $$\hat\sigma^2_{0k}=\hat p_{0k}(1-\hat p_{0k})\left(\frac{1}{n_{1k}}+ \frac{1}{n_{2k}}\right),$$
  $k=1,2,\ldots,K$. Statistical information for each of these quantities and their corresponding estimators are denoted by
  
  $$\begin{align}
  \mathcal{I}_k = &1/\sigma^2_k,\\
  \mathcal{\hat I}_k = &1/\hat \sigma^2_k,\\
  \mathcal{I}_{0k} =& 1/ \sigma^2_{0k},\\
  \mathcal{\hat I}_{0k} =& 1/\hat \sigma^2_{0k},
  \end{align}$$
  $k=1,2,\ldots,K$. 
  Testing, as recommended by @LachinBook, is done with the large sample test with the null hypothesis variance estimate and without continuity correction:
  $$Z_k = \hat\theta_k/\hat\sigma_{0k}=\frac{\hat p_{1k} - \hat p_{2k}}{\sqrt{(1/n_{1k}+ 1/n_{2k})\hat p_{0k}(1-\hat p_{0k})} },$$
  which is asymptotically Normal(0,1) if $p_1=p_2$ and Normal(0, $\sigma_{0k}^2/\sigma_k^2$) more generally for any $p_1, p_2$, $k=1,2,\ldots,K$.
  We note that $\chi^2=Z^2_k$ is the $\chi^2$ test without continuity correction as recommended by @CCmyth. 
  Note finally that this extends in a straightforward way the non-inferiority test of @FarringtonManning if the null hypothesis is $\theta = p_1 - p_2 - \delta = 0$ for some non-inferiority margin $\delta > 0$; $\delta < 0$ would correspond to what is referred to as super-superiority @Chan2002, requiring that experimental therapy has been shown to be superior to control by at least a margin $-\delta>0$.
  
  ## Power Calculations
  
  We begin with a fixed design to simplify.
  We follow the approach of @LachinBook to compute power accounting for the different variance (statistical information) computations under the null hypothesis noted in the previous section.
  Noting the asymptotic equivalence
  
  $$Z_k\approx \hat\theta_k/\sigma_{0k}=\frac{\hat p_{1k} - \hat p_{2k}}{\sqrt{(1/n_{1k}+ 1/n_{2k})p_{0}(1- p_0)} }.$$
  We denote $n_k=n_{1k}+n_{2k},$ $k=1,2,\ldots, K$ and assume a constant proportion $\xi_i$ randomized to each group $i=1,2.$ Thus, 
  
  $$Z_k\approx \frac{\sqrt{n_k}(\hat p_{1k} - \hat p_{2k})}{\sqrt{(1/\xi_1+ 1/\xi_2)p_{0}(1- p_0)} }.$$
  
  we have the asymptotic distribution
  
  $$Z_k\sim\hbox{Normal}\left(\sqrt{n_k}\frac{p_1 - p_2}{\sqrt{(1/\xi_1+ 1/\xi_2) p_0(1- p_0)} },\sigma^2_{0k}/\sigma^2_{1k}\right).$$
  We note that 
  
  $$ \sigma^2_{0k}/\sigma^2_{1k} = \frac{ p_0(1-p_0)\left(1/\xi_1+ 1/\xi_2\right)}{p_1(1-p_1)/\xi_1+p_2(1-p_2)/\xi_2}.$$ 
  We also note by definition that $\sigma^2_{0k}/\sigma^2_{1k}=\mathcal I_k/\mathcal I_{0k}.$
  Based on an input $p_1, p_2, \xi_1, \xi_2 = 1-\xi_1, n_k$ we will compute $\theta, \mathcal{I}_k, \mathcal{I}_{0k}$, $k=1,2,\ldots,K$.
  
  
  ```{r}
  library(tibble)
  gs_info_binomial <- function(p1, p2, xi1, n, delta = NULL){
    if (is.null(delta)) delta <- p1 - p2
    # Compute (constant) effect size at each analysis theta
    theta <- rep(p1 - p2, length(n))
    # compute null hypothesis rate, p0
    p0 <- xi1 * p1 + (1 - xi1) * p2
    # compute information based on p1, p2
    info <-  n / (p1 * (1 - p1) / xi1 + p2 * (1 - p2) / (1 - xi1))
    # compute information based on null hypothesis rate of p0
    info0 <- n / (p0 * (1 - p0)*(1 / xi1 + 1 / (1 - xi1)))
    # compute information based on H1 rates of p1star, p2star
    p1star <- p0 + delta * xi1
    p2star <- p0 - delta * (1 - xi1)
    info1 <-  n / (p1star * (1 - p1star) / xi1 + p2star * (1 - p2star) / (1 - xi1))
    return(tibble(Analysis = 1:length(n),
                  n = n,
                  theta = theta,
                  theta1 = rep(delta, length(n)),
                  info = info,
                  info0 = info0,
                  info1 = info1))
  }
  ```
  
  
  For the CAPTURE trial, we have 
  
  
  ```{r}
  h1 <- gs_info_binomial(p1 = .15, p2 = .1, xi1 = .5, n = c(350, 700, 1400))
  h1
  ```
  
  Now we examine information for a smaller assumed treatment difference than the alternative:
  
  ```{r}
  h <- gs_info_binomial(p1 = .15, p2 = .12, xi1 = .5, delta = .05, n = c(350, 700, 1400))
  h
  ```
  
  We can plug these into `gs_power_npe()` with the intended spending functions.
  We begin with power under the alternate hypothesis
  
  ```{r}
  gs_power_npe(theta = h1$theta, theta1 = h1$theta, info = h1$info,
               info0 = h1$info0, info1 = h1$info1,
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfHSD, param = -2, total_spend = 0.2)
  )
  ```
  
  ```{r}
  gs_power_npe(theta = h$theta, theta1 = h$theta1, info = h$info,
               info0 = h$info0, info1 = h$info1,
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
               lower = gs_spending_bound,
               lpar = list(sf = gsDesign::sfHSD, param = -2, total_spend = 0.2)
  )
  ```
  
  # References

Package: gsdmvn
File: vignettes/QuickStart.Rmd
Format: text
Content:
  ---
  title: "Quick Start for NPH Sample Size and Power"
  output:
    html_vignette:
      df_print: paged
      toc: yes
      toc_depth: '2'
  bibliography: gsDesign.bib
  vignette: |
    %\VignetteIndexEntry{Quick Start for NPH Sample Size and Power}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
  ---
  
  ```{r, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  ```
  
  # Overview
  
  We provide simple examples for use of the **gsdmvn** package for deriving fixed and group sequential designs under non-proportional hazards.
  The piecewise model for enrollment, failure rates, dropout rates and changing hazard ratio over time allow great flexibility in design assumptions. 
  Users are encouraged to suggest features that would be of immediate and long-term interest to add.
  
  Topics included here are:
  
  - Packages required and how they are used.
  - Specifying enrollment rates.
  - Specifying failure and dropout rates with possibly changing hazard ratio over time.
  - Deriving a fixed design with no interim analysis.
  - Simple boundary specification for group sequential design.
  - Deriving a group sequential design under non-proportional hazards.
  - Displaying design properties.
  - Design properties under alternate assumptions.
  - Differences from **gsDesign**.
  - Future enhancement priorities.
  
  All of these items are discussed briefly to enable a quick start for early adopters while also suggesting the ultimate possibilities that the software enables.
  Finally, while the final section provides current enhancement priorities, potential topic-related enhancements are discussed throughout the document.
  
  # Packages Used
  
  - The **gsdmvn** package is used here to implement group sequential distribution theory under non-proportional hazards and to derive a wide variety of boundary types for group sequential designs.
  - The **gsDesign** package is used as a check for results under proportional hazards as well as a source from deriving bounds using spending functions.
  - The **gsDesign2** package provides computations to compute expected event accumulation and average hazard ratio over time; these are key inputs to the group sequential distribution parameters.
  - The **simtrial** package is used to verify design properties using simulation.
  
  The **gsdmvn** package will likely will likely be incorporated eventually into the **gsDesign2** package, resulting in a fully featured design package.
  However, features and implementation in **gsdmvn** will be allowed to change as needed during the agile rapid development phase.
  
  
  ```{r setup, warning = FALSE}
  library(gsdmvn)
  library(gsDesign)
  library(gsDesign2)
  library(simtrial)
  library(knitr)
  library(dplyr)
  ```
  
  # Enrollment Rates
  
  Piecewise constant enrollment rates are input in a tabular format.
  Here we assume enrollment will ramp-up with 25%, 50%, and 75% of the final enrollment rate for 2 months each followed by a steady state 100% enrollment for another 6 months.
  The rates will be increased later to power the design appropriately.
  However, the fixed enrollment rate periods will remain unchanged.
  
  ```{r}
  enrollRates <- tibble::tibble(Stratum = "All", duration = c(2, 2, 2, 6), rate = (1:4)/4)
  enrollRates
  ```
  
  
  # Failure and Dropout Rates
  
  Constant failure and dropout rates are specified by study period and stratum; we consider a single stratum here.
  A hazard ratio is provided for treatment/control hazard rate for each period and stratum.
  The dropout rate for each period is assumed the same for each treatment group; this restriction could be eliminated in a future version, if needed.
  Generally, we take advantage of the identity for an exponential distribution with median $m$, the corresponding failure rate $\lambda$ is
  
  $$\lambda = \log(2) / m.$$
  
  We consider a control group exponential time-to-event with a 12 month median.
  We assume a hazard ratio of 1 for 4 months, followed by a hazard ratio of 0.6 thereafter.
  Finally, we assume a low 0.001 exponential dropout rate for both treatment groups.
  
  
  ```{r}
  medianSurv <- 12
  failRates = tibble::tibble(Stratum = "All",
                             duration = c(4, Inf),
                             failRate = log(2) / medianSurv,
                             hr = c(1, .6),
                             dropoutRate = .001)
  failRates
  ```
  
  # Fixed Design
  
  Under the above enrollment, failure and dropout rate assumptions we now derive sample size for a trial targeted to complete in 36 months with no interim analysis, 90% power and 2.5% Type I error.
  The parameter `upar = qnorm(1 - .025)` in this case is the upper bound for the single analysis, while the parameter `beta = 0.1` is the Type II error (1 - power).
  The information fraction `IF = 1` at the final analysis is just a statement that the analysis is done with 100% of the design planned endpoints.
  Finally, `lpar = -Inf` just means there is no futility bound for the design.
  The design can be derived under these assumptions.
  
  ```{r}
  # Type I error
  alpha <- .025
  design <-
     gs_design_ahr(enrollRates = enrollRates,
                   failRates = failRates,
                   alpha = alpha,
                   beta = .1, # Type II error = 1 - power
                   analysisTimes = 36, # Planned trial duration
                   IF = 1, # single analysis at information-fraction of 1
                   upar = qnorm(1 - alpha), # Final analysis bound
                   lpar = -Inf # No futility bound
                 )
  ```
  
  There are three components to the resulting design.
  
  ```{r}
  names(design)
  ```
  
  First, the enrollment rates for each period have been increased proportionately to size the trial for the desired properties; the duration for each enrollment rate has not changed.
  
  ```{r}
  design$enrollRates %>% kable()
  ```
  
  The output `failRates` are the same as input. 
  The output bounds are just an upper bound of $\Phi^{-1}(1-\alpha)$ and a lower bound of $-\infty$.
  The targeted time of the single analysis is in the `Time` column. 
  The average hazard ratio used to compute the sample size based on the @Schoenfeld1981 approximation is in `AHR`. The targeted events at the analysis is in the column `Events`.
  The targeted time of the analysis is in `Time`.
  The power is in the `Probability` column on the row with the `Upper` bound. The parameter `theta` is the natural parameter for the effect size as outlined in @JTBook and elsewhere in this package; in this case it is `-log(AHR)`.
  The variables `info` and `info0` are statistical information at each analysis under the alternate and null hypothesis, respectively.
  
  ```{r,message=FALSE,warning=FALSE}
  design$bounds 
  ```
  
  We note that the targeted `Events` are approximately what would be proposed with the @Schoenfeld1981 formula:
  
  ```{r}
  gsDesign::nEvents(hr=design$bounds$AHR)
  ```
  
  The difference is that `gs_design_ahr()` accounts for both a null and alternate hypothesis variance estimate for `theta=log(AHR)` at each analysis to yield a slightly more conservative event target due to slower accumulation of statistical information under the alternate hypothesis. See @LachinBook for this approach for fixed design; to our knowledge, this has not previously been extended to group sequential design. Due to simplicity, there may be reasons to allow approaches with a single variance estimate in future releases.
  
  Finally, we note that with a shorter trial duration of 30 months, we need both a larger sample size and targeted number of events due to a larger expected AHR at the time of analysis:
  
  ```{r}
  gs_design_ahr(enrollRates = enrollRates,
              failRates = failRates,
              alpha = alpha,
              beta = .1, # Type II error = 1 - power
              analysisTimes = 30, # Planned trial duration
              IF = 1, # single analysis at information-fraction of 1
              upar = qnorm(1 - alpha), # Final analysis bound
              lpar = -Inf # No futility bound
            )$bounds %>% kable(digits=c(0,0,1,0,0,3,4,3,3,2,2))
  ```
  
  # Group Sequential Design
  
  We will not go into detail for group sequential designs here.
  In brief, however, a sequence of tests $Z_1, Z_2,\ldots, Z_K$ that follow a multivariate normal distribution are peformed to test if a new treatment is better than control @JTBook.
  We assume $Z_k>0$ is favorable for the experimental treatment.
  Generally Type I error for this set of tests will be controlled under the null hypothesis of no treatment difference by a sequence of bounds $b_1, b_2,\ldots,b_K$ such that for a chosen Type I error $\alpha > 0$ we have
  
  $$\alpha = 1 - P_0(\cap_{k=1}^K Z_k < b_k)$$
  Where $P_0()$ refers to a probability under the null hypothesis.
  This is referred to as a non-binding bound since it is assumed the trial will not be stopped early for futility if some $Z_k$ is small.
  
  ## Simple Efficacy Bound Definition
  
  
  @LanDeMets developed the spending function method for deriving group sequential bounds.
  This involves use of a non-decreasing spending function $f(t)$ for $t\ge 0$ where $f(0)=0$ and $f(t)=\alpha$ for $t \ge 1$.
  Suppose for $K>0$ analyses are performed when proportion $t_1< t_2 <\ldots t_K=1$ of some planned statistical information (e.g., proportion of planned events for a time-to-event endpoint trial for proportion of observations for a binomial or normal endpoint). 
  Bounds through the first $k$ analyses $1\le k\le K$ are recursively defined by the spending function and the multivariate normal distribution to satisfy
  
  $$f(t_k) = 1 - P_0(\cap_{j=1}^k Z_j < b_j).$$
  For this quick start, we will only illustrate this type of efficacy bound.
  
  Perhaps the most common spending function for this approach is the @LanDeMets approximation to the O'Brien-Fleming bound with
  
  $$f(t) = 2-2\Phi\left(\frac{\Phi^{-1}(1-\alpha/2)}{t^{1/2}}\right).$$
  
  ```{r, echo=FALSE, fig.width=6.5}
  t <- (0:50)/50
  plot(t, 2 - 2 * pnorm(qnorm(1-.0125)/sqrt(t)), type="l", ylab = "f(t)", xlab = "t")
  ```
  
  Suppose $K=3$ and $t_1=0.5$, $t_2 = 0.75$, $t_3 = 1$.
  We can define bounds with the **gsDesign** group sequential design function `gsDesign()` and Lan-DeMets O'Brien-Fleming spending function for $\alpha = 0.025$.
  
  ```{r}
  b <- gsDesign::gsDesign(k=3, timing = c(0.5, 0.75, 1), test.type=1, alpha = 0.025, sfu = gsDesign::sfLDOF)$upper$bound
  b
  ```
  
  Now we can define a one-sided group sequential design under the same enrollment, failure and dropout assumptions used previously.
  
  ```{r}
  design1s <- gs_design_ahr(enrollRates = enrollRates,
                            failRates = failRates,
                            analysisTimes = 36, # Trial duration
                            upper = gs_spending_bound,
                            upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
                            lower = gs_b,
                            lpar = rep(-Inf, 3), # No futility bound
                            IF = c(.5, .75, 1)
                          )
  ```
  
  Bounds at the 3 analyses are as follows; note that expected sample size at time of each data cutoff for analysis is also here in `N`. We filter on the upper bound so that lower bounds with `Z = -Inf` are not shown.
  
  ```{r}
  design1s$bounds %>% filter(Bound == "Upper") %>% kable(digits=c(0,0,1,0,0,3,4,3,3,2,2))
  ```
  
  The boundary crossing probabilities column labeled `Probability` here are under the alternate hypothesis.
  These are cumulative probabilities totaling 0.9 at the final analysis, representing 90% power.
  If we wish to see the boundary probabilities under the null hypothesis, we can change the hazard ratio to 1 in the input `failRates`, use the output `enrollRates` from `designs` as follows.
  
  ```{r}
  gs_power_ahr(enrollRates = design1s$enrollRates,
               failRates = design1s$failRates %>% mutate(hr = 1),
               upper = gs_spending_bound,
               upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
               lpar = rep(-Inf, 3), # No futility bound
               events = design1s$bound$Events[1:3]
  ) %>% filter(Bound == "Upper") %>% kable(digits=c(0,0,1,0,0,3,4,3,3,2,2))
  ```
  
  
  
  ## Two-Sided Testing
  
  We will consider both symmetric and asymmetric 2-sided designs.
  
  
  ### Symmetric 2-sided bounds
  
  For a symmetric design, we can again define a bound using `gsDesign::gsDesign()`.
  For this example, the bound is identical to `b` above to the digits calculated.
  
  ```{r}
  b2 <- gsDesign::gsDesign(test.type = 2, sfu = sfLDOF, alpha = 0.025, timing = c(.5, .75, 1))$upper$bound
  b2
  ```
  
  Now we replicate with 'gs_design_ahr()`:
  
  
  ```{r}
  design2ss <- gs_design_ahr(enrollRates = enrollRates,
                             failRates = failRates,
                             analysisTimes = 36, # Trial duration
                             IF = c(.5, .75, 1), # Information fraction at analyses
                             upper = gs_spending_bound,
                             upar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
                             lower = gs_spending_bound,
                             lpar = list(sf = gsDesign::sfLDOF, total_spend = 0.025),
                             h1_spending = FALSE
                          )
  ```
  
  
  Design bounds are confirmed with: 
  
  ```{r, message=FALSE}
  design2ss$bounds %>% kable(digits=c(0,0,1,0,0,3,4,3,3,2,2))
  ```
  
  The bounds can be plotted easily:
  
  ```{r,fig.width=6.5}
  ggplot(data = design2ss$bound, aes(x=Events, y=Z, group = Bound)) + geom_line(aes(linetype=Bound)) + geom_point() +
    ggtitle("2-sided symmetric bounds with O'Brien-Fleming-like spending")
  ```
  
  
  ### Asymmetric 2-sided bounds
  
  Asymmetric 2-sided designs are more common than symmetric since the objectives of the two bounds tend to be different.
  There is often caution to analyze early for efficacy or to use other than a conservative bound; both of these principles have been used with the example designs so far.
  Stopping when there is a lack of benefit for experimental treatment over control or for an overt indication of an unfavorable trend generally might be examined early and bounds be less stringent.
  We will add an early futility analysis where if there is a nominal 1-sided p-value of 0.05 in the wrong direction ($Z=\Phi^{-1}(0.05)$ after 30% or 50% of events have accrued.
  This might be considered a *disaster check*. After this point in time, there may not be a perceived need for further futility analysis.
  For efficacy, we add an infinite bound at this first interim analysis.
  
  ```{r}
  b2sa <- c(Inf, b) # Same efficacy bound as before
  a2sa <- c(rep(qnorm(.05), 2), rep(-Inf, 2)) # Single futility analysis bound
  
  design2sa <- gs_design_nph(enrollRates = enrollRates,
                          failRates = failRates,
                          analysisTimes = 36, # Trial duration
                          upper = gs_b, upar = b2sa,
                          lower = gs_b, lpar = a2sa, # Asymmetric 2-sided bound
                          IF = c(.3, .5, .75, 1)
                          )
  ```
  
  We now have a slightly larger sample size to account for the possibility of an early futility stop.
  
  ```{r}
  design2sa$enrollRates %>% summarise(N = ceiling(sum(rate * duration)))
  ```
  
  Bounds are now:
  
  ```{r}
  design2sa$bounds %>% filter(abs(Z) < Inf) %>% kable(digits=c(0,0,2,4,2,1,2,1,1,1))
  ```
  
  We see that there does not need to be a (finite) stopping rule for each bound at each analysis.
  That is, there is a futility bound only at the first 2 analyses and an efficacy bound only for the last 3 analyses.
  We still have the targeted power. 
  The efficacy bound has not changed from our first design.
  The reason for not changing it is to address regulator concerns that such bounds are not always stopped for.
  However, if the bound is obeyed, the Type I error can be seen to be slightly reduced as follows.
  This price is generally acceptable for regulatory acceptance and operational flexibility that is enabled by controlling Type I error even if the futility bound is not obeyed.
  
  ```{r}
  events <- (design2sa$bounds %>% filter(Bound == "Upper"))$Events
  gs_power_nph(enrollRates = design1s$enrollRates,
             failRates = design2sa$failRates %>% mutate(hr = 1),
             upar = b2sa,
             lpar = a2sa,
             events = events,
             maxEvents = max(events)) %>% 
  # filter eliminates bounds that are infinite
    filter(abs(Z) < Inf)  %>% kable(digits=c(0,0,2,4,1,1,2,2,1,1))
  ```
  
  # Confirmation by Simulation
  
  We do a small simulation to approximate the boundary crossing probabilities just shown in the 2-sided asymmetric design `design2sa`.
  First, we generate test statistics for each analysis for a number of simulated trials.
  
  ```{r}
  fr <- simfix2simPWSurv(failRates = failRates)
  nsim <- 200 # Number of trial simulations
  simresult <- NULL
  N <- ceiling(design2sa$enrollRates %>% summarize(N = sum(rate/2 * duration)))*2
  K <- max(design2sa$bounds$Analysis)
  events <- ceiling(sort(unique(design2sa$bounds$Events)))
  
  for(i in 1:nsim){
    sim <- simPWSurv(n = as.numeric(N),
                     enrollRates = design2sa$enrollRates,
                     failRates = fr$failRates,
                     dropoutRates = fr$dropoutRates
                    )
    for(k in 1:K){
      Z <- sim %>% cutDataAtCount(events[k]) %>% # cut simulation for analysis at targeted events
           tensurv(txval = "Experimental") %>% tenFH(rg = tibble(rho = 0, gamma = 0))
      simresult <- rbind(simresult,
                         tibble(sim = i, k = k, Z = -Z$Z)) # Change sign for Z
    }
  }
  ```
  
  Now we analyze the individual trials and summarize results. A larger simulation would be required to more accurately assess the asymptotic approximation for boundary crossing probabilities.
  
  ```{r}
  bds <- tibble::tibble(k = sort(unique(design2sa$bounds$Analysis)),
              upper = (design2sa$bounds %>% filter(Bound == "Upper"))$Z,
              lower = (design2sa$bounds %>% filter(Bound == "Lower"))$Z
  )
  trialsum <- simresult %>% 
              full_join(bds, by = "k") %>%
              filter(Z < lower | Z >= upper | k == K) %>%
              group_by(sim) %>%
              slice(1) %>% 
              ungroup()
  trialsum %>% summarize(nsim = n(),
                         "Early futility (%)" = 100 * mean(Z < lower),
                         "Power (%)" = 100 * mean(Z >= upper))
  ```
  
  ```{r}
  xx <- trialsum %>% mutate(Positive = (Z >= upper))
  table(xx$k, xx$Positive)
  ```
  
  
  # Differences From **gsDesign**
  
  The sample size computation from the **gsdmvn** package is slightly different than from the **gsDesign** package for proportional hazards model.
  We demonstrate this with the bounds for the 1-sided test above under a proportional hazards model with an underlying hazard ratio of 0.7.
  
  ```{r}
  design1sPH <- gs_design_nph(enrollRates = enrollRates,
                          failRates = failRates %>% mutate(hr = .7),
                          analysisTimes = 36, # Trial duration
                          upar = b,
                          lpar = rep(-Inf, 3), # No futility bound
                          IF = c(.5, .75, 1)
                          )
  design1sPH$enrollRates %>% kable()
  ```
  
  This results in total sample size:
  
  ```{r}
  design1sPH$enrollRates %>% tail(1) %>% select(N)
  ```
  
  Now we derive a design with the same targeted properties using the **gsDesign** package.
  
  ```{r,warning=FALSE}
  x <- gsSurv(k = 3,
              test.type = 1,
              alpha = .025,
              beta = .1,
              timing = c(.5, .75, 1),
              sfu = sfLDOF,
              lambda = log(2) / medianSurv,
              hr = .7, 
              eta = .001, # dropout rate
              gamma = c(.25, .5, .75, 1),
              R = c(2, 2, 2, 6),
              minfup = 24,
              T = 36)
  gsBoundSummary(x) %>% kable(row.names = FALSE)
  ```
  
  Enrollment rates are:
  
  ```{r}
  x$gamma %>% kable()
  ```
  
  
  
  # References

Package: gsdmvn
File: vignettes/SpendingTimeExamples.Rmd
Format: text
Content:
  ---
  title: "Spending Time Examples"
  author: "Keaven Anderson"
  date: "`r Sys.Date()`"
  output:
    html_document:
      code_folding: hide
      toc: TRUE
      toc_float: TRUE
  bibliography: gsDesign.bib
  vignette: >
    %\VignetteIndexEntry{Spending time examples}
    %\VignetteEncoding{UTF-8}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r setup, include=FALSE}
  knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning = FALSE)
  ```
  
  
  ```{r}
  library(gsdmvn)
  library(gsDesign)
  library(tibble)
  library(dplyr)
  library(gt)
  ```
  
  # Overview
  
  There are multiple scenarios where event-based spending for group sequential designs has limitations in terms of ensuring adequate follow-up and in ensuring adequate spending is preserved for the final analysis.
  Example contexts where this often arises is in trials where 
  
  - there may be a delayed treatment effect,
  - control failure rates are different than expected, and
  - multiple hypotheses are being tested.
  
  In general, for such situations we have found that ensuring both adequate follow-up duration and an adequate number of events is important to fully evaluate the potential effectiveness of a new treatment.
  For testing of multiple hypotheses, carefully thinking through possible spending issues can be critical.
  In addition, for group sequential trials, preserving adequate $\alpha$-spending for a final evaluation of a hypothesis is important and difficult to do using traditional event-based spending.
  
  In this document, we outline three examples to demonstrate these issues:
  
  - For a delayed effect scenario we demonstrate:
      - the importance of both adequate events and adequate follow-up duration to ensure power in a fixed design, and
      - the importance of guaranteeing a reasonable amount of $\alpha$-spending for the final analysis in a group sequential design.
  - For a trial examining an outcome in a biomarker positive and overall populations, we show the importance of considering how the design reacts to incorrect design assumptions on biomarker prevalence.
  
  For the group sequential design options, we demonstrate that the concept of spending time is an effective way to adapt.
  Traditionally @LanDeMets, spending has been done according to targeting a specific number of events for an outcome at the end of the trial.
  However, for delayed treatment effect scenarios there is substantial literature (e.g., @NPHWG2020sim, @NPHWG2021Design) documenting the importance of adequate follow-up duration in addition to requiring an adequate number of events under the traditional proportional hazards assumption.
  
  While other approaches could be taken, we have found the spending time approach generalizes well for addressing a variety of scenarios.
  The fact that spending does not need to correspond to information fraction was perhaps first raised by @LanDeMets1989 where calendar-time spending was discussed.
  However, we note that @PLWBook have raised other scenarios where spending alternatives are considered. 
  Two specific spending approaches are suggested here:
  
  - Spending according to the minimum of planned and observed event counts. This is suggested for the delayed effect examples.
  - Spending with a common spending time across multiple hypotheses; e.g., in the multiple population example, spending in the overall population at the same rate as in the biomarker positive subgroup regardless of event counts over time in the overall population. This is consistent with @FPG as applied when multiple experimental treatments are compared to a common control. Spending time in this case corresponds to the approach of @FHO where fixed incremental spending is set for a potentially variable number of interim analyses.
  
  
  This document is fairly long in that it demonstrates a number of scenarios relevant to the spending time concept.
  The layout is intended to make it as easy as possibly to focus on the individual examples for those not interested in the full review.
  Code is available to unhide for those interested in implementation.
  Rather than bog down the conceptual discussion with implementation details, we have tried to provide sufficient comments in the code to guide implementation for those who are interested in that.
  
  
  # Delayed effect scenario 
  
  
  We consider an example in a single stratum where there is a possibility of a delayed treatment effect.
  The next two sections will consider both a 1) fixed design with no interim analysis, and 2) a design with interim analysis.
  Following are the common assumptions:
  
  - The control group time-to-event is exponentially distributed with a median of 12 months.
  - 2.5% one-sided Type I error.
  - 90% power.
  - A constant enrollment rate with an expected enrollment duration of 12 months.
  - A targeted trial duration of 30 months.
  - A delayed effect for the experimental group compared to control, with a hazard ratio of 1 for the first 4 months and a hazard ratio of 0.6 thereafter.
  
  The restrictions on constant control failure rate, only two hazard ratio time intervals and constant enrollment are not required, but simplify the example.
  The approach taken uses an average-hazard ratio approach for approximating treatment effect as in @Mukhopadhyay2020 and the asymptotic group sequential theory of @Tsiatis.
  
  ```{r}
  m <- 12 # Control median
  enrollRates <- tibble(Stratum="All", duration = 12, rate = 1)
  failRates <- tibble(Stratum="All", duration = c(4, 100), hr = c(1, .6), 
                      failRate = log(2) / m, dropoutRate = .001)
  ```
  
  # Fixed design, delayed effect
  
  
  ```{r}
  # Output table function for fixed design
  
  table_fixed_design <- function(x, enrollRates){
    N <- sum(enrollRates$rate * enrollRates$duration)
    x %>%
      filter(Bound == "Upper") %>%
      transmute(Time = Time, N = ceiling(N), Events = ceiling(Events), "Nominal p" = pnorm(-Z), AHR = AHR, Power = Probability) %>%
    gt() %>% 
    fmt_number(columns = c("N", "Events"), decimals=0) %>%
    fmt_number(columns = "Time", decimals = 1) %>%
    fmt_number(columns = c("Nominal p", "Power", "AHR"), decimals=3)
  }
  ```
  
  The sample size and events for this design are shown below.
  We see that the average hazard ratio (AHR) under the above assumptions is 0.703, part way between the early HR of 1 and the later HR of 0.6 assumed for experimental versus control therapy.
  
  ```{r}
  # Bounds for fixed design are just a fixed bound for nominal p = 0.025, 1-sided
  Z_025 <- qnorm(.975)
  
  # Fixed design, single stratum
  # Find sample size for 30 month trial under given 
  # enrollment and sample size assumptions
  xx <- gs_design_ahr(enrollRates, 
                      failRates, 
                      analysisTimes= 30,
                      upar = Z_025, 
                      lpar = Z_025)
  xx$bounds %>% table_fixed_design(enrollRates)
  ```
  
  ## Power when assumptions design are wrong
  
  ### Scenario with less experimental benefit
  
  If we assume instead that the effect delay is 6 months instead of 4 and the control median is 10 months instead of 12,  there is a substantial impact on power.
  Here, we have assumed only the targeted events is required to do the final analysis resulting in an expected final analysis time of 25 months instead of the planned 30 and an average hazard ratio of 0.78 at the expected time of analysis rather than the targeted average hazard ratio of 0.70 under the original assumptions.
  
  
  ```{r}
  am <- 10 # Alternate control median
  failRates$duration[1] <- 6
  failRates$failRate <- log(2) / am
  yy <- 
    gs_power_ahr(
        enrollRates = xx$enrollRates,
        failRates = failRates,
        events = xx$bounds$Events,
        upar = Z_025,
        lpar = Z_025
  )
  yy %>% table_fixed_design(xx$enrollRates)
  ```
  
  Now we also require 30 months trial duration in addition to the targeted events.
  This improves the power from 63% above to 76% with an increase from 25 to 30 months duration and 340 to 377 expected events, an important gain.
  This is driven both by the average hazard ratio of 0.78 above compared to 0.76 below and by the increased expected number of events.
  It also ensures adequate follow-up to better describe longer-term differences in survival; this may be particularly important if early follow-up suggests a delayed effect or crossing survival curves.
  Thus, the adaptation of event-based design based to also require adequate follow-up can help ensure power for a large clinical trial investment where there is an clinically relevant underlying survival benefit. 
  
  ```{r}
  yy <- 
    gs_power_ahr(
        enrollRates = xx$enrollRates,
        failRates = failRates,
        events = xx$bounds$Events,
        analysisTimes = 30,
        upar = Z_025,
        lpar = Z_025
  )
  yy %>% table_fixed_design(xx$enrollRates)
  ```
  
  ## Scenario with low control event rates
  
  Now we assume a longer than planned control median, 16 months to demonstrate the value of retaining the event count requirement.
  If we analyze after 30 months, the power of the trial is 87% with 288 events expected.
  
  ```{r}
  am <- 16 # Alternate control median
  failRates$failRate <- log(2) / am
  failRates$duration[1] <- 4
  yy <- 
    gs_power_ahr(
        enrollRates = xx$enrollRates,
        failRates = failRates,
        events = 1,
        analysisTimes = 30,
        upar = Z_025,
        lpar = Z_025
  )
  yy %>% table_fixed_design(xx$enrollRates)
  ```
  
  If we also require adequate events, we restore power to 94.5, above the originally targeted level of 90%.
  The cost is that the expected trial duration becomes 38.5 months rather than 30; however, since the control median is now larger, the additional follow-up should be useful to characterize tail behavior.
  Note that for this scenario we are likely particularly interested in retaining power as the treatment effect is actually stronger than the original alternate hypothesis.
  Thus, for this example, the time cutoff alone would not have ensured sufficient follow-up to power the trial.
  
  
  ```{r}
  yy <- 
    gs_power_ahr(
        enrollRates = xx$enrollRates,
        failRates = failRates,
        events = xx$bounds$Events,
        analysisTimes = 30,
        upar = Z_025,
        lpar = Z_025
  )
  yy %>% table_fixed_design(xx$enrollRates)
  ```
  ## Conclusions for fixed design
  
  In summary, we have demonstrated the value of requiring both adequate events and adequate follow-up duration over an approach where the analysis is done with only one of these requirements.
  Requiring both will retain both power and important treatment benefit characterization over time when there is potential for delayed onset of a positive beneficial treatment effect.
  
  
  # Group sequential design
  
  ## Alternative spending strategies
  
  We extend the above design to detect a delayed effect to a group sequential design with a single interim analysis after 80% of the final planned events have accrued.
  We will assume the final analysis will require both the targeted trial duration and events based on the fixed design based on the evaluations above.
  We assume the efficacy bound uses the @LanDeMets spending function approximating an O'Brien-Fleming bound.
  No futility bound is planned, with the exception of a demonstration for one scenario.
  The interim analysis is far enough into the trial so that there is a substantial probability of stopping early under design assumptions.
  
  Coding for the different strategies must be done carefully.
  At the time of design, we specify only the spending function when specifying the use of information fraction for design. 
  
  ```{r}
  # Spending for design with planned information fraction
  upar_design_IF <- list(
                         # total_spend represents one-sided Type I error
                         total_spend = 0.025,
                         # Spending function and associated 
                         # parameter (NULL, in this case)
                         sf = sfLDOF, param = NULL,
                         # Do NOT specify spending time here as it will be set
                         # by information fraction specified in call to
                         # gs_design_ahr()
                         timing = NULL,
                         # Do NOT specify maximum information here as it will be
                         # set as the design maximum information
                         max_info = NULL
  )
  ```
  
  If we wished to use 22 and 30 months as calendar analysis times and use calendar fraction for spending, we would need to specify spending time for the design.
  
  
  ```{r}
  upar_design_CF <- upar_design_IF
  # Now switch spending time to calendar fraction
  upar_design_CF$timing <- c(22, 30)/30
  ```
  
  Next we show how to set up information-based spending for power calculation when timing of analysis is not based on information fraction; e.g., we will propose requiring not only achieving planned event counts, but also planned study duration before an analysis is performed.
  It is critical to set the maximum planned information to update the information fraction calculation in this case.
  
  ```{r}
  # We now need to change max_info from spending as specified for design
  upar_actual_IF <- upar_design_IF
  # Note that we still have timing = NULL, unchanged from information-based design
  upar_actual_IF <- NULL
  # Replace NULL maximum information with planned maximum null hypothesis
  # information from design
  # This max will be updated for each planned design later
  upar_actual_IF$max_info <- 100
  ```
  
  The final case will be to replace information fraction for a design to a specific spending time which will be plugged into the spending function to compute incremental $\alpha$-spending for each analysis.
  For our case, we will use planned information fraction from the design, which is 0.8 at the interim analysis and 1 for the final analysis.
  This will be used regardless of what scenario we are using to compute power, but recall that information fraction is still used for computing correlations in the asymptotic distribution approximation for design tests.
  
  ```{r}
  # Copy original upper planned spending
  upar_planned_IF <- upar_design_IF
  # Interim and final spending time will always be the same, regardless of 
  # expected events or calendar timing of analysis
  upar_planned_IF$timing <- c(0.8, 1)
  # We will reset planned maximum information later
  ```
  
  Finally, we set up a function to print a table to describe characteristics of a design and its updates as it is updated for different scenarios.
  
  ```{r}
  # Function to print table for group sequential design
  # Desire is to print both information fraction relative to planned with planned
  # input in maxinfo0. 
  # Input spending_time here needs to be consistent with what is given in design;
  # if NULL, information fraction will be used to compute spending time based in input max_info0.
  # Enrollment rates input are used to compute sample size.
  # This works for cases here, but N calculation will NOT work for
  # IA's before planned enrollment completion; for that gs_power_ahr needs to be fixed
  table_gs_design <- function(x, enrollRates, spending_time = NULL, max_info0 = NULL){
    N <- sum(enrollRates$rate * enrollRates$duration)
    if(is.null(max_info0)) stop("Must enter maximum H0 planned information in max_info0")
    x$N <- N
    if (is.null(spending_time)) {x$spending_time <- pmin(x$info0 / max_info0, 1)
    }else x$spending_time <- spending_time
    x %>% 
       transmute(Time = Time, N = ceiling(N), Events = ceiling(Events), "Information fraction" = info0 / max_info0,
                 "Spending time" = spending_time,
                 "Nominal p" = pnorm(-Z), AHR = AHR, "~HR at bound" = exp(-Z / sqrt(info)), Power = Probability) %>%
       gt() %>%
       fmt_number(columns = c("N","Events"), decimals = 0) %>%
       fmt_number(columns = "Time", decimals=1) %>%
       fmt_number(columns = c("AHR", "Power","Information fraction", "Spending time", "~HR at bound"), decimals=3) %>%
       fmt_number(columns = "Nominal p", decimals = 4)
  }
  ```
  
  
  ## Planned design
  
  We extend the design studied above to a group sequential design with a single interim analysis after 80% of the final planned events have accrued.
  We will assume the final analysis will require both the targeted trial duration and events based on the fixed design evaluations made above.
  We assume the efficacy bound uses the Lan-DeMets spending function approximating an O'Brien-Fleming bound.
  No futility bound is planned.
  The interim analysis is far enough into the trial that there is a substantial probability of stopping early under design assumptions.
  
  
  
  ```{r}
  m <- 12 # Control median
  failRates$failRate[1] <- log(2) / m
  failRates$duration[1] <- 4
  # Planned information fraction at interim(s) and final
  planned_IF <- c(.8,1)
  # No futility bound
  lpar <- rep(-Inf,2) # lower Z bound of -Inf at all analyses
  enrollRates <- tibble(Stratum="All", duration = 12, rate = 1)
  failRates <- tibble(Stratum="All", duration = c(4, 100), hr = c(1, .6), 
                      failRate = log(2) / m, dropoutRate = .001)
  # Note that timing here matches what went into planned_IF above
  # Final analysis time set to targeted study duration; analysis times before are 'small'
  # to ensure use of information fraction for timing
  xx <- gs_design_ahr(enrollRates, failRates, analysisTimes = c(1,30), IF = planned_IF,
        upper = gs_spending_bound, upar = upar_design_IF, 
        lower = gs_b, lpar = lpar)
  # Get upper bounds
  planned_bounds <- xx$bounds %>% filter(Bound == "Upper")
  # Planned number of analyses
  K <- nrow(planned_bounds)
  # Planned events
  max_events <- max(planned_bounds$Events)
  # save max information planned under H0
  # This will be used for future information fraction calculations
  # when event accumulation is not same as planned
  max_info0 <- max(xx$bounds$info0)
  # Planned analysis timing
  planned_time <- planned_bounds$Time
  planned_bounds %>% table_gs_design(xx$enrollRates, max_info0 = max(xx$bounds$info0))
  ```
  ## Two alternate approaches
  
  We consider two alternate approaches to demonstrate the spending time concept that may be helpful in practice.
  However, skipping the following two subsections can be done if these are not of interest.
  The first demonstrates calendar spending as in @LanDeMets1989.
  The second is a basically the method of @FHO where a fixed incremental spend is used for a potentially variable number of interim analyses, with the final bound computed based on the unspent one-sided Type I error assigned to a hypothesis.
  
  ### Calendar spending
  
  We use the same sample size as above, but change efficacy bound spending to calendar-based.
  The reason this spending is different than information-based spending is mainly due to the fact that the expected
  information is not linear in time.
  In this case, the calendar fraction at interim is less than the information fraction, but exactly the opposite would be true earlier in the trial.
  We just note that if calendar-based spending is chosen, it may be worth comparing the design bounds with bounds using the same spending function, but with information-based spending to see if there are important differences to the trial team or possibly to the scientific or regulatory community.
  We note also that there is risk there will not be enough events to achieve targeted power at the final analysis under a calendar-based spending strategy.
  We will not examine calendar-based spending further in this document.
  
  
  ```{r}
  yy <- 
    gs_power_ahr(enrollRates = xx$enrollRates, 
                 failRates = xx$failRates, 
                 events = 1:2, # Planned time will drive timing since information accrues faster
                 analysisTimes = c(22, 30), # Interim time rounded
                 upper = gs_spending_bound, upar = upar_design_CF, lpar = lpar) %>% filter(Bound == "Upper")
  actual_IF <- NULL # yy$info0 / max(yy$info0)
  yy %>% table_gs_design(xx$enrollRates, spending_time = upar_design_CF$timing, max_info0 = max(yy$info0))
  ```
  ### Fixed incremental spend with a variable number of analyses
  
  As noted, this method was proposed by @FHO. 
  The general strategy demonstrated is to do an interim analses every 6 months until a both a final targeted follow-up time and cumulative number of events is achieved.
  Once efficacy analyses start, a fixed incremental spend of 0.001 is used at each interim.
  When the criteria for final analysis is met, the remaining $\alpha$ is spent.
  Cumulative spending at months 18 and 24 will be 0.001 and 0.002, respectively, with the full cumulative $\alpha$-spending of 0.025 at the final analysis.
  This is done by setting the spending time at 18 and 24 months to 1/25, 2/25 and 1; i.e., 1/25 incremental $\alpha$-spending is incorporated at each interim analysis and any remaining $\alpha$ is spent at the final analysis.
  This enables a strategy such as analyzing every 6 months until both a minimum targeted follow-up and minimum number of events are observed, at which time the final analysis is performed.
  We will skip efficacy analyses at the first two interim analyses at months 6 and 12.
  
  For futility, we simply use a nominal 1-sided p-value of 0.05 favoring control at each interim.
  We note that this only raises a flag if the futility bound is crossed and a Data Monitoring Committee (DMC) can choose to continue the trial even if a futility bound is crossed.
  However, the bound may be more effective in providing a DMC guidance not to stop for futility prematurely.
  For comparison with the above designs, we will leave the enrollment rates, failure rates, dropout rates and final analysis time as before.
  
  We see in the following table summarizing efficacy bounds and power that there is little impact on the total power by having futility analyses as specified.
  While the cumulative $\alpha$-spending is 0.001 and 0.002 at the efficacy interim analyses, we see that the nominal p-value bound at the second interim is 0.0015, more then the 0.001 incremental $\alpha$-spend.
  We also note that with these nominal p-values for testing, the approximate hazard ratio required to cross the bounds would presumably help justify consideration of completing the trial based on a definitive interim efficacy finding.
  Also, with the small interim spend, the final nominal p-value is not reduced much from the overall $\alpha=0.025$ Type I error set for the group sequential design.
  
  
  ```{r}
  # Cumulative spending at IA3 and IA4 will be 0.001 and 0.002, respectively.
  # Power spending function sfPower with param = 1 is linear in timing
  # which makes setting the above cumulative spending targets simple by
  # setting timing variable the the cumulative proportion of spending at each analysis.
  # There will be no efficacy testing at IA1 or IA2.
  # Thus, incremental spend, which will be unused, is set very small for these analyses.
  upar_FHO <- list(total_spend = 0.025,
                   sf = sfPower,
                   param = 1,
                   timing = c((1:2)/250, (1:2)/25, 1)
  )
  FHO <-
    gs_power_ahr(enrollRates = xx$enrollRates,
                 failRates = xx$failRates,
                 events = NULL,
                 analysisTimes = seq(6, 30, 6),
                 # No efficacy testing at IA1 or IA2
                 # Thus, the small alpha the spending function would have
                 # allocated will not be used
                 test_upper = c(FALSE, FALSE, TRUE, TRUE, TRUE),
                 upper = gs_spending_bound,
                 upar = upar_FHO,
                 lpar = c(rep(qnorm(.05), 4), -Inf),
  )
  FHO %>% filter(Bound == "Upper", Z < Inf) %>% table_gs_design(xx$enrollRates, spending_time = upar_FHO$timing[3:5], max_info0 = max(FHO$info0))
  ```
  
  We also examine the futility bound.
  The nominal p-value of 0.05 at each analysis is the one-sided p-value in favor of control over experimental treatment.
  We can see that the probability of stopping early under the alternate hypothesis ($\beta$-spending) is not substantial even given the early delayed effect.
  Also, the substantial approximate observed hazard ratios to cross a futility bound seem reasonable given the timing and number of events observed; the exception to this is the small number of events at the first interim, but a larger number could be observed by this time if there were early excess risk.
  It may be useful to plan additional analyses if a futility bound is crossed to support stopping or not.
  For example, looking in subgroups or evaluating smoothed hazard rates over time for each treatment group may be useful.
  A clinical trial study team should have a complete discussion of futility bound considerations at the time of design.
  
  ```{r}
  FHO %>% 
    filter(Bound == "Lower", abs(Z) < Inf) %>%
    transmute(Time = Time, 
              Events = ceiling(Events),
              "Nominal p" = pnorm(Z), 
              "Information fraction" = info0 / max_info0,
              "~HR at bound" = exp(-Z / sqrt(info)),
              "Cumulative beta-spend" = Probability
              ) %>%
    gt() %>%
    fmt_number(columns = c("Events"), decimals = 0) %>%
    fmt_number(columns = "Time", decimals=1) %>%
    fmt_number(columns = c("~HR at bound", "Cumulative beta-spend","Information fraction"), decimals=3) %>%
    fmt_number(columns = "Nominal p", decimals = 4)
  ```
  
  ## Less treatment effect scenario
  
  As before, we compute power under the assumption of changing the median control group time-to-event to 10 months rather than the assumed 12 and the delay in effect onset is 6 months rather than 4.
  We otherwise do not change enrollment, dropout or hazard ratio assumptions.
  In both of the following examples, we require both the targeted number of events and targeted trial duration from the group sequential design before doing the interim and final analyses.
  The first example, which uses interim spending based on the event count observed over the originally planned final event count has the information fraction 323 / 355 = 0.91.
  This gives event-based spending of 0.0191, substantially above the targeted information fraction of 284 / 355 = 0.8 with targeted interim spending of 0.0122.
  This reduces the power overall from 76% to 73% and lowers the nominal p-value bound at the final analysis from 0.0218 to 0.0165; see the following two tables.
  Noting that the average hazard ratio is 0.8 at the interim and 0.76 at the final analysis emphasizes the value of preserving $\alpha$-spending until the final analysis.
  Thus, in this example it is valuable to limit spending at the interim analysis to the minimum of planned spending as opposed to using event-based spending.
  
  ```{r}
  am <- 10 # Alternate control median
  failRates$failRate <- log(2) / am
  failRates$duration[1] <- 6
  # Set planned maximum information from planned design
  max_info0 <- max(xx$bounds$info0)
  upar_actual_IF <- upar_design_IF
  upar_actual_IF$max_info <- max_info0
  # compute power if actual information fraction relative to original
  # planned total is used
  yy <- 
    gs_power_ahr(enrollRates = xx$enrollRates, 
                 failRates = failRates, 
                 events = 1:2, # Planned time will drive timing since information accrues faster
                 analysisTimes = planned_time,
                 upper = gs_spending_bound, upar = upar_actual_IF, lpar = lpar) %>% filter(Bound == "Upper")
  actual_IF <- yy$info0 / max_info0
  yy %>% table_gs_design(xx$enrollRates, spending_time = actual_IF, max_info0 = max_info0)
  ```
  
  Just as important, the general design principle of making interim analysis criteria more stringent that final is ensured for this alternate scenario.
  There are multiple trials where delayed effects have been observed where this difference in the final nominal p-value bound would have made a difference to ensure a statistically significant finding.
  
  ```{r}
  yz <- 
    gs_power_ahr(enrollRates = xx$enrollRates, 
                 failRates = failRates, 
                 events = xx$bounds$Events[1:2],
                 analysisTimes = planned_time,
                 upper = gs_spending_bound, 
                 upar = upar_planned_IF, 
                 lpar = lpar) %>% filter(Bound == "Upper")
  # Note that max_info0 is denominator to compute "Information fraction" in table
  # However, spending time is less since we use planned IF to compute spending
  yz %>% table_gs_design(xx$enrollRates, spending_time = planned_IF, max_info0 = max_info0)
  ```
  
  ## Scenario with longer control median
  
  Now we return to the example where the control median is longer than expected to confirm that spending according to the planned level alone without considering the actual number of events will also result in a power reduction.
  While the power gain is not great (94.2% vs 95.0%) the interim and final p-value bounds are more aligned with the intent of emphasizing the final analysis where a smaller average hazard ratio is expected (0.680 vs 0.723 at the interim).
  First, we show the result using planned spending.
  
  ```{r}
  am <- 16 # Alternate control median
  failRates$failRate <- log(2) / am
  # Return to 4 month delay with HR=1 before HR = 0.6
  failRates$duration[1] <- 4
  # Start with spending based on planned information
  # which is greater than actual information
  yy <- 
    gs_power_ahr(enrollRates = xx$enrollRates, 
                 failRates = failRates, 
                 events = c(1,max_events),
                 analysisTimes = planned_time,
                 upper = gs_spending_bound, upar = upar_planned_IF, lpar = lpar) %>% filter(Bound == "Upper")
  yy %>% table_gs_design(xx$enrollRates, spending_time = planned_IF, max_info0 = max_info0)
  ```
  
  Since the number of events was less than expected, if we had used the actual number of events the interim bound would be more stringent than above and we obtain slightly greater power.
  
  ```{r}
  yz <- 
    gs_power_ahr(enrollRates = xx$enrollRates, 
                 failRates = failRates, 
                 events = c(1,xx$bounds$Events[2]),
                 analysisTimes = planned_time,
                 upper = gs_spending_bound, upar = upar_actual_IF, lpar = lpar) %>% filter(Bound == "Upper")
  yz %>% table_gs_design(xx$enrollRates, spending_time = NULL, max_info0 = max_info0)
  ```
  
  ## Summary for spending time motivation assuming delayed benefit
  
  In summary, using the minimum of planned and actual spending to adapt the design based on event-based spending adapts the interim bound to be more stringent than the final bound under different scenarios and ensures better power than event-based interim analysis and spending.
  
  
  # Testing multiple hypotheses
  
  ## Assumptions
  
  We consider a simple case where we use the method of @MaurerBretz2013 to test both in the overall population and in a biomarker subgroup for the same endpoint.
  We assume an exponential failure rate with a median of 12 for the control group regardless of population.
  The hazard ratio in the biomarker positive subgroup will be assumed to be 0.6, and in the negative population 0.8.
  We assume the biomarker positive group represents half of the population, meaning that enrollment rates will be assumed to be the same in negative and positive patients.
  The only difference between failure rates in the two strata is the hazard ratio.
  For this case, we assume proportional hazards within negative (HR = 0.8) and positive (HR = 0.6) patients.
  
  ```{r}
  enrollRates <- tibble(Stratum=c("Positive","Negative"), duration = 12, rate=20)
  m <- 12
  failRates <- tibble(Stratum = c("Positive", "Negative"), hr = c(0.6, 0.8),
                      duration = 100,
                      failRate = log(2) / m, dropoutRate = 0.001)
  ```
  
  For illustrative purposes, we are choosing a strategy based on the possible feeling of much less certainty at study start as to whether there is any underlying benefit in the biomarker negative population.
  We wish to ensure power for the biomarker positive group, but allow a good chance of a positive overall population finding if there is a lesser benefit in the biomarker negative population.
  If an alternative trial strategy is planned, an alternate approach to the following should be considered.
  In any case, we design first for the biomarker positive population with one-sided Type I error controlled at $\alpha = 0.0125$:
  
  
  ## Planned design for biomarker positive population
  
  ```{r}
  # Spending based on information fraction
  # At time of design, timing = NULL is used to select
  # maximum planned information for information fraction.
  # Since execution will be event-based for biomarker population,
  # there will be no need to change spending plan for different scenarios.
  # Total alpha spend is now 0.0125 
  upar_design_spend <- list(sf = gsDesign::sfLDOF, total_spend = 0.0125, param = NULL, timing = NULL)
  # No futility bound
  lpar <- rep(-Inf,2) # Z = -infinity for lower bound
  # We will base the combined hypothesis design to ensure power in the biomarker subgroup
  positive <- 
    gs_design_ahr(enrollRates = enrollRates %>% filter(Stratum == "Positive"),
                  failRates = failRates %>% filter(Stratum == "Positive"),
                  # Following drives information fraction for interim
                  IF = c(.8, 1), 
                  # Total study duration driven by final analysisTimes value
                  # Enter small increasing values before that so information
                  # fraction in planned_IF drives timing of interims
                  analysisTimes = c(1,30),
                  upper =gs_spending_bound, upar = upar_design_spend,
                  # Fixed lower bound with Z = -infinity
                  lower = gs_b, lpar = lpar
                 )
  # This planned design will drive adaptation for deviations from plan
  positive_planned_bounds <- positive$bounds %>% filter(Bound == "Upper")
  # Planned timing of analysis
  positive_planned_time <- positive_planned_bounds$Time
  # Planned sample size
  positive_planned_N <- max(positive_planned_bounds$N)
  # Planned events
  positive_max_events <- max(positive_planned_bounds$Events)
  # save max information planned under H0
  positive_max_info0 <- max(positive_planned_bounds$info0)
  positive_planned_bounds %>% 
    table_gs_design(enrollRates = positive$enrollRates, 
                    max_info0 = positive_max_info0)  
  ```
  
  
  ## Planned design for overall population
  
  We adjust the overall study enrollment rate to match the design requirement for the biomarker positive population.
  
  ```{r}
  # Get enrollment rate inflation factor compared to originally input rate
  inflation_factor <- positive$enrollRates$rate[1] / enrollRates$rate[1]
  # Using this inflation factor, set planned enrollment rates
  planned_enrollRates <- enrollRates %>% mutate(rate = rate * inflation_factor)
  planned_enrollRates %>% gt()
  # Store overall enrollment rates for future use
  overall_enrollRates <- 
    planned_enrollRates %>% 
    summarize(Stratum = "All", duration=first(duration), rate = sum(rate))
  ```
  
  Now we can examine the power for the overall population based on hazard ratio assumptions in biomarker negative and biomarker positive subgroups and the just calculated enrollment assumption.
  We use the analysis times from the biomarker positive population design. 
  We see that the interim information fraction for the overall population is slightly greater than the biomarker positive population above.
  To compensate for this and to enable flexibility below as biomarker positive prevalence changes, we use the same spending time as the biomarker positive subgroup regardless of the true fraction of final planned events at each analysis.
  Thus, the interim nominal p-value bound is the same for both the biomarker positive and overall populations.
  While this does not make much difference here, we see that we have a very natural way to adapt the design if the observed biomarker positive prevalence is different than what was assumed for the design.
  
  ```{r}
  # Set total spend for overall population, O'Brien-Fleming spending function, and 
  # same spending time as biomarker subgroup
  upar_overall_planned_IF <- list(sf = gsDesign::sfLDOF,  param = NULL,
                                     total_spend = 0.0125, timing = c(.8, 1),
                    # We will use actual final information as planned initially
                                     max_info = NULL)
  overall_planned_bounds <- 
    gs_power_ahr(enrollRates = planned_enrollRates,
                 failRates = failRates,
                 analysisTimes = positive_planned_time,
                 # Events will be determined by expected events at planned analysis times
                 events = NULL, 
                 upper = gs_spending_bound,
                 # Recall planned spending times are specified the same as before 
                 upar = upar_overall_planned_IF,
                 lower = gs_b,
                 lpar = lpar # fixed lower Z = -infinity
                ) %>% filter(Bound == "Upper")
  # Planned events
  overall_max_events <- max(overall_planned_bounds$Events)
  # save max information planned under H0
  overall_max_info0 <- max(overall_planned_bounds$info0)
  overall_planned_bounds %>% 
    table_gs_design(planned_enrollRates,
                    spending_time = planned_IF,
                    max_info0 = overall_max_info0)
  ```
  
  ## Alternate scenarios overview
  
  We divide our further evaluations into three subsections, one with a higher prevalence of biomarker positive patients than expected, one with a lower biomarker prevalence, followed by a section with differing event rate and hazard ratio assumptions.
  For each case, we will assume the total enrollment rate of `r round(sum(planned_enrollRates$rate),1)` per month as planned above.
  We also assume that we enroll until the targeted biomarker positive subgroup enrollment of 
  `r ceiling(sum(positive$enrollRates$rate*positive$enrollRates$duration))` from above is achieved, regardless of the overall enrollment.
  The specify interim analysis timing to require both 80% of the planned final analysis events in the biomarker positive population and at least 10 months of minimum follow-up; thus, for the biomarker population we will never vary events or spending here.
  The same spending time will be used for the overall population, but we will compare with event-based spending.
  The above choices are arbitrary. 
  While we think they are reasonable, the design planner should think carefully about other variations to suit their clinical trial team needs.
  
  ```{r}
  ## Setting spending alternatives
  
  # Using information (event)-based spending time relative to overall population plan
  # Set total spend for overall population, O'Brien-Fleming spending function. 
  # For design information-spending, we set timing =  NULL and max_info to plan from above
  upar_overall_planned_IF <- list(sf = gsDesign::sfLDOF,  param = NULL,
                                     total_spend = 0.0125, timing = planned_IF,
                    # We will use planned final information for overall population from design
                    # to compute information fraction relative to plan
                                     max_info = overall_max_info0)
  # Using planned information fraction will demonstrate problems below.
  # Set total spend for overall population, O'Brien-Fleming spending function, and 
  # same spending time as biomarker subgroup
  upar_overall_actual_IF <- list(sf = gsDesign::sfLDOF,  param = NULL,
                                    total_spend = 0.0125, timing = NULL,
                    # We will use planned final information for overall population from design
                                    max_info = overall_max_info0)
  ```
  
  ## Biomarker subgroup prevalence higher than planned
  
  ### Biomarker subgroup power
  
  We suppose the biomarker prevalence is 60%, higher then the 50% prevalence the design anticipated.
  The enrollment rates by positive versus negative patients and expected enrollment duration are now:
  
  ```{r}
  positive_60_enrollRates <- rbind(
    overall_enrollRates %>% mutate(Stratum = "Positive", rate = 0.6 * rate),
    overall_enrollRates %>% mutate(Stratum = "Negative", rate = 0.4 * rate)
  )
  positive_60_duration <- positive_planned_N / overall_enrollRates$rate / 0.6
  positive_60_enrollRates$duration <- positive_60_duration
  positive_60_enrollRates %>% gt() %>% fmt_number(columns="rate",decimals=1)
  ```
  
  Now we can compute the power for the biomarker positive group with the targeted events.
  Since we have a simple proportional hazards model, they only thing that is changing here from the original design is that this takes slightly less time.
  
  ```{r}
  positive_60_power <- 
  gs_power_ahr(enrollRates = positive_60_enrollRates %>% filter(Stratum == "Positive"),
               failRates = failRates %>% filter(Stratum == "Positive"),
               events = positive$bounds$Events[1:K],
               analysisTimes = NULL,
               upper = gs_spending_bound,
               upar = upar_design_spend,
               lower = gs_b,
               lpar = lpar
  ) %>% filter(Bound == "Upper")
  positive_60_power %>% 
    table_gs_design(enrollRates = positive_60_enrollRates %>% filter(Stratum == "Positive"), 
                    max_info0 = positive_max_info0)
  ```
  
  
  ### Overall population power
  
  Now we use the same spending as above for the overall population, resulting in full $\alpha$-spending at the end of the trial even though the originally targeted events are not expected to be achieved.
  We note that the information fraction computed here is based on the originally planned events for the overall population.
  Given this and the larger proportion of patients that are biomarker positive, the average hazard ratio is stronger than originally planned and the power for the overall population is still over 90%.
  
  ```{r}
  gs_power_ahr(enrollRates = positive_60_enrollRates,
               failRates = failRates,
               events = 1:2,
               analysisTimes = positive_60_power$Time[1:2],
               upper = gs_spending_bound,
               # use planned spending in spite of lower overall information
               upar = upar_overall_planned_IF,
               # Still use Z = -infinity lower bound 
               lower = gs_b, lpar = rep(-Inf, 2)) %>% 
    filter(Bound == "Upper") %>% 
    table_gs_design(enrollRates = positive_60_enrollRates, spending_time = c(.8,1), max_info0 = overall_max_info0)
  ```
  
  If we had used information-based (i.e., event-based) spending, we would not have reached full spending at final analysis and thus would have lower power.
  
  ```{r}
  xz <-
  gs_power_ahr(enrollRates = positive_60_enrollRates,
               failRates = failRates,
               events = 1:2,
               analysisTimes = positive_60_power$Time[1:2],
               upper = gs_spending_bound,
               # use actual spending which uses less than complete alpha
               upar = upar_overall_actual_IF,
               # Still use Z = -infinity lower bound 
               lower = gs_b, lpar = lpar) %>% 
    filter(Bound == "Upper" ) 
  xz %>% table_gs_design(enrollRates = positive_60_enrollRates, spending_time = NULL, max_info0 = overall_max_info0)
  ```
  
  ## Biomarker subgroup prevalence lower than planned
  
  We suppose the biomarker prevalence is 40%, lower than the 50% prevalence the design anticipated.
  The enrollment rates by positive versus negative patients and expected enrollment duration will now be:
  
  ```{r}
  positive_40_enrollRates <- rbind(
    overall_enrollRates %>% mutate(Stratum = "Positive", rate = 0.4 * rate),
    overall_enrollRates %>% mutate(Stratum = "Negative", rate = 0.6 * rate)
  )
  positive_40_enrollRates$duration <- positive_planned_N / positive_40_enrollRates$rate[1]
  positive_40_enrollRates %>% gt() %>% fmt_number(columns = "rate", decimals=1)
  ```
  
  ### Biomarker positive subgroup power
  
  
  Now we can compute the power for the biomarker positive group with the targeted events.
  
  ```{r}
  upar_actual_IF$total_spend <- 0.0125
  upar_actual_IF$max_info <- positive_max_info0
  positive_40_power <- 
  gs_power_ahr(enrollRates = positive_40_enrollRates %>% filter(Stratum == "Positive"),
               failRates = failRates %>% filter(Stratum == "Positive"),
               events = (positive$bounds %>% filter(Bound == "Upper"))$Events,
               analysisTimes = NULL,
               upper = gs_spending_bound,
               upar = upar_actual_IF, 
               lpar = rep(-Inf, 2)
  )
  positive_40_power %>% filter(Bound == "Upper") %>% 
    table_gs_design(enrollRates = positive_40_enrollRates %>% filter(Stratum == "Positive"), 
                    max_info0 = positive_max_info0)
  ```
  
  ### Overall population power
  
  We see that by adapting the overall sample size and spending according to the biomarker subgroup, we retain 90% power.
  \In spite of the lower overall effect size, the larger adapted sample size ensures power retention.
  
  ```{r}
  gs_power_ahr(enrollRates = positive_40_enrollRates,
               failRates = failRates,
               events = 1:2,
               analysisTimes = positive_40_power$Time[1:2],
               upper = gs_spending_bound,
               upar = upar_overall_planned_IF,
               lpar = rep(-Inf,2)
  ) %>% filter(Bound == "Upper")  %>% 
    table_gs_design(enrollRates = positive_60_enrollRates, 
                    spending_time = c(.8, 1),
                    max_info0 = overall_max_info0)
  ```
  
  
  # Summary of findings
  
  We suggested two overall findings when planning and executing a trial with a potentially delayed treatment effect:
  
  - Require both a targeted event count minimum follow-up before completing analysis of a trial helps ensure both powering the trial appropriately and having a better description of the tail behavior that may be essential if long-term results are key to establishing a potentially positive risk-benefit.
  - Do not over-spend Type I error at interim analyses by using event-based spending. 
  This helps to ensure the least stringent bounds are at the final analysis when the most complete risk-benefit assessment can be made.
  We gave two options to this:
      - Use a fixed, small incremental $\alpha$-spend at each interim such as proposed by @FHO with a variable number of interim analyses to ensure adequate follow-up.
      - Use the minimum of planned and actual spending at interim analyses.
  
  When implementing the @FHO approach, we also suggested a simple approach to futility that may be quite useful practically in a scenario with a potentially delayed onset of treatment effect.
  This basically looks for evidence of a favorable control group effect relative to experimental by setting a nominal p-value cutoff at a 1-sided 0.05 level for early interim futility analyses.
  Where crossing survival curves or inferior survival curves may exist, this may be a useful way to ensure continuing a trial is ethical; this approach is perhaps most useful when the experimental treatment is replacing components of the control treatment or in a case where add-on treatment may be toxic or potentially have other detrimental effects.
  
  In addition to the delayed effect example, we considered an example testing in both a biomarker positive subgroup and the overall population. 
  Using a common spending time for all hypotheses with a common interim analysis strategy as advocated by @FPG can be helpful to implement spending so that all hypotheses have adequate $\alpha$ to spend at the final analysis and also to ensure full utilization of $\alpha$-spending. We suggested again using the minimum of planned and actual spending at interim analysis. Spending can be based on a key hypothesis (e.g., the biomarker positive population) or the minimum spending time among all hypotheses being tested. 
  Taking advantage of know correlations to ensure full $\alpha$ utilitization in multiple hypothesis testing is also more simply implemented with this strategy @AGZS2021unified. 
  
  In summary, we have illustrated both the motivation and the illustration of the spending time approach through examples we have commonly encountered. Approaches suggested included an implementation of @FHO with a fixed incremental $\alpha$-spend at each interim analysis as well as the use of the minimum of planned and actual spending at interim analyses.
  
  
  # References
  
    

