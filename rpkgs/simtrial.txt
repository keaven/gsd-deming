# Generated by pkglite: do not edit by hand
# Use pkglite::unpack() to restore the packages

Package: simtrial
File: .Rbuildignore
Format: text
Content:
  ^\.github$
  ^.*\.Rproj$
  ^\.Rproj\.user$
  ^LICENSES_THIRD_PARTY$
  ^_pkgdown\.yml$
  ^\.gitattributes$
  ^pkgdown$
  ^LICENSE$
  ^LICENSES_THIRD_PARTY$
  ^codecov\.yml$
  ^doc$
  ^data-raw$

Package: simtrial
File: DESCRIPTION
Format: text
Content:
  Package: simtrial
  Type: Package
  Title: Clinical Trial Simulation
  Version: 0.2.0
  Authors@R: c(
      person(given = "Keaven", family = "Anderson", email = "keaven_anderson@merck.com", role = c("aut")),
      person("Yilong", "Zhang", email = "yilong.zhang@merck.com", role =  c("aut","cre")),
      person("Amin", "Shirazi", email = "ashirazist@gmail.com", role = c("ctb")),
      person("Ruixue", "Wang", email = "ruixue.wang@merck.com", role = c("ctb")),
      person("Yi", "Cui", email = "yi.cui@merck.com", role = c("ctb")),
      person("Ping", "Yang", email = "ping.yang1@merck.com", role = c("ctb")),
      person("Yalin", "Zhu", email = "yalin.zhu@merck.com", role = c("ctb")),
      person("Heng", "Zhou", email = "heng.zhou@merck.com", role = c("ctb")),
      person("Merck Sharp & Dohme Corp", role = "cph")
      )
  Maintainer: Yilong Zhang <yilong.zhang@merck.com>
  Description: simtrial provides some basic routines for simulating a clinical trial. 
      The primary intent is to provide some tools to generate trial simulations for trials with time to event outcomes. 
      Piecewise exponential failure rates and piecewise constant enrollment rates are the underlying mechanism used 
      to simulate a broad range of scenarios.
      However, the basic generation of data is done using pipes to allow maximum flexibility for users to meet different needs.
  License: GPL-3
  URL: https://merck.github.io/simtrial/, https://github.com/Merck/simtrial
  BugReports: https://github.com/Merck/simtrial/issues
  Depends: R (>= 3.5.0)
  Imports:
      dplyr,
      tibble,
      tidyr,
      survival,
      mvtnorm
  Suggests: 
      knitr,
      rmarkdown,
      testthat,
      modestWLRT,
      ggplot2,
      bshazard, 
      markdown,
      survMisc,
      gsDesign,
      stringr,
      Matrix
  Encoding: UTF-8
  LazyData: true
  Roxygen: list(markdown = TRUE)  
  RoxygenNote: 7.1.1
  VignetteBuilder: knitr

Package: simtrial
File: LICENSE
Format: text
Content:
                      GNU GENERAL PUBLIC LICENSE
                         Version 3, 29 June 2007
  
   Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
   Everyone is permitted to copy and distribute verbatim copies
   of this license document, but changing it is not allowed.
  
                              Preamble
  
    The GNU General Public License is a free, copyleft license for
  software and other kinds of works.
  
    The licenses for most software and other practical works are designed
  to take away your freedom to share and change the works.  By contrast,
  the GNU General Public License is intended to guarantee your freedom to
  share and change all versions of a program--to make sure it remains free
  software for all its users.  We, the Free Software Foundation, use the
  GNU General Public License for most of our software; it applies also to
  any other work released this way by its authors.  You can apply it to
  your programs, too.
  
    When we speak of free software, we are referring to freedom, not
  price.  Our General Public Licenses are designed to make sure that you
  have the freedom to distribute copies of free software (and charge for
  them if you wish), that you receive source code or can get it if you
  want it, that you can change the software or use pieces of it in new
  free programs, and that you know you can do these things.
  
    To protect your rights, we need to prevent others from denying you
  these rights or asking you to surrender the rights.  Therefore, you have
  certain responsibilities if you distribute copies of the software, or if
  you modify it: responsibilities to respect the freedom of others.
  
    For example, if you distribute copies of such a program, whether
  gratis or for a fee, you must pass on to the recipients the same
  freedoms that you received.  You must make sure that they, too, receive
  or can get the source code.  And you must show them these terms so they
  know their rights.
  
    Developers that use the GNU GPL protect your rights with two steps:
  (1) assert copyright on the software, and (2) offer you this License
  giving you legal permission to copy, distribute and/or modify it.
  
    For the developers' and authors' protection, the GPL clearly explains
  that there is no warranty for this free software.  For both users' and
  authors' sake, the GPL requires that modified versions be marked as
  changed, so that their problems will not be attributed erroneously to
  authors of previous versions.
  
    Some devices are designed to deny users access to install or run
  modified versions of the software inside them, although the manufacturer
  can do so.  This is fundamentally incompatible with the aim of
  protecting users' freedom to change the software.  The systematic
  pattern of such abuse occurs in the area of products for individuals to
  use, which is precisely where it is most unacceptable.  Therefore, we
  have designed this version of the GPL to prohibit the practice for those
  products.  If such problems arise substantially in other domains, we
  stand ready to extend this provision to those domains in future versions
  of the GPL, as needed to protect the freedom of users.
  
    Finally, every program is threatened constantly by software patents.
  States should not allow patents to restrict development and use of
  software on general-purpose computers, but in those that do, we wish to
  avoid the special danger that patents applied to a free program could
  make it effectively proprietary.  To prevent this, the GPL assures that
  patents cannot be used to render the program non-free.
  
    The precise terms and conditions for copying, distribution and
  modification follow.
  
                         TERMS AND CONDITIONS
  
    0. Definitions.
  
    "This License" refers to version 3 of the GNU General Public License.
  
    "Copyright" also means copyright-like laws that apply to other kinds of
  works, such as semiconductor masks.
  
    "The Program" refers to any copyrightable work licensed under this
  License.  Each licensee is addressed as "you".  "Licensees" and
  "recipients" may be individuals or organizations.
  
    To "modify" a work means to copy from or adapt all or part of the work
  in a fashion requiring copyright permission, other than the making of an
  exact copy.  The resulting work is called a "modified version" of the
  earlier work or a work "based on" the earlier work.
  
    A "covered work" means either the unmodified Program or a work based
  on the Program.
  
    To "propagate" a work means to do anything with it that, without
  permission, would make you directly or secondarily liable for
  infringement under applicable copyright law, except executing it on a
  computer or modifying a private copy.  Propagation includes copying,
  distribution (with or without modification), making available to the
  public, and in some countries other activities as well.
  
    To "convey" a work means any kind of propagation that enables other
  parties to make or receive copies.  Mere interaction with a user through
  a computer network, with no transfer of a copy, is not conveying.
  
    An interactive user interface displays "Appropriate Legal Notices"
  to the extent that it includes a convenient and prominently visible
  feature that (1) displays an appropriate copyright notice, and (2)
  tells the user that there is no warranty for the work (except to the
  extent that warranties are provided), that licensees may convey the
  work under this License, and how to view a copy of this License.  If
  the interface presents a list of user commands or options, such as a
  menu, a prominent item in the list meets this criterion.
  
    1. Source Code.
  
    The "source code" for a work means the preferred form of the work
  for making modifications to it.  "Object code" means any non-source
  form of a work.
  
    A "Standard Interface" means an interface that either is an official
  standard defined by a recognized standards body, or, in the case of
  interfaces specified for a particular programming language, one that
  is widely used among developers working in that language.
  
    The "System Libraries" of an executable work include anything, other
  than the work as a whole, that (a) is included in the normal form of
  packaging a Major Component, but which is not part of that Major
  Component, and (b) serves only to enable use of the work with that
  Major Component, or to implement a Standard Interface for which an
  implementation is available to the public in source code form.  A
  "Major Component", in this context, means a major essential component
  (kernel, window system, and so on) of the specific operating system
  (if any) on which the executable work runs, or a compiler used to
  produce the work, or an object code interpreter used to run it.
  
    The "Corresponding Source" for a work in object code form means all
  the source code needed to generate, install, and (for an executable
  work) run the object code and to modify the work, including scripts to
  control those activities.  However, it does not include the work's
  System Libraries, or general-purpose tools or generally available free
  programs which are used unmodified in performing those activities but
  which are not part of the work.  For example, Corresponding Source
  includes interface definition files associated with source files for
  the work, and the source code for shared libraries and dynamically
  linked subprograms that the work is specifically designed to require,
  such as by intimate data communication or control flow between those
  subprograms and other parts of the work.
  
    The Corresponding Source need not include anything that users
  can regenerate automatically from other parts of the Corresponding
  Source.
  
    The Corresponding Source for a work in source code form is that
  same work.
  
    2. Basic Permissions.
  
    All rights granted under this License are granted for the term of
  copyright on the Program, and are irrevocable provided the stated
  conditions are met.  This License explicitly affirms your unlimited
  permission to run the unmodified Program.  The output from running a
  covered work is covered by this License only if the output, given its
  content, constitutes a covered work.  This License acknowledges your
  rights of fair use or other equivalent, as provided by copyright law.
  
    You may make, run and propagate covered works that you do not
  convey, without conditions so long as your license otherwise remains
  in force.  You may convey covered works to others for the sole purpose
  of having them make modifications exclusively for you, or provide you
  with facilities for running those works, provided that you comply with
  the terms of this License in conveying all material for which you do
  not control copyright.  Those thus making or running the covered works
  for you must do so exclusively on your behalf, under your direction
  and control, on terms that prohibit them from making any copies of
  your copyrighted material outside their relationship with you.
  
    Conveying under any other circumstances is permitted solely under
  the conditions stated below.  Sublicensing is not allowed; section 10
  makes it unnecessary.
  
    3. Protecting Users' Legal Rights From Anti-Circumvention Law.
  
    No covered work shall be deemed part of an effective technological
  measure under any applicable law fulfilling obligations under article
  11 of the WIPO copyright treaty adopted on 20 December 1996, or
  similar laws prohibiting or restricting circumvention of such
  measures.
  
    When you convey a covered work, you waive any legal power to forbid
  circumvention of technological measures to the extent such circumvention
  is effected by exercising rights under this License with respect to
  the covered work, and you disclaim any intention to limit operation or
  modification of the work as a means of enforcing, against the work's
  users, your or third parties' legal rights to forbid circumvention of
  technological measures.
  
    4. Conveying Verbatim Copies.
  
    You may convey verbatim copies of the Program's source code as you
  receive it, in any medium, provided that you conspicuously and
  appropriately publish on each copy an appropriate copyright notice;
  keep intact all notices stating that this License and any
  non-permissive terms added in accord with section 7 apply to the code;
  keep intact all notices of the absence of any warranty; and give all
  recipients a copy of this License along with the Program.
  
    You may charge any price or no price for each copy that you convey,
  and you may offer support or warranty protection for a fee.
  
    5. Conveying Modified Source Versions.
  
    You may convey a work based on the Program, or the modifications to
  produce it from the Program, in the form of source code under the
  terms of section 4, provided that you also meet all of these conditions:
  
      a) The work must carry prominent notices stating that you modified
      it, and giving a relevant date.
  
      b) The work must carry prominent notices stating that it is
      released under this License and any conditions added under section
      7.  This requirement modifies the requirement in section 4 to
      "keep intact all notices".
  
      c) You must license the entire work, as a whole, under this
      License to anyone who comes into possession of a copy.  This
      License will therefore apply, along with any applicable section 7
      additional terms, to the whole of the work, and all its parts,
      regardless of how they are packaged.  This License gives no
      permission to license the work in any other way, but it does not
      invalidate such permission if you have separately received it.
  
      d) If the work has interactive user interfaces, each must display
      Appropriate Legal Notices; however, if the Program has interactive
      interfaces that do not display Appropriate Legal Notices, your
      work need not make them do so.
  
    A compilation of a covered work with other separate and independent
  works, which are not by their nature extensions of the covered work,
  and which are not combined with it such as to form a larger program,
  in or on a volume of a storage or distribution medium, is called an
  "aggregate" if the compilation and its resulting copyright are not
  used to limit the access or legal rights of the compilation's users
  beyond what the individual works permit.  Inclusion of a covered work
  in an aggregate does not cause this License to apply to the other
  parts of the aggregate.
  
    6. Conveying Non-Source Forms.
  
    You may convey a covered work in object code form under the terms
  of sections 4 and 5, provided that you also convey the
  machine-readable Corresponding Source under the terms of this License,
  in one of these ways:
  
      a) Convey the object code in, or embodied in, a physical product
      (including a physical distribution medium), accompanied by the
      Corresponding Source fixed on a durable physical medium
      customarily used for software interchange.
  
      b) Convey the object code in, or embodied in, a physical product
      (including a physical distribution medium), accompanied by a
      written offer, valid for at least three years and valid for as
      long as you offer spare parts or customer support for that product
      model, to give anyone who possesses the object code either (1) a
      copy of the Corresponding Source for all the software in the
      product that is covered by this License, on a durable physical
      medium customarily used for software interchange, for a price no
      more than your reasonable cost of physically performing this
      conveying of source, or (2) access to copy the
      Corresponding Source from a network server at no charge.
  
      c) Convey individual copies of the object code with a copy of the
      written offer to provide the Corresponding Source.  This
      alternative is allowed only occasionally and noncommercially, and
      only if you received the object code with such an offer, in accord
      with subsection 6b.
  
      d) Convey the object code by offering access from a designated
      place (gratis or for a charge), and offer equivalent access to the
      Corresponding Source in the same way through the same place at no
      further charge.  You need not require recipients to copy the
      Corresponding Source along with the object code.  If the place to
      copy the object code is a network server, the Corresponding Source
      may be on a different server (operated by you or a third party)
      that supports equivalent copying facilities, provided you maintain
      clear directions next to the object code saying where to find the
      Corresponding Source.  Regardless of what server hosts the
      Corresponding Source, you remain obligated to ensure that it is
      available for as long as needed to satisfy these requirements.
  
      e) Convey the object code using peer-to-peer transmission, provided
      you inform other peers where the object code and Corresponding
      Source of the work are being offered to the general public at no
      charge under subsection 6d.
  
    A separable portion of the object code, whose source code is excluded
  from the Corresponding Source as a System Library, need not be
  included in conveying the object code work.
  
    A "User Product" is either (1) a "consumer product", which means any
  tangible personal property which is normally used for personal, family,
  or household purposes, or (2) anything designed or sold for incorporation
  into a dwelling.  In determining whether a product is a consumer product,
  doubtful cases shall be resolved in favor of coverage.  For a particular
  product received by a particular user, "normally used" refers to a
  typical or common use of that class of product, regardless of the status
  of the particular user or of the way in which the particular user
  actually uses, or expects or is expected to use, the product.  A product
  is a consumer product regardless of whether the product has substantial
  commercial, industrial or non-consumer uses, unless such uses represent
  the only significant mode of use of the product.
  
    "Installation Information" for a User Product means any methods,
  procedures, authorization keys, or other information required to install
  and execute modified versions of a covered work in that User Product from
  a modified version of its Corresponding Source.  The information must
  suffice to ensure that the continued functioning of the modified object
  code is in no case prevented or interfered with solely because
  modification has been made.
  
    If you convey an object code work under this section in, or with, or
  specifically for use in, a User Product, and the conveying occurs as
  part of a transaction in which the right of possession and use of the
  User Product is transferred to the recipient in perpetuity or for a
  fixed term (regardless of how the transaction is characterized), the
  Corresponding Source conveyed under this section must be accompanied
  by the Installation Information.  But this requirement does not apply
  if neither you nor any third party retains the ability to install
  modified object code on the User Product (for example, the work has
  been installed in ROM).
  
    The requirement to provide Installation Information does not include a
  requirement to continue to provide support service, warranty, or updates
  for a work that has been modified or installed by the recipient, or for
  the User Product in which it has been modified or installed.  Access to a
  network may be denied when the modification itself materially and
  adversely affects the operation of the network or violates the rules and
  protocols for communication across the network.
  
    Corresponding Source conveyed, and Installation Information provided,
  in accord with this section must be in a format that is publicly
  documented (and with an implementation available to the public in
  source code form), and must require no special password or key for
  unpacking, reading or copying.
  
    7. Additional Terms.
  
    "Additional permissions" are terms that supplement the terms of this
  License by making exceptions from one or more of its conditions.
  Additional permissions that are applicable to the entire Program shall
  be treated as though they were included in this License, to the extent
  that they are valid under applicable law.  If additional permissions
  apply only to part of the Program, that part may be used separately
  under those permissions, but the entire Program remains governed by
  this License without regard to the additional permissions.
  
    When you convey a copy of a covered work, you may at your option
  remove any additional permissions from that copy, or from any part of
  it.  (Additional permissions may be written to require their own
  removal in certain cases when you modify the work.)  You may place
  additional permissions on material, added by you to a covered work,
  for which you have or can give appropriate copyright permission.
  
    Notwithstanding any other provision of this License, for material you
  add to a covered work, you may (if authorized by the copyright holders of
  that material) supplement the terms of this License with terms:
  
      a) Disclaiming warranty or limiting liability differently from the
      terms of sections 15 and 16 of this License; or
  
      b) Requiring preservation of specified reasonable legal notices or
      author attributions in that material or in the Appropriate Legal
      Notices displayed by works containing it; or
  
      c) Prohibiting misrepresentation of the origin of that material, or
      requiring that modified versions of such material be marked in
      reasonable ways as different from the original version; or
  
      d) Limiting the use for publicity purposes of names of licensors or
      authors of the material; or
  
      e) Declining to grant rights under trademark law for use of some
      trade names, trademarks, or service marks; or
  
      f) Requiring indemnification of licensors and authors of that
      material by anyone who conveys the material (or modified versions of
      it) with contractual assumptions of liability to the recipient, for
      any liability that these contractual assumptions directly impose on
      those licensors and authors.
  
    All other non-permissive additional terms are considered "further
  restrictions" within the meaning of section 10.  If the Program as you
  received it, or any part of it, contains a notice stating that it is
  governed by this License along with a term that is a further
  restriction, you may remove that term.  If a license document contains
  a further restriction but permits relicensing or conveying under this
  License, you may add to a covered work material governed by the terms
  of that license document, provided that the further restriction does
  not survive such relicensing or conveying.
  
    If you add terms to a covered work in accord with this section, you
  must place, in the relevant source files, a statement of the
  additional terms that apply to those files, or a notice indicating
  where to find the applicable terms.
  
    Additional terms, permissive or non-permissive, may be stated in the
  form of a separately written license, or stated as exceptions;
  the above requirements apply either way.
  
    8. Termination.
  
    You may not propagate or modify a covered work except as expressly
  provided under this License.  Any attempt otherwise to propagate or
  modify it is void, and will automatically terminate your rights under
  this License (including any patent licenses granted under the third
  paragraph of section 11).
  
    However, if you cease all violation of this License, then your
  license from a particular copyright holder is reinstated (a)
  provisionally, unless and until the copyright holder explicitly and
  finally terminates your license, and (b) permanently, if the copyright
  holder fails to notify you of the violation by some reasonable means
  prior to 60 days after the cessation.
  
    Moreover, your license from a particular copyright holder is
  reinstated permanently if the copyright holder notifies you of the
  violation by some reasonable means, this is the first time you have
  received notice of violation of this License (for any work) from that
  copyright holder, and you cure the violation prior to 30 days after
  your receipt of the notice.
  
    Termination of your rights under this section does not terminate the
  licenses of parties who have received copies or rights from you under
  this License.  If your rights have been terminated and not permanently
  reinstated, you do not qualify to receive new licenses for the same
  material under section 10.
  
    9. Acceptance Not Required for Having Copies.
  
    You are not required to accept this License in order to receive or
  run a copy of the Program.  Ancillary propagation of a covered work
  occurring solely as a consequence of using peer-to-peer transmission
  to receive a copy likewise does not require acceptance.  However,
  nothing other than this License grants you permission to propagate or
  modify any covered work.  These actions infringe copyright if you do
  not accept this License.  Therefore, by modifying or propagating a
  covered work, you indicate your acceptance of this License to do so.
  
    10. Automatic Licensing of Downstream Recipients.
  
    Each time you convey a covered work, the recipient automatically
  receives a license from the original licensors, to run, modify and
  propagate that work, subject to this License.  You are not responsible
  for enforcing compliance by third parties with this License.
  
    An "entity transaction" is a transaction transferring control of an
  organization, or substantially all assets of one, or subdividing an
  organization, or merging organizations.  If propagation of a covered
  work results from an entity transaction, each party to that
  transaction who receives a copy of the work also receives whatever
  licenses to the work the party's predecessor in interest had or could
  give under the previous paragraph, plus a right to possession of the
  Corresponding Source of the work from the predecessor in interest, if
  the predecessor has it or can get it with reasonable efforts.
  
    You may not impose any further restrictions on the exercise of the
  rights granted or affirmed under this License.  For example, you may
  not impose a license fee, royalty, or other charge for exercise of
  rights granted under this License, and you may not initiate litigation
  (including a cross-claim or counterclaim in a lawsuit) alleging that
  any patent claim is infringed by making, using, selling, offering for
  sale, or importing the Program or any portion of it.
  
    11. Patents.
  
    A "contributor" is a copyright holder who authorizes use under this
  License of the Program or a work on which the Program is based.  The
  work thus licensed is called the contributor's "contributor version".
  
    A contributor's "essential patent claims" are all patent claims
  owned or controlled by the contributor, whether already acquired or
  hereafter acquired, that would be infringed by some manner, permitted
  by this License, of making, using, or selling its contributor version,
  but do not include claims that would be infringed only as a
  consequence of further modification of the contributor version.  For
  purposes of this definition, "control" includes the right to grant
  patent sublicenses in a manner consistent with the requirements of
  this License.
  
    Each contributor grants you a non-exclusive, worldwide, royalty-free
  patent license under the contributor's essential patent claims, to
  make, use, sell, offer for sale, import and otherwise run, modify and
  propagate the contents of its contributor version.
  
    In the following three paragraphs, a "patent license" is any express
  agreement or commitment, however denominated, not to enforce a patent
  (such as an express permission to practice a patent or covenant not to
  sue for patent infringement).  To "grant" such a patent license to a
  party means to make such an agreement or commitment not to enforce a
  patent against the party.
  
    If you convey a covered work, knowingly relying on a patent license,
  and the Corresponding Source of the work is not available for anyone
  to copy, free of charge and under the terms of this License, through a
  publicly available network server or other readily accessible means,
  then you must either (1) cause the Corresponding Source to be so
  available, or (2) arrange to deprive yourself of the benefit of the
  patent license for this particular work, or (3) arrange, in a manner
  consistent with the requirements of this License, to extend the patent
  license to downstream recipients.  "Knowingly relying" means you have
  actual knowledge that, but for the patent license, your conveying the
  covered work in a country, or your recipient's use of the covered work
  in a country, would infringe one or more identifiable patents in that
  country that you have reason to believe are valid.
  
    If, pursuant to or in connection with a single transaction or
  arrangement, you convey, or propagate by procuring conveyance of, a
  covered work, and grant a patent license to some of the parties
  receiving the covered work authorizing them to use, propagate, modify
  or convey a specific copy of the covered work, then the patent license
  you grant is automatically extended to all recipients of the covered
  work and works based on it.
  
    A patent license is "discriminatory" if it does not include within
  the scope of its coverage, prohibits the exercise of, or is
  conditioned on the non-exercise of one or more of the rights that are
  specifically granted under this License.  You may not convey a covered
  work if you are a party to an arrangement with a third party that is
  in the business of distributing software, under which you make payment
  to the third party based on the extent of your activity of conveying
  the work, and under which the third party grants, to any of the
  parties who would receive the covered work from you, a discriminatory
  patent license (a) in connection with copies of the covered work
  conveyed by you (or copies made from those copies), or (b) primarily
  for and in connection with specific products or compilations that
  contain the covered work, unless you entered into that arrangement,
  or that patent license was granted, prior to 28 March 2007.
  
    Nothing in this License shall be construed as excluding or limiting
  any implied license or other defenses to infringement that may
  otherwise be available to you under applicable patent law.
  
    12. No Surrender of Others' Freedom.
  
    If conditions are imposed on you (whether by court order, agreement or
  otherwise) that contradict the conditions of this License, they do not
  excuse you from the conditions of this License.  If you cannot convey a
  covered work so as to satisfy simultaneously your obligations under this
  License and any other pertinent obligations, then as a consequence you may
  not convey it at all.  For example, if you agree to terms that obligate you
  to collect a royalty for further conveying from those to whom you convey
  the Program, the only way you could satisfy both those terms and this
  License would be to refrain entirely from conveying the Program.
  
    13. Use with the GNU Affero General Public License.
  
    Notwithstanding any other provision of this License, you have
  permission to link or combine any covered work with a work licensed
  under version 3 of the GNU Affero General Public License into a single
  combined work, and to convey the resulting work.  The terms of this
  License will continue to apply to the part which is the covered work,
  but the special requirements of the GNU Affero General Public License,
  section 13, concerning interaction through a network will apply to the
  combination as such.
  
    14. Revised Versions of this License.
  
    The Free Software Foundation may publish revised and/or new versions of
  the GNU General Public License from time to time.  Such new versions will
  be similar in spirit to the present version, but may differ in detail to
  address new problems or concerns.
  
    Each version is given a distinguishing version number.  If the
  Program specifies that a certain numbered version of the GNU General
  Public License "or any later version" applies to it, you have the
  option of following the terms and conditions either of that numbered
  version or of any later version published by the Free Software
  Foundation.  If the Program does not specify a version number of the
  GNU General Public License, you may choose any version ever published
  by the Free Software Foundation.
  
    If the Program specifies that a proxy can decide which future
  versions of the GNU General Public License can be used, that proxy's
  public statement of acceptance of a version permanently authorizes you
  to choose that version for the Program.
  
    Later license versions may give you additional or different
  permissions.  However, no additional obligations are imposed on any
  author or copyright holder as a result of your choosing to follow a
  later version.
  
    15. Disclaimer of Warranty.
  
    THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
  APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
  HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
  OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
  PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
  IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
  ALL NECESSARY SERVICING, REPAIR OR CORRECTION.
  
    16. Limitation of Liability.
  
    IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
  WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
  THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
  GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
  USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
  DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
  PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
  EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
  SUCH DAMAGES.
  
    17. Interpretation of Sections 15 and 16.
  
    If the disclaimer of warranty and limitation of liability provided
  above cannot be given local legal effect according to their terms,
  reviewing courts shall apply local law that most closely approximates
  an absolute waiver of all civil liability in connection with the
  Program, unless a warranty or assumption of liability accompanies a
  copy of the Program in return for a fee.
  
                       END OF TERMS AND CONDITIONS
  
              How to Apply These Terms to Your New Programs
  
    If you develop a new program, and you want it to be of the greatest
  possible use to the public, the best way to achieve this is to make it
  free software which everyone can redistribute and change under these terms.
  
    To do so, attach the following notices to the program.  It is safest
  to attach them to the start of each source file to most effectively
  state the exclusion of warranty; and each file should have at least
  the "copyright" line and a pointer to where the full notice is found.
  
      CopyrightÂ© 2020 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
  
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
  
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  Also add information on how to contact you by electronic and paper mail.
  
    If the program does terminal interaction, make it output a short
  notice like this when it starts in an interactive mode:
  
      {project}  Copyright (C) {year}  {fullname}
      This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
      This is free software, and you are welcome to redistribute it
      under certain conditions; type `show c' for details.
  
  The hypothetical commands `show w' and `show c' should show the appropriate
  parts of the General Public License.  Of course, your program's commands
  might be different; for a GUI interface, you would use an "about box".
  
    You should also get your employer (if you work as a programmer) or school,
  if any, to sign a "copyright disclaimer" for the program, if necessary.
  For more information on this, and how to apply and follow the GNU GPL, see
  <http://www.gnu.org/licenses/>.
  
    The GNU General Public License does not permit incorporating your program
  into proprietary programs.  If your program is a subroutine library, you
  may consider it more useful to permit linking proprietary applications with
  the library.  If this is what you want to do, use the GNU Lesser General
  Public License instead of this License.  But first, please read
  <http://www.gnu.org/philosophy/why-not-lgpl.html>.

Package: simtrial
File: NAMESPACE
Format: text
Content:
  # Generated by roxygen2: do not edit by hand
  
  export(cutData)
  export(cutDataAtCount)
  export(fixedBlockRand)
  export(getCutDateForCount)
  export(pMaxCombo)
  export(pwexpfit)
  export(rpwenroll)
  export(rpwexp)
  export(simPWSurv)
  export(simfix)
  export(simfix2simPWSurv)
  export(tenFH)
  export(tenFHcorr)
  export(tensurv)
  export(wMB)
  import(dplyr)
  import(mvtnorm)
  import(survival)
  import(tibble)
  import(tidyr)

Package: simtrial
File: NEWS.md
Format: text
Content:
  ## simtrial 0.2.0, August, 2020
  
  - Updated vignettes and web site
  - Prepared for Regulatory/Industry Training session in September
  
  ## simtrial 0.1.7.9004, February, 2020
  
  - Added wMB() to compute Magirr-Burman weights
  - Added vignette to demonstrate working with different weighting schemes
  - Replaced Depends with Imports in DESCRIPTION
  
  ## simtrial 0.1.7.9003, November, 2019
  
  - Incorporated new functions to simplify use (simfix, simfix2simPWSurv, pMaxCombo)
  - Removed hgraph with intent to put it into a release of gsDesign
  - Limited to 2 essential vignettes
  - Added continuous integration/continuous deployment (yaml) and pkgdown for web site development
  - Limited dependencies to those that are essential; this removed some convenience functions not related to core package functionality
  
  

Package: simtrial
File: README.md
Format: text
Content:
  # simtrial
  
  <!-- badges: start -->
  [![R build status](https://github.com/Merck/simtrial/workflows/R-CMD-check/badge.svg)](https://github.com/Merck/simtrial/actions)
  [![Codecov test coverage](https://codecov.io/gh/Merck/simtrial/branch/main/graph/badge.svg)](https://codecov.io/gh/Merck/simtrial?branch=main)
  <!-- badges: end -->
  
  ## Installation
  
  You can install from GitHub:
  
  ```r
  remotes::install_github("Merck/simtrial")
  ```
  
  ## Overview
  
  `simtrial` is a small package built initially to focus on evaluating weighted logrank tests and combination tests based on such tests. The intent is to use `tidyverse` (data wrangling) programming procedures and to have a package that is easy to qualify for use in a regulated environment.
  
  Initial areas of focus are:
  
  - Generating time-to-event data for stratified trials using piecewise constant enrollment and piecewise exponential failure rates. Both proportional and non-proportional hazards are supported.
  Under proportional hazards, the assumptions are along the lines of those used by Lachin and Foulkes as implemented in the `gsDesign` for deriving group sequential designs.
  - Setting up data cutoffs for (interim and final) analyses.
  - Support for weighted logrank tests with arbitrary weighting schemes, specifically supporting the Fleming-Harrington set of tests, including the logrank test.
  
  # Future developments
  
  Expectations for future development include:
  
  - Provide a test suite to document that the package is fit for use in a regulatory environment.
  - Further examples.

Package: simtrial
File: R/cutData.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  NULL
  
  #' Cut a Dataset for Analysis at a Specified Date
  #' 
  #' @param x a time-to-event dataset, e.g., generated by \code{simPWSurv}
  #' @param cutDate date relative to start of randomization (\code{cte} from input dataset)
  #' at which dataset is to be cut off for analysis
  #' @return A dataset ready for survival analysis
  #' @examples
  #' # Use default enrollment and event rates and cut at calendar time 5 after start
  #' # of randomization
  #' library(dplyr)
  #' simPWSurv(n=20) %>% cutData(5)
  #' @export
  cutData <- function(x,cutDate){
    return(x %>%
             filter(enrollTime <= cutDate) %>%
             mutate(tte=pmin(cte,cutDate)-enrollTime,
                    event=fail*(cte<=cutDate)) %>%
             select(tte,event,Stratum,Treatment)
    )
  }
  

Package: simtrial
File: R/cutDataAtCount.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Cut a Dataset for Analysis at a Specified Event Count
  #'
  #' `cutDataAtCount` takes a time-to-event data set and cuts the data at which an
  #' event count is reached. 
  #'
  #' @param x a time-to-event dataset, e.g., generated by \code{simPWSurv}
  #' @param count event count at which data cutoff is to be made
  #' 
  #' @examples
  #' library(tidyr)
  #' # Use default enrollment and event rates at cut at 100 events
  #' x <- simPWSurv(n=200) %>% cutDataAtCount(100)
  #' table(x$event,x$Treatment)
  #' 
  #' @return a \code{tibble} ready for survival analysis, including culumns time to event (`tte`), 
  #' `event`, the `stratum` and the `treatment.`
  #' 
  #' @export
  
  cutDataAtCount <- function(x,count){
    ctecut <- getCutDateForCount(x,count)
    return(cutData(x,ctecut))
  }

Package: simtrial
File: R/Ex1delayedEffect.R
Format: text
Content:
  #' @import survival
  NULL
  
  #' Time-to-event data example 1 for non-proportional hazards working group
  #'
  #' Survival objects reverse-engineered datasets from published Kaplan-Meier
  #' curves. 
  #' Individual trials are de-identified since the data are only
  #' approximations of the actual data.
  #' Data are intended to evaluate methods and designs for trials where
  #' non-proportional hazards may be anticipated for outcome data.  
  #'
  #' @docType data
  #'
  #' @usage data(Ex1delayedEffect)
  #'
  #' @format Data frame with 4 variables:
  #' \describe{ 
  #' \item{id}{sequential numbering of unique identifiers}
  #' \item{month}{time-to-event}
  #' \item{event}{1 for event, 0 for censored} 
  #' \item{trt}{1 for experimental, 0 for control}
  #' }
  #' 
  #' @keywords datasets
  #'
  #' @references TBD
  #' 
  #' @seealso \code{\link{Ex2delayedEffect}}, \code{\link{Ex3curewithph}}, \code{\link{Ex4belly}},
  #' \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  #'
  #' @examples
  #' library(survival)
  #' data(Ex1delayedEffect)
  #' km1 <- with(Ex1delayedEffect,survfit(Surv(month,evntd)~trt))
  #' km1
  #' plot(km1)
  #' with(subset(Ex1delayedEffect,trt==1),survfit(Surv(month,evntd)~trt))
  #' with(subset(Ex1delayedEffect,trt==0),survfit(Surv(month,evntd)~trt))
  "Ex1delayedEffect"

Package: simtrial
File: R/Ex2delayedEffect.R
Format: text
Content:
  #' @import survival
  NULL
  
  #' Time-to-event data example 2 for non-proportional hazards working group
  #'
  #' Survival objects reverse-engineered datasets from published Kaplan-Meier
  #' curves. 
  #' Individual trials are de-identified since the data are only
  #' approximations of the actual data.
  #' Data are intended to evaluate methods and designs for trials where
  #' non-proportional hazards may be anticipated for outcome data.  
  #'
  #' @docType data
  #'
  #' @usage data(Ex2delayedEffect)
  #'
  #' @format Data frame with 4 variables:
  #' \describe{ 
  #' \item{id}{sequential numbering of unique identifiers}
  #' \item{month}{time-to-event}
  #' \item{event}{1 for event, 0 for censored} 
  #' \item{trt}{1 for experimental, 0 for control}
  #' }
  #' 
  #' @keywords datasets
  #'
  #' @references TBD
  #' 
  #' @seealso \code{\link{Ex1delayedEffect}}, \code{\link{Ex3curewithph}}, \code{\link{Ex4belly}},
  #' \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  #'
  #' @examples
  #' library(survival)
  #' data(Ex2delayedEffect)
  #' km1 <- with(Ex2delayedEffect,survfit(Surv(month,evntd)~trt))
  #' km1
  #' plot(km1)
  #' with(subset(Ex2delayedEffect,trt==1),survfit(Surv(month,evntd)~trt))
  #' with(subset(Ex2delayedEffect,trt==0),survfit(Surv(month,evntd)~trt))
  "Ex2delayedEffect"

Package: simtrial
File: R/Ex3curewithph.R
Format: text
Content:
  #' @import survival
  NULL
  
  #' Time-to-event data example 3 for non-proportional hazards working group
  #'
  #' Survival objects reverse-engineered datasets from published Kaplan-Meier
  #' curves. 
  #' Individual trials are de-identified since the data are only
  #' approximations of the actual data.
  #' Data are intended to evaluate methods and designs for trials where
  #' non-proportional hazards may be anticipated for outcome data.  
  #'
  #' @docType data
  #'
  #' @usage data(Ex3curewithph)
  #'
  #' @format Data frame with 4 variables:
  #' \describe{ 
  #' \item{id}{sequential numbering of unique identifiers}
  #' \item{month}{time-to-event}
  #' \item{event}{1 for event, 0 for censored} 
  #' \item{trt}{1 for experimental, 0 for control}
  #' }
  #' 
  #' @keywords datasets
  #'
  #' @references TBD
  #' 
  #' @seealso \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, \code{\link{Ex4belly}},
  #' \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  #'
  #' @examples
  #' library(survival)
  #' data(Ex3curewithph)
  #' km1 <- with(Ex3curewithph,survfit(Surv(month,evntd)~trt))
  #' km1
  #' plot(km1)
  "Ex3curewithph"

Package: simtrial
File: R/Ex4belly.R
Format: text
Content:
  #' @import survival
  NULL
  
  #' Time-to-event data example 4 for non-proportional hazards working group
  #'
  #' Survival objects reverse-engineered datasets from published Kaplan-Meier
  #' curves. 
  #' Individual trials are de-identified since the data are only
  #' approximations of the actual data.
  #' Data are intended to evaluate methods and designs for trials where
  #' non-proportional hazards may be anticipated for outcome data.  
  #'
  #' @docType data
  #'
  #' @usage data(Ex4belly)
  #'
  #' @format Data frame with 4 variables:
  #' \describe{ 
  #' \item{id}{sequential numbering of unique identifiers}
  #' \item{month}{time-to-event}
  #' \item{event}{1 for event, 0 for censored} 
  #' \item{trt}{1 for experimental, 0 for control}
  #' }
  #' 
  #' @keywords datasets
  #'
  #' @references TBD
  #' 
  #' @seealso \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, 
  #' \code{\link{Ex3curewithph}},
  #' \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  #'
  #' @examples
  #' library(survival)
  #' data(Ex4belly)
  #' km1 <- with(Ex4belly,survfit(Surv(month,evntd)~trt))
  #' km1
  #' plot(km1)
  "Ex4belly"

Package: simtrial
File: R/Ex5widening.R
Format: text
Content:
  #' @import survival
  NULL
  
  #' Time-to-event data example 5 for non-proportional hazards working group
  #'
  #' Survival objects reverse-engineered datasets from published Kaplan-Meier
  #' curves. 
  #' Individual trials are de-identified since the data are only
  #' approximations of the actual data.
  #' Data are intended to evaluate methods and designs for trials where
  #' non-proportional hazards may be anticipated for outcome data.  
  #'
  #' @docType data
  #'
  #' @usage data(Ex5widening)
  #'
  #' @format Data frame with 4 variables:
  #' \describe{ 
  #' \item{id}{sequential numbering of unique identifiers}
  #' \item{month}{time-to-event}
  #' \item{event}{1 for event, 0 for censored} 
  #' \item{trt}{1 for experimental, 0 for control}
  #' }
  #' 
  #' @keywords datasets
  #'
  #' @references TBD
  #' 
  #' @seealso \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, 
  #' \code{\link{Ex3curewithph}},
  #' \code{\link{Ex4belly}}, \code{\link{Ex6crossing}}
  #'
  #' @examples
  #' library(survival)
  #' data(Ex5widening)
  #' km1 <- with(Ex5widening,survfit(Surv(month,evntd)~trt))
  #' km1
  #' plot(km1)
  "Ex5widening"

Package: simtrial
File: R/Ex6crossing.R
Format: text
Content:
  #' @import survival
  NULL
  
  #' Time-to-event data example 6 for non-proportional hazards working group
  #'
  #' Survival objects reverse-engineered datasets from published Kaplan-Meier
  #' curves. 
  #' Individual trials are de-identified since the data are only
  #' approximations of the actual data.
  #' Data are intended to evaluate methods and designs for trials where
  #' non-proportional hazards may be anticipated for outcome data.  
  #'
  #' @docType data
  #'
  #' @usage data(Ex6crossing)
  #'
  #' @format Data frame with 4 variables:
  #' \describe{ 
  #' \item{id}{sequential numbering of unique identifiers}
  #' \item{month}{time-to-event}
  #' \item{event}{1 for event, 0 for censored} 
  #' \item{trt}{1 for experimental, 0 for control}
  #' }
  #' 
  #' @keywords datasets
  #'
  #' @references TBD
  #' 
  #' @seealso \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, 
  #' \code{\link{Ex3curewithph}},
  #' \code{\link{Ex4belly}}, \code{\link{Ex5widening}}
  #'
  #' @examples
  #' library(survival)
  #' data(Ex6crossing)
  #' km1 <- with(Ex6crossing,survfit(Surv(month,evntd)~trt))
  #' km1
  #' plot(km1)
  "Ex6crossing"

Package: simtrial
File: R/fixedBlockRand.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' Permuted fixed block randomization
  #'
  #' Fixed block randomization. The `block` input should repeat each treatment code the number of
  #' times it is to be included within each block. The final block will be a partial block if `n` is not an
  #' exact multiple of the block length.
  #'
  #' @param n sample size to be randomized
  #' @param block Vector of treatments to be included in each block
  #' @return A treatment group sequence (vector) of length `n` with treatments from `block` permuted within
  #' each block having block size equal to the length of `block`
  #' @examples
  #' library(dplyr)
  #' # 2:1 randomization with block size 3, treatments "A" and "B"
  #' tibble(x=1:10) %>% mutate(Treatment=fixedBlockRand(block=c("A","B","B")))
  #' # Stratified randomization
  #' tibble(Stratum=c(rep("A",10),rep("B",10))) %>%
  #' group_by(Stratum) %>%
  #' mutate(Treatment=fixedBlockRand())
  #' @export
  fixedBlockRand <- function(n=10, block=c(0,0,1,1)){
    length_block <- length(block)
    nblock <- ceiling(n/length_block)
    ntot <- nblock * length_block
    tx <- NULL
    u <- NULL
    tx <- rep(block, each = nblock)
    u <- rep(1:nblock, length_block)
    u <- u + stats::runif(ntot)
    return((tx[order(u)])[1:n])
  }

Package: simtrial
File: R/getCutDateForCount.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  NULL
  #' Get Date at Which an Event Count is Reached
  #'
  #' @param x a time-to-event dataset, e.g., generated by \code{simPWSurv}
  #' @param count event count at which dataset is to be cut off for analysis
  #' 
  #' @examples
  #' library(dplyr)
  #' # Use default enrollment and calendar cut date for 50 events in Positive stratum
  #' x <- simPWSurv(n=200,
  #'                strata = tibble::tibble(Stratum=c("Positive","Negative"), p = c(.5, .5)),
  #'                failRates = tibble::tibble(Stratum = rep(c("Positive","Negative"),2),
  #'                                           period = rep(1, 4),
  #'                                           Treatment = c(rep("Control", 2), 
  #'                                                         rep("Experimental", 2)),
  #'                                           duration = rep(1, 4),
  #'                                           rate = log(2) / c(6, 9, 9, 12)
  #'                                           ),
  #'                dropoutRates = tibble::tibble(Stratum = rep(c("Positive","Negative"),2),
  #'                                              period = rep(1, 4),
  #'                                              Treatment = c(rep("Control", 2), 
  #'                                                            rep("Experimental", 2)),
  #'                                              duration = rep(1, 4),
  #'                                              rate = rep(.001, 4)
  #'                                             )
  #'               )
  #' d <- getCutDateForCount(filter(x,Stratum=="Positive"),count=50)
  #' y <- cutData(x,cutDate=d)
  #' table(y$Stratum,y$event)
  #' 
  #' @return The a numeric value with the \code{cte} from the input dataset at which the targeted event count
  #' is reached, or if the final event count is never reached, the final \code{cte} at which an event occurs.
  #' 
  #' @export
  
  getCutDateForCount <- function(x,count){
    y <- ungroup(x) %>% select(cte,fail) %>% filter(fail==1) %>% select(cte) %>% arrange(cte) %>%
           mutate(eventCount=row_number()) %>%
           subset(eventCount<=count)
    return(last(y$cte))
  }

Package: simtrial
File: R/global.R
Format: text
Content:
  # These global variables are declared to eliminate associated R cmd check warnings.
  # There is no other identified functional impact of these global declarations.
  
  utils::globalVariables(
    c('atrisk',
      'Count',
      'cte',
      'dropoutRate',
      'dropoutTime',
      'enrollTime',
      'eventCount',
      'events', 
      'Ex1delayedEffect', 
      'fail',
      'failRate',
      'failTime',
      'hr',
      'duration',
      'enrollTime',
      'event',
      'finish',
      'lambda',
      'mtte', 
      'N',
      'OminusE',
      'one',
      'origin',
      'period',
      'rate',
      's', 
      'S',
      'status',
      'Stratum',
      'time',
      'Treatment',
      'tte',
      'txevents',
      'Var',
      'w',
      'wOminusE',
      'wVar',
      'txatrisk'
    )
  )

Package: simtrial
File: R/MBdelayed.R
Format: text
Content:
  #' Simulated survival dataset with delayed treatment effect
  #' 
  #' Magirr and Burman (2019) considered several scenarios for their modestly weighted logrank test.
  #' One of these had a delayed treatment effect with a hazard ratio of 1 for 6 months followed by a hazard ratio of 1/2
  #' thereafter.
  #' The scenario enrolled 200 patients uniformly over 12 months and 
  #' cut data for analysis 36 months after enrollment was opened. 
  #' This dataset was generated by the `simtrial::simPWSurv()` function under the above scenario.
  #' 
  #' @format A tibble with 200 rows and xx columns
  #' \describe{
  #' \item{tte}{time to event}
  #' }
  #' @examples
  #' library(tidyr)
  #' library(dplyr)
  #' library(survival)
  #' library(mvtnorm)
  #' fit <- survfit(Surv(tte, event) ~ Treatment, data = MBdelayed)
  #' 
  #' # Plot survival
  #' plot(fit, lty=1:2) 
  #' legend("topright", legend = c("Control", "Experimental"), lty = 1:2)
  #' 
  #' # Set up time, event, number of event dataset for testing
  #' # with arbitrary weights
  #' ten <- MBdelayed %>% tensurv(txval = "Experimental")
  #' head(ten)
  #' 
  #' # MaxCombo with logrank, FH(0,1), FH(1,1)
  #' ten %>% tenFHcorr(rg=tibble(rho=c(0, 0, 1), gamma=c(0, 1, 1))) %>%
  #'         pMaxCombo()
  #' 
  #' # Magirr-Burman modestly down-weighted rank test with 6 month delay
  #' # First, add weights
  #' ten <- ten %>% wMB(6)
  #' head(ten)
  #' 
  #' # Now compute test based on these weights
  #' ten %>% summarise(S = sum(OminusE*wMB),
  #'                   V = sum(Var*wMB^2),
  #'                   Z = S/sqrt(V)) %>%
  #'         mutate(p=pnorm(Z))
  #' 
  #' # Create 0 weights for first 6 months
  #' ten <- ten %>% mutate(w6 = 1 * (tte >= 6))
  #' ten %>% summarise(S = sum(OminusE*w6),
  #'                   V = sum(Var*w6^2),
  #'                   Z = S/sqrt(V)) %>% 
  #'         mutate(p=pnorm(Z))
  #'         
  #' # Generate another dataset
  #' ds <- simPWSurv(n = 200,
  #'                 enrollRates = tibble(rate = 200 / 12, duration = 12),
  #'                 failRates = tribble(
  #'                    ~Stratum, ~Period, ~Treatment,     ~duration, ~rate,
  #'                    "All",        1,   "Control",      42,        log(2) / 15,
  #'                    "All",        1,   "Experimental", 6,         log(2) / 15,
  #'                    "All",        2,   "Experimental", 36,        log(2) / 15 * 0.6),
  #'                 dropoutRates = tribble(
  #'                    ~Stratum, ~Period, ~Treatment,     ~duration, ~rate,
  #'                    "All",        1,   "Control",      42,        0,
  #'                    "All",        1,   "Experimental", 42,        0)
  #'        )
  #' # Cut data at 24 months after final enrollment
  #' MBdelayed2 <- ds %>% cutData(max(ds$enrollTime) + 24)
  #' 
  #' @references 
  #' Magirr, Dominic, and CarlâFredrik Burman. 
  #' "Modestly weighted logrank tests." 
  #' \emph{Statistics in Medicine} 38.20 (2019): 3782-3790.
  #' 
  "MBdelayed"

Package: simtrial
File: R/pMaxCombo.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import dplyr
  #' @import tibble
  #' @import mvtnorm
  NULL
  
  #' MaxCombo p-value
  #'
  #' \code{pMaxCombo()} computes p-values for the MaxCombo test
  #' based on output from \code{simtrial::tenFHcorr()}.
  #' This is still in an experimental stage and is intended for use with
  #' the \code{simtrial::simfix()} trial simulation routine.
  #' However, it can also be used to analyze clinical trial data such as that provided in the
  #' ADaM ADTTE format.
  #' @param Z a dataset output from \code{tenFHcorr()}; see examples.
  #' @param dummyvar a dummy input that allows \code{group_map()} to be used to
  #' compute p-values for multiple simulations.
  #' @param algorithm This is passed directly to the \code{algorithm} argument in the \code{mvtnorm::pmvnorm()}
  #' @return A numeric p-value
  #' @examples
  #' library(tidyr)
  #' x <- simfix(nsim=1,timingType=5,rg=tibble::tibble(rho=c(0,0,1),gamma=c(0,1,1)))
  #' head(x)
  #' pMaxCombo(x)
  #' # Only use cuts for events, events + min follow-up
  #' xx <- simfix(nsim=100,timingType=5,rg=tibble::tibble(rho=c(0,0,1),gamma=c(0,1,1)))
  #' head(xx)
  #' # MaxCombo power estimate for cutoff at max of targeted events, minimum follow-up
  #' p <- unlist(xx %>%  dplyr::group_by(Sim) %>% dplyr::group_map(pMaxCombo))
  #' mean(p<.025)
  #' @export
  pMaxCombo <- function(Z,dummyvar, algorithm=GenzBretz(maxpts=50000,abseps=0.00001)){
    MaxCombo <- as.numeric(min(Z$Z))
    # correlation matrix
    corr <- data.matrix(Z %>% select(starts_with("V")))
    as.numeric(1-mvtnorm::pmvnorm(lower=rep(MaxCombo,nrow(Z)),
                                  corr=corr,
                                  algorithm=algorithm)[1])
  }

Package: simtrial
File: R/pwexpfit.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import survival
  NULL
  
  #' Piecewise exponential survival estimation
  #'
  #' Computes survival function, density function, -2*log-likelihood based
  #' on input dataset and intervals for piecewise constant failure rates.
  #' Initial version assumes observations are right censored or events only.
  #'
  #' @param Srv input survival object (see \code{Surv}); note that only 0=censored, 1=event for \code{Surv}
  #' @param intervals Vector containing positive values indicating interval lengths where the 
  #' exponential rates are assumed. 
  #' Note that a final infinite interval is added if any events occur after the final interval 
  #' specified.
  #'
  #' @return A matrix with rows containing interval length, estimated rate, -2*log-likelihood for each interval.
  #'
  #' @examples
  #' # use default arguments for delayed effect example dataset (Ex1delayedEffect)
  #' library(survival)
  #' rateall <- pwexpfit()
  #' rateall
  #' # Estimate by treatment effect
  #' rate1 <- with(subset(Ex1delayedEffect,trt==1), pwexpfit(Surv(month,evntd)))
  #' rate0 <- with(subset(Ex1delayedEffect,trt==0), pwexpfit(Surv(month,evntd)))
  #' rate1
  #' rate0
  #' rate1$rate/rate0$rate
  #' # chi-square test for (any) treatment effect (8 - 4 parameters = 4 df)
  #' pchisq(sum(rateall$m2ll)-sum(rate1$m2ll+rate0$m2ll), df = 4, lower.tail=FALSE)
  #' # compare with logrank
  #' survdiff(formula = Surv(month, evntd) ~ trt, data = Ex1delayedEffect)
  #' # simple model with 3 rates same for each for 3 months, 
  #' # different for each treatment after months
  #' rate1a <- with(subset(Ex1delayedEffect,trt==1), pwexpfit(Surv(month,evntd),3))
  #' rate0a <- with(subset(Ex1delayedEffect,trt==0), pwexpfit(Surv(month,evntd),3))
  #' rate1a$rate/rate0a$rate
  #' m2ll0 <- rateall$m2ll[1]+rate1a$m2ll[2]+rate0a$m2ll[2]
  #' m2ll1 <- sum(rate0$m2ll)+sum(rate1$m2ll)
  #' # as a measure of strength, chi-square examines improvement in likelihood
  #' pchisq(m2ll0-m2ll1, 5, lower.tail=FALSE)
  #' 
  #' @export
  pwexpfit <- function(Srv = Surv(time=Ex1delayedEffect$month, event=Ex1delayedEffect$evntd),
                        intervals=array(3,3)){
    if (!is.Surv(Srv)) stop("Srv must be a survival object")
    xx <- data.frame(time=Srv[,"time"], status=Srv[,"status"])
    # only allow status 0,1
    if (nrow(subset(xx,status != 0 & status!=1))) stop("Srv may only have status values of 0 or 1")
    # check for late observation after sum(intervals)
    if (nrow(subset(xx,time>sum(intervals)&status>0))>0) intervals <- c(intervals,Inf)
    times <- c(0,cumsum(intervals))
    rval <- NULL
    for(i in 1:length(intervals)){
      dat <- subset(xx,time>times[i])
      dat$status[dat$time>times[i+1]]<-0
      dat$time[dat$time>times[i+1]]<-times[i+1]
      dat$time <- dat$time - times[i]
      events <- sum(dat$status)
      TTOT <- sum(dat$time)
      rate <- events/TTOT
      if (TTOT>0) rval <- rbind(rval,data.frame(intervals=intervals[i],
                                                TTOT=TTOT, events=events, rate=rate, 
                                                m2ll=2*(rate*TTOT-events*log(rate))))
    }
    return(rval)
  }

Package: simtrial
File: R/rpwenroll.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  #' @import tidyr
  NULL
  
  #' Generate Piecewise Exponential Enrollment
  #'
  #' With piecewise exponential enrollment rate generation any enrollment rate distribution can be easily approximated.
  #' \code{rpwenroll()} is to support simulation of both the Lachin and Foulkes (1986) sample size
  #' method for (fixed trial duration) as well as the Kim and Tsiatis(1990) method
  #' (fixed enrollment rates and either fixed enrollment duration or fixed minimum follow-up);
  #' see \code{\link[gsDesign:nSurv]{gsDesign}}.
  #'
  #' @param n Number of observations.
  #' Default of \code{NULL} yields random enrollment size.
  #' @param enrollRates A tibble containing period duration (\code{duration}) and enrollment rate (\code{rate})
  #' for specified enrollment periods.
  #' If necessary, last period will be extended to ensure enrollment of specified \code{n}.
  #' @return A vector of random enrollment times.
  #' @examples
  #' # piecewise uniform (piecewise exponential inter-arrival times) for 10k patients enrollment
  #' # enrollment rates of 5 for time 0-100, 15 for 100-300, and 30 thereafter
  #' x <- rpwenroll(n=10000, enrollRates=tibble::tibble(rate = c(5, 15, 30), duration = c(100,200,100)))
  #' plot(x,1:10000,
  #'      main="Piecewise uniform enrollment simulation",xlab="Time",
  #'      ylab="Enrollment")
  #' # exponential enrollment
  #' x <- rpwenroll(10000, enrollRates=tibble::tibble(rate = .03, duration = 1))
  #' plot(x,1:10000,main="Simulated exponential inter-arrival times",
  #'      xlab="Time",ylab="Enrollment")
  #'
  #' @export
  rpwenroll <- function(n = NULL,
                        enrollRates = tibble::tibble(duration=c(1,2),rate=c(2,5))
  ){# take care of the simple case first
    if(nrow(enrollRates)==1) {
      # stop with error message if only 1 enrollment period and the enrollment rate is less or equal with 0
      if (enrollRates$rate <=0) stop("Please specify > 0 enrollment rate, otherwise enrollment cannot finish.")
      # otherwise, return inter-arrival exponential times
      else return(cumsum(stats::rexp(n=n,rate=enrollRates$rate)))
    }
    y <- enrollRates %>%
            dplyr::mutate(period=dplyr::row_number(),
                          finish=cumsum(duration),
                          lambda=duration*rate,
                          origin=dplyr::lag(finish,default=0)) %>%
            group_by(period) %>%
            dplyr::mutate(N=stats::rpois(n=1,lambda=lambda))
    # deal with extreme cases where none randomized in fixed intervals
    if (sum(y$N)==0){
      if (is.null(n)) return(NULL)
      # stop with error message if enrollment has not finished but enrollment rate for last period is less or equal with 0
      if (dplyr::last(enrollRates$rate) <=0) stop("Please specify > 0 enrollment rate for the last period; otherwise enrollment cannot finish.")
      # otherwise, return inter-arrival exponential times
      else return(cumsum(stats::rexp(n,rate=dplyr::last(enrollRates$rate)))+dplyr::last(y$finish))
    }
    # generate sorted uniform observations for Poisson count for each interval
    z <- tidyr::expand(y,enrollTime=sort(stats::runif(n=N,min=origin,max=finish)))
    if (is.null(n)) return(z$enrollTime) # if n not specified, return generated times
    if (nrow(z) >= n) return(z$enrollTime[1:n]) # if n already achieved, return first n observations
    # after specified finite intervals, add required additional observations with
    # exponential inter-arrival times
    nadd <- n-nrow(z)
    # stop with error message if enrollment has not finished but enrollment rate for last period is less or equal with 0
    if (dplyr::last(enrollRates$rate) <=0) stop("Please specify > 0 enrollment rate for the last period; otherwise enrollment cannot finish.")
    # Otherwise, return inter-arrival exponential times
    else return(c(z$enrollTime, cumsum(stats::rexp(nadd,rate=dplyr::last(enrollRates$rate)))+dplyr::last(y$finish)))
  }

Package: simtrial
File: R/rpwexp.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  NULL
  
  #' The Piecewise Exponential Distribution
  #'
  #' The piecewise exponential distribution allows a simple method to specify a distribtuion
  #' where the hazard rate changes over time. It is likely to be useful for conditions where
  #' failure rates change, but also for simulations where there may be a delayed treatment
  #' effect or a treatment effect that that is otherwise changing (e.g., decreasing) over time.
  #' \code{rpwexp()} is to support simulation of both the Lachin and Foulkes (1986) sample size
  #' method for (fixed trial duration) as well as the Kim and Tsiatis(1990) method
  #' (fixed enrollment rates and either fixed enrollment duration or fixed minimum follow-up);
  #' see \code{\link[gsDesign:nSurv]{gsDesign}}.
  #'
  #' Using the \code{cumulative=TRUE} option, enrollment times that piecewise constant over
  #' time can be generated.
  #'
  #' @param n Number of observations to be generated.
  #' @param failRates A tibble containing \code{duration} and \code{rate} variables.
  #' \code{rate} specifies failure rates during the corresponding interval duration
  #' specified in \code{duration}. The final interval is extended to be infinite
  #' to ensure all observations are generated.
  #' @examples
  #' # get 10k piecewise exponential failure times
  #' # failure rates are 1 for time 0-.5, 3 for time .5 - 1 and 10 for >1.
  #' # intervals specifies duration of each failure rate interval
  #' # with the final interval running to infinity
  #' x <- rpwexp(10000, failRates=tibble::tibble(rate = c(1, 3, 10), duration = c(.5,.5,1)))
  #' plot(sort(x),(10000:1)/10001,log="y", main="PW Exponential simulated survival curve",
  #' xlab="Time",ylab="P{Survival}")
  #' # exponential failure times
  #' x <- rpwexp(10000, failRates=tibble::tibble(rate = 5, duration=1))
  #'
  #' plot(sort(x),(10000:1)/10001,log="y", main="Exponential simulated survival curve",
  #'      xlab="Time",ylab="P{Survival}")
  #'
  #' @export
  rpwexp <- function(n=100,
                     failRates=tibble(duration=c(1,1),rate=c(10,20))
                    ){
    n_rates <- nrow(failRates)
    if (n_rates == 1){
      # set failure time to Inf if 0 failure rate
      if(failRates$rate == 0) times = rep(Inf, n)
      # generate exponential failure time if non-0 failure rate
      else times = stats::rexp(n,failRates$rate)
    }else{
      starttime <- 0 # start of first failure rate interval
      finish <- cumsum(failRates$duration) # ends of failure rate interval
      times <- rep(0,n) # initiate vector for failure times
      indx <- rep(TRUE,n) # index for event times not yet reached
      for(i in 1:n_rates){
        nindx <- sum(indx) # number of event times left to generate
        if (nindx==0) break # stop if finished
        # set failure time to Inf for inveral i if 0 fail rate
        if (failRates$rate[i] == 0) times[indx] = starttime + rep(Inf, nindx)
        # generate exponential failure time for interval i if non-0 faiurel rate
        else times[indx] <- starttime + stats::rexp(nindx,failRates$rate[i])
        if (i < n_rates){ # skip this for last interval as all remaining times are generated there
          starttime <- finish[i]
          indx <- (times > finish[i]) # update index of event times not yet reached
        }
      }
    }
    times
  }

Package: simtrial
File: R/simfix.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import dplyr
  #' @import tibble
  #' @import survival
  NULL
  
  #' Simulation of fixed sample size design for time-to-event endpoint
  #'
  #' `simfix()` provide simulations of a single endpoint two-arm trial
  #' where the enrollment, hazard ratio, and failure and dropout rates change over time.
  #' @param nsim Number of simulations to perform.
  #' @param sampleSize Total sample size per simulation.
  #' @param targetEvents Targeted event count for analysis.
  #' @param strata A tibble with strata specified in `Stratum`, probability (incidence) of each stratum in `p`.
  #' @param enrollRates Piecewise constant enrollment rates by time period.
  #' Note that these are overall population enrollment rates and the `strata` argument controls the
  #' random distribution between strata.
  #' @param failRates Piecewise constant control group failure rates, hazard ratio for experimental vs control,
  #'  and dropout rates by stratum and time period.
  #' @param totalDuration Total follow-up from start of enrollment to data cutoff.
  #' @param block As in `simtrial::simPWSurv()`. Vector of treatments to be included in each block.
  #' @param timingType A numeric vector determining data cutoffs used; see details.
  #' Default is to include all available cutoff methods.
  #' @param rg As in `simtrial::tenFHCorr()`.
  #' A \code{tibble} with variables \code{rho} and \code{gamma}, both greater than equal
  #' to zero, to specify one Fleming-Harrington weighted logrank test per row.
  #'
  #' @details \code{timingType} has up to 5 elements indicating different options for data cutoff.
  #' 1 uses the planned study duration, 2 the time the targeted event count is achieved,
  #' 3 the planned minimum follow-up after enrollment is complete,
  #' 4 the maximum of planned study duration and targeted event count cuts (1 and 2),
  #' 5 the maximum of targeted event count and minimum follow-up cuts (2 and 3).
  #' @return A \code{tibble} including columns \code{Events} (event count), \code{lnhr} (log-hazard ratio),
  #' \code{Z} (normal test statistic; < 0 favors experimental) cut (text describing cutoff used),
  #' \code{Duration} (duration of trial at cutoff for analysis) and \code{sim} (sequential simulation id).
  #' One row per simulated dataset per cutoff specified in \code{timingType}, per test statistic specified.
  #' If multiple Fleming-Harrington tests are specified in \code{rg}, then columns {rho,gamma}
  #' are also included.
  #' @examples
  #' library(tidyr)
  #' library(dplyr)
  #' # Show output structure
  #' simfix(nsim=3)
  #' # Example with 2 tests: logrank and FH(0,1)
  #' simfix(nsim=1,rg=tibble::tibble(rho=0,gamma=c(0,1)))
  #' # Power by test
  #' # Only use cuts for events, events + min follow-up
  #' xx <- simfix(nsim=100,timingType=c(2,5),rg=tibble::tibble(rho=0,gamma=c(0,1)))
  #' # Get power approximation for FH, data cutoff combination
  #' xx %>% group_by(cut,rho,gamma) %>% summarise(mean(Z<=qnorm(.025)))
  #' # MaxCombo power estimate for cutoff at max of targeted events, minimum follow-up
  #' p <- xx %>%  filter(cut != "Targeted events") %>% group_by(Sim) %>% group_map(pMaxCombo)
  #' p <- unlist(p)
  #' mean(p<.025)
  #' # MaxCombo estimate for targeted events cutoff
  #' p <- unlist(xx %>%  filter(cut == "Targeted events") %>% group_by(Sim) %>% group_map(pMaxCombo))
  #' mean(p<.025)
  #' @export
  simfix <- function(nsim=1000,
                     sampleSize=500, # sample size
                     targetEvents=350,  # targeted total event count
                     # multinomial probability distribution for strata enrollment
                     strata = tibble::tibble(Stratum = "All", p = 1),
                     # enrollment rates as in AHR()
                     enrollRates=tibble::tibble(duration=c(2,2,10),
                                                rate=c(3,6,9)),
                     # failure rates as in AHR()
                     failRates=tibble::tibble(Stratum="All",
                                              duration=c(3,100),
                                              failRate=log(2)/c(9,18),
                                              hr=c(.9,.6),
                                              dropoutRate=rep(.001,2)),
                     totalDuration=30, # total planned trial duration; single value
                     block=rep(c("Experimental","Control"),2), # Fixed block randomization specification
                     timingType=1:5, # select desired cutoffs for analysis (default is all types)
                     # default is to to logrank testing, but one or more Fleming-Harrington tests
                     # can be specified
                     rg=tibble::tibble(rho=0,gamma=0)
  ){# check input values
    # check input enrollment rate assumptions
    if(max(names(enrollRates)=="duration") != 1){stop("enrollRates column names in `simfix()` must contain duration")}
    if(max(names(enrollRates)=="rate") != 1){stop("enrollRates column names in `simfix()` must contain  rate")}
  
    # check input failure rate assumptions
    if(max(names(failRates)=="Stratum") != 1){stop("failRates column names in `simfix()` must contain Stratum")}
    if(max(names(failRates)=="duration") != 1){stop("failRates column names in `simfix()` must contain duration")}
    if(max(names(failRates)=="failRate") != 1){stop("failRates column names in `simfix()` must contain failRate")}
    if(max(names(failRates)=="hr") != 1){stop("failRates column names in `simfix()` must contain hr")}
    if(max(names(failRates)=="dropoutRate") != 1){stop("failRates column names in `simfix()` must contain dropoutRate")}
    
    # check input trial durations
    if(!is.numeric(totalDuration)){stop("totalDuration in `simfix()` must be a single positive number")}
    if(!is.vector(totalDuration)){stop("totalDuration in `simfix()` must be a single positive number")}
    if(length(totalDuration) != 1){stop("totalDuration in `simfix()` must be a single positive number")}
    if(!min(totalDuration) > 0){stop("totalDuration in `simfix()` must be a single positive number")}
  
    strata2 <- names(table(failRates$Stratum))
    if(nrow(strata)!= length(strata2)){stop("Stratum in `simfix()` must be the same in strata and failRates")}
    for(s in strata$Stratum){ 
      if(max(strata2==s) != 1){stop("Stratum in `simfix()` must be the same in strata and failRates")}
    }
  
    if(!nsim > 0){stop("nsim in `simfix()` must be positive integer")}
    if(length(nsim) != 1){stop("nsim in `simfix()` must be positive integer")}
    if(nsim != ceiling(nsim)){stop("nsim in `simfix()` must be positive integer")}
  
    if(!targetEvents > 0){stop("targetEvents in `simfix()` must be positive")}
    if(length(targetEvents) != 1){stop(("targetEvents in `simfix()` must be positive"))}
    
    if(!sampleSize > 0){stop("sampleSize in `simfix()` must be positive")}
    if(length(sampleSize) != 1){stop("sampleSize in `simfix()` must be positive")}
    
    nstrata <- nrow(strata)
    doAnalysis <- function(d,rg,nstrata){
      if (nrow(rg)==1){Z = tibble::tibble(Z=(d %>%
                                             simtrial::tensurv(txval="Experimental") %>%
                                             simtrial::tenFH(rg=rg)
      )$Z
      )
      }else Z = d %>%
                simtrial::tensurv(txval="Experimental") %>%
                simtrial::tenFHcorr(rg=rg,corr=TRUE)
      r <- cbind(tibble::tibble(Events = sum(d$event),
                                lnhr = as.numeric(ifelse(nstrata>1,
                                       survival::coxph(survival::Surv(tte,event)~
                                                         (Treatment=="Experimental")+
                                                         survival::strata(Stratum),data=d)$coefficients,
                                       survival::coxph(survival::Surv(tte,event)~
                                                         (Treatment=="Experimental"),data=d)$coefficients))
                               ),
                  Z
      )
      r
    }
    # compute minimum planned follow-up time
    minFollow <- max(0,totalDuration - sum(enrollRates$duration))
    # put failure rates into simPWSurv format
    xx <- simfix2simPWSurv(failRates)
    fr <- xx$failRates
    dr <- xx$dropoutRates
    results <- NULL
    for(i in 1:nsim){
      sim <- simtrial::simPWSurv(n = sampleSize,
                                 strata = strata,
                                 enrollRates = enrollRates,
                                 failRates = fr,
                                 dropoutRates = dr,
                                 block = block)
      # study date that targeted event rate achieved
      tedate <- sim %>% getCutDateForCount(targetEvents)
      # study data that targeted minimum follow-up achieved
      tmfdate <- max(sim$enrollTime) + minFollow
      # Compute tests for all specified cutoff options
      r1 <- NULL ; r2 <- NULL; r3 <- NULL;
      tests <- rep(FALSE,3)
      ## Total planned trial duration or max of this and targeted events
      if (max(1 == timingType)) tests[1] <- TRUE # planned duration cutoff
      if (max(2 == timingType)) tests[2] <- TRUE # targeted events cutoff
      if (max(3 == timingType)) tests[3] <- TRUE # minimum follow-up duration target
      if (max(4 == timingType)){ # max of planned duration, targeted events
        if (tedate > totalDuration){
          tests[2] <- TRUE
        }else tests[1] <- TRUE
      }
      if (max(5 == timingType)){ # max of minimum follow-up, targeted events
        if (tedate > tmfdate){
          tests[2] <- TRUE
        }else tests[3] <- TRUE
      }
      if (tests[1]){ # Total duration cutoff
        d <- sim %>% cutData(totalDuration)
        r1 <- d %>% doAnalysis(rg,nstrata)
      }
      if (tests[2]){ # targeted events cutoff
        d <- sim %>% cutData(tedate)
        r2 <- d %>% doAnalysis(rg,nstrata)
      }
      if (tests[3]){ # minimum follow-up cutoff
        d <- sim %>% cutData(tmfdate)
        r3 <- d %>% doAnalysis(rg,nstrata)
      }
      addit <- NULL
      # planned duration cutoff
      if (max(1 == timingType)) addit <- rbind(addit,
                                               r1 %>% mutate(cut="Planned duration",Duration=totalDuration))
      # targeted events cutoff
      if (max(2 == timingType)) addit <- rbind(addit, r2 %>% mutate(cut="Targeted events",Duration=tedate))
      # minimum follow-up duration target
      if (max(3 == timingType)) addit <- rbind(addit, r3 %>% mutate(cut="Minimum follow-up",Duration=tmfdate))
      if (max(4 == timingType)){ # max of planned duration, targeted events
        if (tedate > totalDuration){
          addit <- rbind(addit, r2 %>% mutate(cut="Max(planned duration, event cut)",Duration=tedate))
        }else addit <- rbind(addit, r1 %>% mutate(cut="Max(planned duration, event cut)",Duration=totalDuration))
      }
      if (max(5 == timingType)){ # max of minimum follow-up, targeted events
        if (tedate > tmfdate){
          addit <- rbind(addit, r2 %>% mutate(cut="Max(min follow-up, event cut)",Duration=tedate))
        }else addit <- rbind(addit, r3 %>% mutate(cut="Max(min follow-up, event cut)",Duration=tmfdate))
      }
      results <- rbind(results, addit %>% mutate(Sim=i))
    }
    results
  }

Package: simtrial
File: R/simfix2simPWSurv.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import dplyr
  #' @import tibble
  NULL
  
  #' Conversion of enrollment and failure rates from simfix() to simPWSurv() format
  #'
  #' `simfix2simPWSurv()` converts failure rates and dropout rates entered in the simpler
  #' format for `simfix()` to that used for `simtrial::simPWSurv()`.
  #' The `failRates` argument for `simfix()` requires enrollment rates, failure rates
  #' hazard ratios and dropout rates by strata for a 2-arm trial, `simtrial::simPWSurv()`
  #' is in a more flexible but less obvious but more flexible format.
  #' Since `simfix()` automatically analyzes data and `simtrial::simPWSurv()` just produces
  #' a simulation dataset, the latter provides additional options to analyze or otherwise evaluate
  #' individual simulations in ways that `simfix()` does not.
  #' @param failRates Piecewise constant control group failure rates, hazard ratio for experimental vs control,
  #'  and dropout rates by stratum and time period.
  #' @return A \code{list} of two `tibble` components formatted for `simtrial::simPWSurv()`:
  #' `failRates` and `dropoutRates`.
  #' @examples
  #' library(tidyr)
  #' library(dplyr)
  #' # Convert standard input
  #' simfix2simPWSurv()
  #' # Stratified example
  #' failRates <- tibble::tibble(Stratum=c(rep("Low",3),rep("High",3)),
  #'                             duration=rep(c(4,10,100),2),
  #'                             failRate=c(.04,.1,.06,
  #'                                        .08,.16,.12),
  #'                             hr=c(1.5,.5,2/3,
  #'                                  2, 10/16, 10/12),
  #'                             dropoutRate=.01
  #' )
  #' x <- simfix2simPWSurv(failRates)
  #' # Do a single simulation with the above rates
  #' # Enroll 300 patients in ~12 months at constant rate
  #' sim <-
  #'     simPWSurv(n=300,
  #'           strata=tibble::tibble(Stratum=c("Low","High"),p=c(.6,.4)),
  #'           enrollRates=tibble::tibble(duration=12,rate=300/12),
  #'           failRates=x$failRates,
  #'           dropoutRates=x$dropoutRates)
  #' # Cut after 200 events and do a stratified logrank test
  #' dat <- sim %>%
  #'        cutDataAtCount(200) %>%            # cut data
  #'        tensurv(txval="Experimental") %>%  # convert format for tenFH
  #'        tenFH(rg=tibble(rho=0,gamma=0))    # stratified logrank
  #' @export
  simfix2simPWSurv <-
    function(# failure rates as in simfix()
    failRates=tibble::tibble(Stratum="All",
                             duration=c(3,100),
                             failRate=log(2)/c(9,18),
                             hr=c(.9,.6),
                             dropoutRate=rep(.001,2))
  ){# put failure rates into simPWSurv format
    fr <- rbind(failRates %>% group_by(Stratum) %>% mutate(Treatment="Control",rate=failRate,period=1:n()) %>%
                  ungroup(),
                failRates %>% group_by(Stratum) %>% mutate(Treatment="Experimental",rate=failRate*hr,period=1:n()) %>%
                  ungroup()
    ) %>% select("Stratum","period","Treatment","duration","rate")
    # put dropout rates into simPWSurv format
    dr <-  failRates %>%  group_by(Stratum) %>% mutate(Treatment="Control",rate=dropoutRate,period=1:n()) %>%
      select("Stratum","period","Treatment","duration","rate") %>%
      ungroup()
    dr <- rbind(dr, dr %>% mutate(Treatment="Experimental"))
    return(list(failRates=fr,dropoutRates=dr))
  }

Package: simtrial
File: R/simPWSurv.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import dplyr
  #' @import tibble
  NULL
  
  #' Simulate a stratified time-to-event outcome randomized trial
  #'
  #' \code{simPWSurv} enables simulation of a clinical trial with essentially arbitrary
  #' patterns of enrollment, failure rates and censoring.
  #' The piecewise exponential distribution allows a simple method to specify a distribtuion
  #' and enrollment pattern
  #' where the enrollment, failure and dropout rate changes over time.
  #' While the main purpose may be to generate a trial that can be analyzed at a single point in time or
  #' using group sequential methods, the routine can also be used to simulate an adaptive trial design.
  #' Enrollment, failure and dropout rates are specified by treatment group, stratum and time period.
  #' Fixed block randomization is used; blocks must include treatments provided in failure and dropout
  #' specification.
  #' Default arguments are set up to allow very simple implementation of a non-proportional hazards assumption
  #' for an unstratified design.
  #'
  #' @param n Number of observations.
  #' If length(n) > 1, the length is taken to be the number required.
  #' @param strata A tibble with strata specified in `Stratum`, probability (incidence) of each stratum
  #' in `p`
  #' @param block Vector of treatments to be included in each  block
  #' @param enrollRates Enrollment rates; see details and examples
  #' @param failRates Failure rates; see details and examples; note that treatments need
  #' to be the same as input in block
  #' @param dropoutRates Dropout rates; see details and examples; note that treatments need
  #' to be the same as input in block
  #'
  #' @return a \code{tibble} with the following variables for each observation
  #' \code{Stratum},
  #' \code{enrollTime} (enrollment time for the observation),
  #' \code{Treatment} (treatment group; this will be one of the values in the input \code{block}),
  #' \code{failTime} (failure time generated using \code{rpwexp()}),
  #' \code{dropoutTime} (dropout time generated using \code{rpwexp()}),
  #' \code{cte} (calendar time of enrollment plot the minimum of failure time and dropout time),
  #' \code{fail} (indicator that \code{cte} was set using failure time; i.e., 1 is a failure, 0 is a dropout).
  #' @examples
  #' library(dplyr)
  #' # Tests
  #'  simPWSurv(n=20)
  #'  # 3:1 randomization
  #'  simPWSurv(n=20,block=c(rep("Experimental",3),"Control"))
  #'
  #' # Simulate 2 strata; will use defaults for blocking and enrollRates
  #' simPWSurv(n=20,
  #'           # 2 strata,30% and 70% prevalence
  #'           strata=tibble::tibble(Stratum=c("Low","High"),p=c(.3,.7)),
  #'           failRates=tibble::tibble(Stratum=c(rep("Low",4),rep("High",4)),
  #'                                    period=rep(1:2,4),
  #'                                    Treatment=rep(c(rep("Control",2),rep("Experimental",2)),2),
  #'                                    duration=rep(c(3,1),4),
  #'                                    rate=c(.03,.05,.03,.03,.05,.08,.07,.04)),
  #'           dropoutRates=tibble::tibble(Stratum=c(rep("Low",2),rep("High",2)),
  #'                                       period=rep(1,4),
  #'                                       Treatment=rep(c("Control","Experimental"),2),
  #'                                       duration=rep(1,4),
  #'                                       rate=rep(.001,4))
  #')
  #'
  #'# If you want a more rectangular entry for a tibble
  #'failRates <- bind_rows(
  #'    tibble(Stratum="Low" ,period=1,Treatment="Control"     ,duration=3,rate=.03),
  #'    tibble(Stratum="Low" ,period=1,Treatment="Experimental",duration=3,rate=.03),
  #'    tibble(Stratum="Low" ,period=2,Treatment="Experimental",duration=3,rate=.02),
  #'    tibble(Stratum="High",period=1,Treatment="Control"     ,duration=3,rate=.05),
  #'    tibble(Stratum="High",period=1,Treatment="Experimental",duration=3,rate=.06),
  #'    tibble(Stratum="High",period=2,Treatment="Experimental",duration=3,rate=.03)
  #')
  #'dropoutRates <- bind_rows(
  #'    tibble(Stratum="Low" ,period=1,Treatment="Control"     ,duration=3,rate=.001),
  #'    tibble(Stratum="Low" ,period=1,Treatment="Experimental",duration=3,rate=.001),
  #'    tibble(Stratum="High",period=1,Treatment="Control"     ,duration=3,rate=.001),
  #'    tibble(Stratum="High",period=1,Treatment="Experimental",duration=3,rate=.001)
  #')
  #'simPWSurv(n=12,strata=tibble(Stratum=c("Low","High"),p=c(.3,.7)),
  #'          failRates=failRates,dropoutRates=dropoutRates)
  #' @export
  simPWSurv <- function(n=100,
                        strata=tibble::tibble(Stratum="All",p=1),
                        block=c(rep("Control",2),rep("Experimental",2)),
                        enrollRates=tibble::tibble(rate=9,
                                                   duration=1),
                        failRates=tibble::tibble(Stratum=rep("All",4),
                                                 period=rep(1:2,2),
                                                 Treatment=c(rep("Control",2), rep("Experimental",2)),
                                                 duration=rep(c(3,1),2),
                                                 rate=log(2)/c(9,9,9,18)),
                        dropoutRates=tibble::tibble(Stratum=rep("All",2),
                                                period=rep(1,2),
                                                Treatment=c("Control","Experimental"),
                                                duration=rep(100,2),
                                                rate=rep(.001,2))
                        ){
  # start tibble by generating strata and enrollment times
      #return(
      x<-  tibble::tibble(Stratum=sample(x=strata$Stratum,size=n,replace=TRUE,prob=strata$p)) %>%
           mutate(enrollTime=rpwenroll(n, enrollRates)) %>%
           group_by(Stratum) %>% mutate(Treatment=fixedBlockRand(n=n(),block=block))  %>% # assign treatment
           # generate time to failure and time to dropout
           dplyr::group_by(Stratum,Treatment)
      utr <- unique(x$Treatment)
      usr <- unique(x$Stratum)
      x$failTime <- 0
      x$dropoutTime <- 0
      for(sr in usr){for(tr in utr){
        indx <- x$Stratum==sr & x$Treatment==tr
        x$failTime[indx] <- rpwexp(n=sum(indx),failRates=filter(failRates,Stratum==sr&Treatment==tr))
        x$dropoutTime[indx] <- rpwexp(n=sum(indx),failRates=filter(dropoutRates,Stratum==sr&Treatment==tr))
      }}
      # set calendar time-to-event and failure indicator
      return(x %>% mutate(cte=pmin(dropoutTime,failTime)+enrollTime,
                          fail=(failTime <= dropoutTime)*1))
  }

Package: simtrial
File: R/simtrial-package.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp., a subsidiary of
  #  Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the pkglite program.
  #
  #  pkglite is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @keywords internal
  "_PACKAGE"

Package: simtrial
File: R/tenFH.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  NULL
  
  #' Fleming-Harrington Weighted Logrank Tests
  #'
  #' With output from the function \code{tensurv}
  #' @param x a \code{tensurv}-class \code{tibble} with a counting process dataset
  #' @param rg a \code{tibble} with variables \code{rho} and \code{gamma}, both greater than equal
  #' to zero, to specify one Fleming-Harrington weighted logrank test per row;
  #' Default: tibble(rho = c(0, 0, 1, 1), gamma = c(0, 1, 0, 1))
  #' @param returnVariance a logical flag that, if true, adds columns
  #' estimated variance for weighted sum of observed minus expected; see details; Default: FALSE
  #' @return a `tibble` with \code{rg} as input and the FH test statistic
  #' for the data in \code{x}
  #' (\code{Z}, a directional square root of the usual weighted logrank test);
  #' if variance calculations are specified (e.g., to be used for covariances in a combination test),
  #' the this will be returned in the column \code{Var}
  #' @details 
  #' The input value \code{x} produced by \code{tensurv()} produces a counting process dataset 
  #' grouped by strata and sorted within strata by increasing times where events occur.
  #' \itemize{
  #' \item \eqn{Z} - standardized normal Fleming-Harrington weighted logrank test
  #' \item \eqn{i}  - stratum index
  #' \item \eqn{d_i} - number of distinct times at which events occurred in stratum \eqn{i} 
  #' \item \eqn{t_{ij}} - ordered times at which events in stratum \eqn{i}, \eqn{j=1,2,\ldots d_i} were observed; 
  #' for each observation, \eqn{t_{ij}} represents the time post study entry
  #' \item {O_{ij.}} - total number of events in stratum \eqn{i} that occurred at time \eqn{t_{ij}}
  #' \item {O_{ije}} - total number of events in stratum \eqn{i} in the experimental treatment group that occurred
  #' at time \eqn{t_{ij}}
  #' \item {N_{ij.}} - total number of study subjects in stratum \eqn{i} who were followed for at least duration
  #' \item {E_{ije}} - expected observations in experimental treatment group given random selection of \eqn{O_{ij.}} 
  #' from those in stratum \eqn{i} at risk at time \eqn{t_{ij}}
  #' \item {V_{ije}} - hypergeometric variance for \eqn{E_{ije}} as produced in \code{Var} 
  #' from the \code{tensurv()} routine  
  #' \item {N_{ije}} - total number of study subjects in stratum \eqn{i} in the experimental treatment group
  #' who were followed for at least duration \eqn{t_{ij}}
  #' \item {E_{ije}} - expected observations in experimental group in stratum \eqn{i} at time \eqn{t_{ij}} 
  #' conditioning on the overall number of events and at risk populations at that time and sampling at risk 
  #' observations without replacement:
  #' \deqn{E_{ije} = O_{ij.} N_{ije}/N_{ij.}}
  #' \item \eqn{S_{ij}} - Kaplan-Meier estimate of survival in combined treatment groups immediately prior 
  #' to time \eqn{t_{ij}}
  #' \item \eqn{\rho, \gamma} - real parameters for Fleming-Harrington test
  #' \item \eqn{X_i} - Numerator for signed logrank test in stratum \eqn{i}
  #' \deqn{X_i = \sum_{j=1}^d_i S_{ij}^\rho(1-S_{ij}^\gamma)(O_{ije}-E_{ije})}
  #' \item {V_{ij}} - variance used in denominator for Fleming-Harrington weighted logrank tests
  #' \deqn{V_i = \sum_{j=1}^d_i (S_{ij}^\rho(1-S_{ij}^\gamma))^2V_{ij})}
  #' 
  #' The stratified Fleming-Harrington weighted logrank test is then computed as:
  #' \deqn{Z = \sum_i X_i/\sqrt{\sum_i V_i}}
  #' }
  #' @examples
  #' library(tidyr)
  #' # Use default enrollment and event rates at cut at 100 events
  #' x <- simPWSurv(n=200) %>% cutDataAtCount(100) %>% tensurv(txval="Experimental")
  #' # compute logrank (FH(0,0)) and FH(0,1)
  #' tenFH(x,rg=tibble(rho=c(0,0),gamma=c(0,1)))
  #' @export
  
  tenFH <- function(x=simPWSurv(n=200) %>% cutDataAtCount(150) %>% tensurv(txval = "Experimental"),
                    rg=tibble(rho=c(0,0,1,1),gamma=c(0,1,0,1)),
                    returnVariance=FALSE){
    # check input failure rate assumptions
    if(!is.data.frame(x)){stop("x in `tenFH()` must be a data frame")}
    if(max(names(x)=="S") != 1){stop("x column names in `tenFH()` must contain S")}
    if(max(names(x)=="OminusE") != 1){stop("x column names in `tenFH()` must contain OminusE ")}
    if(max(names(x)=="Var") != 1){stop("x column names in `tenFH()` must contain Var")}
  
    # get minimal columns from tensurv item
    xx <- x %>%
          ungroup() %>%
          select(S,OminusE,Var)
    rg$Z <- rep(0,nrow(rg))
    if (returnVariance) rg$Var <- rep(0,nrow(rg))
    for(i in 1:nrow(rg)){
      y <- xx %>% mutate(w=S^rg$rho[i]*(1-S)^rg$gamma[i],
                         wOminusE=w*OminusE,
                         wVar=w^2*Var) %>%
                  summarize(wVar=sum(wVar),wOminusE=sum(wOminusE))
      rg$Z[i] <- y$wOminusE/sqrt(y$wVar)
      if (returnVariance) rg$Var[i] <- y$wVar
    }
    rg
  }
  #' @rdname tenFH
  #' @export

Package: simtrial
File: R/tenFHCorr.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  NULL
  
  #' @title Fleming-Harrington Weighted Logrank Tests plus Correlations
  #'
  #' @description
  #' Correlations can be used with \code{mvtnorm::pmvnorm} to compute
  #' p-value for MaxCombo, the maximum of the specifed
  #' Fleming-Harrington tests
  #'
  #' @param x a \code{tensurv}-class \code{tibble} with a counting process dataset
  #' @param rg a \code{tibble} with variables \code{rho} and \code{gamma}, both greater than equal
  #' to zero, to specify one Fleming-Harrington weighted logrank test per row
  #' @param corr a logical; if TRUE (default), return correlation matrix; otherwise, return covariance matrix
  #' @return a `tibble` with \code{rg} as input, the FH test statistics specified
  #' for the data in \code{Z}, and the correlation or covariance matrix for these tests in variables starting
  #' with \code{V}
  #' @examples
  #' library(tidyr)
  #' library(dplyr)
  #' # Use default enrollment and event rates at cut of 100 events
  #' x <- simPWSurv(n=200) %>% cutDataAtCount(100) %>% tensurv(txval="Experimental")
  #' # compute logrank (FH(0,0)) and FH(0,1)
  #' x <- tenFHcorr(rg=tibble(rho=c(0,0),gamma=c(0,1)),x=x)
  #' # compute p-value for MaxCombo
  #' library(mvtnorm)
  #' 1-pmvnorm(lower=rep(min(x$Z),nrow(x)),corr=data.matrix(select(x,-c(rho,gamma,Z))),
  #' algorithm=GenzBretz(maxpts=50000,abseps=0.00001))[1]
  #' # check that covariance is as expected
  #' x <- simPWSurv(n=200) %>%
  #'          cutDataAtCount(100) %>%
  #'          tensurv(txval="Experimental")
  #' x %>% tenFHcorr(rg=tibble(rho=c(0,0),gamma=c(0,1)),corr=FALSE)
  #' # Off-diagonal element should be variance in following
  #' x %>% tenFHcorr(rg=tibble(rho=0,gamma=.5),corr=FALSE)
  #' # compare off diagonal result with tenFH()
  #' x %>% tenFH(rg=tibble(rho=0,gamma=.5))
  #' @export
  
  tenFHcorr <- function(x=simPWSurv(n=200) %>% cutDataAtCount(100) %>%
                          tensurv(txval = "Experimental"),
                        rg=tibble(rho=c(0,0,1,1),gamma=c(0,1,0,1)),
                        corr=TRUE
  ){
    # Get average rho and gamma for FH covariance matrix
    # We want rhoave[i,j] = (rho[i]+rho[j])/2
    # and     gamave[i,j] = (gamma[i]+gamma[j])/2
    nr <- nrow(rg)
    rhoave <- (matrix(rg$rho,nrow=nr,ncol=nr)+matrix(rg$rho,nrow=nr,ncol=nr,byrow=TRUE))/2
    gamave <- (matrix(rg$gamma,nrow=nr,ncol=nr)+matrix(rg$gamma,nrow=nr,ncol=nr,byrow=TRUE))/2
    # Convert back to tibble
    rg2 <- tibble(rho=as.numeric(rhoave), gamma=as.numeric(gamave))
    # get unique values of rho, gamma
    rgu <- rg2 %>% unique()
    # compute FH statistic for unique values
    # and merge back to full set of pairs
    # rgFH <- tenFH(x,rgu,returnVariance=TRUE) %>% right_join(rg2,by=c("rho"="rho","gamma"="gamma"))
    # FIXED by KA TO GET SORT CORRECT 8/21/2020
    rgFH <- rg2 %>% left_join(tenFH(x,rgu,returnVariance=TRUE),by=c("rho"="rho","gamma"="gamma"))
    # get Z statistics for input rho, gamma combinations
    Z <- rgFH$Z[(0:(nrow(rg)-1))*nrow(rg)+1:nrow(rg)]
    # get correlation matrix
    c <- matrix(rgFH$Var,nrow=nrow(rg),byrow=TRUE)
    if (corr) c <- stats::cov2cor(c)
    names(c) <- paste("V",1:ncol(c),sep="")
    # return combined values
    cbind(rg,Z,as_tibble(c))
  }
  #' @rdname tenFHcorr
  #' @export

Package: simtrial
File: R/tensurv.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import dplyr
  #' @import tibble
  NULL
  
  #' Process Survival Data into Counting Process Format
  #'
  #' Produces a tibble that is sorted by stratum and time.
  #' Included in this is only the times at which one or more event occurs.
  #' The output dataset contains Stratum, tte (time-to-event), at risk count and count of events at the specified tte
  #' sorted by Stratum and tte.
  #'
  #' The function only considered two group situation.
  #'
  #' The tie is handled by the Breslow's Method.
  #'
  #' @param x a tibble with no missing values and contain variables
  #'
  #'     \code{Stratum} (Stratum)
  #'
  #'     \code{Treatment} (Treatment group)
  #'
  #'     \code{tte} (Observed time)
  #'
  #'     \code{event} (Binary event indicator, 1 represents event, 0 represents censoring)
  #'
  #' @param txval value in the input \code{Treatment} column that indicates treatment group value.
  #'
  #'
  #' @return A \code{tibble} grouped by
  #' \code{Stratum} and sorted within strata by \code{tte}.
  #' Remain rows with at least one event in the population, at least one subject is at risk in both treatment group and control group.
  #' Other variables in this represent the following within each stratum at each time at which one or more
  #' events are observed:
  #'
  #' \code{events} (Total number of events)
  #'
  #' \code{txevents} (Total number of events at treatment group)
  #'
  #' \code{atrisk} (Number of subjects at risk)
  #'
  #' \code{txatrisk} (Number of subjects at risk in treatment group)
  #'
  #' \code{S} (Left-continuous Kaplan-Meier survival estimate)
  #'
  #' \code{OminusE} (In treatment group, observed number of events minus expected number of events.
  #'           The expected number of events is estimated by assuming no treatment effect with hypergeometric distribution with
  #'           parameters total number of events, total number of events at treatment group and number of events at a time.
  #'           (Same assumption of log-rank test under the null hypothesis)
  #'
  #' \code{Var} (variance of OminusE under the same assumption).
  #'
  #' @examples
  #' library(dplyr)
  #'
  #' # Example 1
  #' x=tibble(Stratum = c(rep(1,10),rep(2,6)),
  #' Treatment = rep(c(1,1,0,0),4),
  #' tte = 1:16,
  #' event= rep(c(0,1),8))
  #'
  #' tensurv(x, txval=1)
  #'
  #' # Example 2
  #' x <- simPWSurv(n=400)
  #' y <- cutDataAtCount(x,150) %>% tensurv(txval = "Experimental")
  #' # weighted logrank test (Z-value and 1-sided p-value)
  #' z <- sum(y$OminusE)/sqrt(sum(y$Var))
  #' c(z,pnorm(z))
  #'
  #' @export
  tensurv <- function(x, txval){
  
      u.trt = unique(x$Treatment)
      if(length(u.trt) > 2){
        stop("Expected two groups")
      }
  
      if(! txval %in% u.trt){
        stop("txval is not a valid treatment group value")
      }
  
      if(! all(unique(x$event) %in% c(0,1) ) ){
        stop("Event indicator must be 0 (censoring) or 1 (event)")
      }
      x %>% group_by(Stratum) %>% arrange(desc(tte)) %>%
            mutate(one=1,
                   atrisk=cumsum(one),
                   txatrisk=cumsum(Treatment==txval)) %>%
            # Handling ties using Breslow's method
            group_by(Stratum, mtte=desc(tte)) %>%
            dplyr::summarise(events=sum(event),
                             txevents=sum((Treatment==txval)*event),
                             tte=first(tte),
                             atrisk=max(atrisk),
                             txatrisk=max(txatrisk)) %>%
            # Keep calculation for observed time with at least one event, at least one subject is
            # at risk in both treatment group and control group.
            filter(events>0, atrisk-txatrisk>0, txatrisk>0) %>%
            select(-mtte) %>%
            mutate(s=1-events/atrisk) %>%
            arrange(Stratum, tte) %>%
            group_by(Stratum) %>%
            mutate(S=lag(cumprod(s), default=1),               # left continuous Kaplan-Meier Estimator
                   OminusE=txevents-txatrisk/atrisk*events,    #  Observed events minus Expected events in treatment group
                   Var=(atrisk-txatrisk)*txatrisk*events*(atrisk-events)/atrisk^2/(atrisk-1)) %>% #Variance of OminusE
                   select(-s)
  }
  

Package: simtrial
File: R/wMB.R
Format: text
Content:
  #  Copyright (c) 2021 Merck Sharp & Dohme Corp. a subsidiary of Merck & Co., Inc., Kenilworth, NJ, USA.
  #
  #  This file is part of the simtrial program.
  #
  #  simtrial is free software: you can redistribute it and/or modify
  #  it under the terms of the GNU General Public License as published by
  #  the Free Software Foundation, either version 3 of the License, or
  #  (at your option) any later version.
  #
  #  This program is distributed in the hope that it will be useful,
  #  but WITHOUT ANY WARRANTY; without even the implied warranty of
  #  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  #  GNU General Public License for more details.
  #
  #  You should have received a copy of the GNU General Public License
  #  along with this program.  If not, see <http://www.gnu.org/licenses/>.
  
  #' @import tibble
  #' @import dplyr
  NULL
  
  #' Magirr and Burman Modestly Weighted Logrank Tests
  #'
  #' Magirr and Burman (2019) have proposed a weighted logrank test to have better power than
  #' the logrank test when the treatment effect is delayed, but to still maintain good power under 
  #' a proportional hazards assumption. 
  #' The weights for some early interval specified by the user are the inverse of the combined treatment group
  #' empirical survival distribution; see details. 
  #' After this initial period, weights are constant at the maximum of the previous weights.
  #' Another advantage of the test is that under strong null hypothesis that the underlying survival in the control group
  #' is greater than or equal to underlying survival in the experimental group, 
  #' Type I error is controlled as the specified level.
  #' 
  #' This function computes Magirr-Burman weights and adds them to a dataset created by the \code{tensurv()} function.
  #' These weights can then be used to compute a Z-statistic for the modestly weighted logrank test proposed.
  #' 
  #' @param x a \code{tensurv}-class \code{tibble} with a counting process dataset
  #' @param delay The initial delay period where weights increase; 
  #' after this, weights are constant at the final weigh in the delay period
  #' @return a vector with weights for the Magirr-Burman weighted logrank test
  #' for the data in \code{x}
  #' @details 
  #' We define \eqn{t^*} to be the input variable \code{delay}.
  #' This specifies an initial period during which weights increase.
  #' To define specific weights, we let \eqn{S(t)} denote the Kaplan-Meier survival estimate at time \eqn{t}
  #' for the combined data (control plus experimental treatment groups).
  #' The weight at time \eqn{t} is then defined as
  #' \deqn{w(t)=S(\min(t,t^*))^{-1}.}
  #' 
  #' @references 
  #' Magirr, Dominic, and CarlâFredrik Burman. 
  #' "Modestly weighted logrank tests." 
  #' \emph{Statistics in Medicine} 38.20 (2019): 3782-3790.
  #' 
  #' @examples
  #' library(tidyr)
  #' library(dplyr)
  #' # Use default enrollment and event rates at cut at 100 events
  #' x <- simPWSurv(n=200) %>% cutDataAtCount(125) %>% tensurv(txval="Experimental")
  #' # compute Magirr-Burman weights with 
  #' ZMB <- x %>% wMB(6) %>% 
  #'              summarize(S=sum(OminusE*wMB),V=sum(Var*wMB^2),Z=S/sqrt(V))
  #' # Compute p-value of modestly weighted logrank of Magirr-Burman
  #' pnorm(ZMB$Z)
  #' @export
  wMB <- function(x, delay = 4) 
  {
    # check input failure rate assumptions
    if(!is.data.frame(x)){stop("x in `wMB()` must be a data frame")}
    
    # check input delay
    if(!is.numeric(delay)){stop("delay in `wMB()` must be a non-negative number")}
    if(!delay >= 0){stop("delay in `wMB()` must be a non-negative number")}
  
    if(max(names(x)=="Stratum") != 1){stop("x column names in `wMB()` must contain Stratum")}
    if(max(names(x)=="tte") != 1){stop("x column names in `wMB()` must contain tte")}
    if(max(names(x)=="S") != 1){stop("x column names in `wMB()` must contain S")}
    
  
    # Compute max weight by stratum
    x2 <- x %>% group_by(Stratum) 
    allstrat <- x2 %>% summarize()                        # Make sure you don't lose any strata!
    maxwgt <- x2 %>% 
              filter(tte <= delay) %>%                    # look only up to delay time
              summarize(maxwgt = max(1/S)) %>%            # weight before delay specified as 1/S
              right_join(allstrat, by = "Stratum") %>%    # get back strata with no records before delay ends
              mutate(maxwgt = replace_na(maxwgt, 1)) %>%  # maxwgt is 1 when there are no records before delay ends
              full_join(x2,by="Stratum") %>%              # Now merge maxwgt back to stratified dataset
              mutate(wMB=pmin(maxwgt,1/S)) %>%            # Weight is min of maxwgt and 1/S which will increase up to delay
              select(-maxwgt)
    return(maxwgt)
  }

Package: simtrial
File: man/cutData.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/cutData.R
  \name{cutData}
  \alias{cutData}
  \title{Cut a Dataset for Analysis at a Specified Date}
  \usage{
  cutData(x, cutDate)
  }
  \arguments{
  \item{x}{a time-to-event dataset, e.g., generated by \code{simPWSurv}}
  
  \item{cutDate}{date relative to start of randomization (\code{cte} from input dataset)
  at which dataset is to be cut off for analysis}
  }
  \value{
  A dataset ready for survival analysis
  }
  \description{
  Cut a Dataset for Analysis at a Specified Date
  }
  \examples{
  # Use default enrollment and event rates and cut at calendar time 5 after start
  # of randomization
  library(dplyr)
  simPWSurv(n=20) \%>\% cutData(5)
  }

Package: simtrial
File: man/cutDataAtCount.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/cutDataAtCount.R
  \name{cutDataAtCount}
  \alias{cutDataAtCount}
  \title{Cut a Dataset for Analysis at a Specified Event Count}
  \usage{
  cutDataAtCount(x, count)
  }
  \arguments{
  \item{x}{a time-to-event dataset, e.g., generated by \code{simPWSurv}}
  
  \item{count}{event count at which data cutoff is to be made}
  }
  \value{
  a \code{tibble} ready for survival analysis, including culumns time to event (`tte`), 
  `event`, the `stratum` and the `treatment.`
  }
  \description{
  `cutDataAtCount` takes a time-to-event data set and cuts the data at which an
  event count is reached.
  }
  \examples{
  library(tidyr)
  # Use default enrollment and event rates at cut at 100 events
  x <- simPWSurv(n=200) \%>\% cutDataAtCount(100)
  table(x$event,x$Treatment)
  
  }

Package: simtrial
File: man/Ex1delayedEffect.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/Ex1delayedEffect.R
  \docType{data}
  \name{Ex1delayedEffect}
  \alias{Ex1delayedEffect}
  \title{Time-to-event data example 1 for non-proportional hazards working group}
  \format{
  Data frame with 4 variables:
  \describe{ 
  \item{id}{sequential numbering of unique identifiers}
  \item{month}{time-to-event}
  \item{event}{1 for event, 0 for censored} 
  \item{trt}{1 for experimental, 0 for control}
  }
  }
  \usage{
  data(Ex1delayedEffect)
  }
  \description{
  Survival objects reverse-engineered datasets from published Kaplan-Meier
  curves. 
  Individual trials are de-identified since the data are only
  approximations of the actual data.
  Data are intended to evaluate methods and designs for trials where
  non-proportional hazards may be anticipated for outcome data.
  }
  \examples{
  library(survival)
  data(Ex1delayedEffect)
  km1 <- with(Ex1delayedEffect,survfit(Surv(month,evntd)~trt))
  km1
  plot(km1)
  with(subset(Ex1delayedEffect,trt==1),survfit(Surv(month,evntd)~trt))
  with(subset(Ex1delayedEffect,trt==0),survfit(Surv(month,evntd)~trt))
  }
  \references{
  TBD
  }
  \seealso{
  \code{\link{Ex2delayedEffect}}, \code{\link{Ex3curewithph}}, \code{\link{Ex4belly}},
  \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  }
  \keyword{datasets}

Package: simtrial
File: man/Ex2delayedEffect.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/Ex2delayedEffect.R
  \docType{data}
  \name{Ex2delayedEffect}
  \alias{Ex2delayedEffect}
  \title{Time-to-event data example 2 for non-proportional hazards working group}
  \format{
  Data frame with 4 variables:
  \describe{ 
  \item{id}{sequential numbering of unique identifiers}
  \item{month}{time-to-event}
  \item{event}{1 for event, 0 for censored} 
  \item{trt}{1 for experimental, 0 for control}
  }
  }
  \usage{
  data(Ex2delayedEffect)
  }
  \description{
  Survival objects reverse-engineered datasets from published Kaplan-Meier
  curves. 
  Individual trials are de-identified since the data are only
  approximations of the actual data.
  Data are intended to evaluate methods and designs for trials where
  non-proportional hazards may be anticipated for outcome data.
  }
  \examples{
  library(survival)
  data(Ex2delayedEffect)
  km1 <- with(Ex2delayedEffect,survfit(Surv(month,evntd)~trt))
  km1
  plot(km1)
  with(subset(Ex2delayedEffect,trt==1),survfit(Surv(month,evntd)~trt))
  with(subset(Ex2delayedEffect,trt==0),survfit(Surv(month,evntd)~trt))
  }
  \references{
  TBD
  }
  \seealso{
  \code{\link{Ex1delayedEffect}}, \code{\link{Ex3curewithph}}, \code{\link{Ex4belly}},
  \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  }
  \keyword{datasets}

Package: simtrial
File: man/Ex3curewithph.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/Ex3curewithph.R
  \docType{data}
  \name{Ex3curewithph}
  \alias{Ex3curewithph}
  \title{Time-to-event data example 3 for non-proportional hazards working group}
  \format{
  Data frame with 4 variables:
  \describe{ 
  \item{id}{sequential numbering of unique identifiers}
  \item{month}{time-to-event}
  \item{event}{1 for event, 0 for censored} 
  \item{trt}{1 for experimental, 0 for control}
  }
  }
  \usage{
  data(Ex3curewithph)
  }
  \description{
  Survival objects reverse-engineered datasets from published Kaplan-Meier
  curves. 
  Individual trials are de-identified since the data are only
  approximations of the actual data.
  Data are intended to evaluate methods and designs for trials where
  non-proportional hazards may be anticipated for outcome data.
  }
  \examples{
  library(survival)
  data(Ex3curewithph)
  km1 <- with(Ex3curewithph,survfit(Surv(month,evntd)~trt))
  km1
  plot(km1)
  }
  \references{
  TBD
  }
  \seealso{
  \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, \code{\link{Ex4belly}},
  \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  }
  \keyword{datasets}

Package: simtrial
File: man/Ex4belly.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/Ex4belly.R
  \docType{data}
  \name{Ex4belly}
  \alias{Ex4belly}
  \title{Time-to-event data example 4 for non-proportional hazards working group}
  \format{
  Data frame with 4 variables:
  \describe{ 
  \item{id}{sequential numbering of unique identifiers}
  \item{month}{time-to-event}
  \item{event}{1 for event, 0 for censored} 
  \item{trt}{1 for experimental, 0 for control}
  }
  }
  \usage{
  data(Ex4belly)
  }
  \description{
  Survival objects reverse-engineered datasets from published Kaplan-Meier
  curves. 
  Individual trials are de-identified since the data are only
  approximations of the actual data.
  Data are intended to evaluate methods and designs for trials where
  non-proportional hazards may be anticipated for outcome data.
  }
  \examples{
  library(survival)
  data(Ex4belly)
  km1 <- with(Ex4belly,survfit(Surv(month,evntd)~trt))
  km1
  plot(km1)
  }
  \references{
  TBD
  }
  \seealso{
  \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, 
  \code{\link{Ex3curewithph}},
  \code{\link{Ex5widening}}, \code{\link{Ex6crossing}}
  }
  \keyword{datasets}

Package: simtrial
File: man/Ex5widening.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/Ex5widening.R
  \docType{data}
  \name{Ex5widening}
  \alias{Ex5widening}
  \title{Time-to-event data example 5 for non-proportional hazards working group}
  \format{
  Data frame with 4 variables:
  \describe{ 
  \item{id}{sequential numbering of unique identifiers}
  \item{month}{time-to-event}
  \item{event}{1 for event, 0 for censored} 
  \item{trt}{1 for experimental, 0 for control}
  }
  }
  \usage{
  data(Ex5widening)
  }
  \description{
  Survival objects reverse-engineered datasets from published Kaplan-Meier
  curves. 
  Individual trials are de-identified since the data are only
  approximations of the actual data.
  Data are intended to evaluate methods and designs for trials where
  non-proportional hazards may be anticipated for outcome data.
  }
  \examples{
  library(survival)
  data(Ex5widening)
  km1 <- with(Ex5widening,survfit(Surv(month,evntd)~trt))
  km1
  plot(km1)
  }
  \references{
  TBD
  }
  \seealso{
  \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, 
  \code{\link{Ex3curewithph}},
  \code{\link{Ex4belly}}, \code{\link{Ex6crossing}}
  }
  \keyword{datasets}

Package: simtrial
File: man/Ex6crossing.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/Ex6crossing.R
  \docType{data}
  \name{Ex6crossing}
  \alias{Ex6crossing}
  \title{Time-to-event data example 6 for non-proportional hazards working group}
  \format{
  Data frame with 4 variables:
  \describe{ 
  \item{id}{sequential numbering of unique identifiers}
  \item{month}{time-to-event}
  \item{event}{1 for event, 0 for censored} 
  \item{trt}{1 for experimental, 0 for control}
  }
  }
  \usage{
  data(Ex6crossing)
  }
  \description{
  Survival objects reverse-engineered datasets from published Kaplan-Meier
  curves. 
  Individual trials are de-identified since the data are only
  approximations of the actual data.
  Data are intended to evaluate methods and designs for trials where
  non-proportional hazards may be anticipated for outcome data.
  }
  \examples{
  library(survival)
  data(Ex6crossing)
  km1 <- with(Ex6crossing,survfit(Surv(month,evntd)~trt))
  km1
  plot(km1)
  }
  \references{
  TBD
  }
  \seealso{
  \code{\link{Ex1delayedEffect}}, \code{\link{Ex2delayedEffect}}, 
  \code{\link{Ex3curewithph}},
  \code{\link{Ex4belly}}, \code{\link{Ex5widening}}
  }
  \keyword{datasets}

Package: simtrial
File: man/fixedBlockRand.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/fixedBlockRand.R
  \name{fixedBlockRand}
  \alias{fixedBlockRand}
  \title{Permuted fixed block randomization}
  \usage{
  fixedBlockRand(n = 10, block = c(0, 0, 1, 1))
  }
  \arguments{
  \item{n}{sample size to be randomized}
  
  \item{block}{Vector of treatments to be included in each block}
  }
  \value{
  A treatment group sequence (vector) of length `n` with treatments from `block` permuted within
  each block having block size equal to the length of `block`
  }
  \description{
  Fixed block randomization. The `block` input should repeat each treatment code the number of
  times it is to be included within each block. The final block will be a partial block if `n` is not an
  exact multiple of the block length.
  }
  \examples{
  library(dplyr)
  # 2:1 randomization with block size 3, treatments "A" and "B"
  tibble(x=1:10) \%>\% mutate(Treatment=fixedBlockRand(block=c("A","B","B")))
  # Stratified randomization
  tibble(Stratum=c(rep("A",10),rep("B",10))) \%>\%
  group_by(Stratum) \%>\%
  mutate(Treatment=fixedBlockRand())
  }

Package: simtrial
File: man/getCutDateForCount.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/getCutDateForCount.R
  \name{getCutDateForCount}
  \alias{getCutDateForCount}
  \title{Get Date at Which an Event Count is Reached}
  \usage{
  getCutDateForCount(x, count)
  }
  \arguments{
  \item{x}{a time-to-event dataset, e.g., generated by \code{simPWSurv}}
  
  \item{count}{event count at which dataset is to be cut off for analysis}
  }
  \value{
  The a numeric value with the \code{cte} from the input dataset at which the targeted event count
  is reached, or if the final event count is never reached, the final \code{cte} at which an event occurs.
  }
  \description{
  Get Date at Which an Event Count is Reached
  }
  \examples{
  library(dplyr)
  # Use default enrollment and calendar cut date for 50 events in Positive stratum
  x <- simPWSurv(n=200,
                 strata = tibble::tibble(Stratum=c("Positive","Negative"), p = c(.5, .5)),
                 failRates = tibble::tibble(Stratum = rep(c("Positive","Negative"),2),
                                            period = rep(1, 4),
                                            Treatment = c(rep("Control", 2), 
                                                          rep("Experimental", 2)),
                                            duration = rep(1, 4),
                                            rate = log(2) / c(6, 9, 9, 12)
                                            ),
                 dropoutRates = tibble::tibble(Stratum = rep(c("Positive","Negative"),2),
                                               period = rep(1, 4),
                                               Treatment = c(rep("Control", 2), 
                                                             rep("Experimental", 2)),
                                               duration = rep(1, 4),
                                               rate = rep(.001, 4)
                                              )
                )
  d <- getCutDateForCount(filter(x,Stratum=="Positive"),count=50)
  y <- cutData(x,cutDate=d)
  table(y$Stratum,y$event)
  
  }

Package: simtrial
File: man/MBdelayed.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/MBdelayed.R
  \docType{data}
  \name{MBdelayed}
  \alias{MBdelayed}
  \title{Simulated survival dataset with delayed treatment effect}
  \format{
  A tibble with 200 rows and xx columns
  \describe{
  \item{tte}{time to event}
  }
  }
  \usage{
  MBdelayed
  }
  \description{
  Magirr and Burman (2019) considered several scenarios for their modestly weighted logrank test.
  One of these had a delayed treatment effect with a hazard ratio of 1 for 6 months followed by a hazard ratio of 1/2
  thereafter.
  The scenario enrolled 200 patients uniformly over 12 months and 
  cut data for analysis 36 months after enrollment was opened. 
  This dataset was generated by the `simtrial::simPWSurv()` function under the above scenario.
  }
  \examples{
  library(tidyr)
  library(dplyr)
  library(survival)
  library(mvtnorm)
  fit <- survfit(Surv(tte, event) ~ Treatment, data = MBdelayed)
  
  # Plot survival
  plot(fit, lty=1:2) 
  legend("topright", legend = c("Control", "Experimental"), lty = 1:2)
  
  # Set up time, event, number of event dataset for testing
  # with arbitrary weights
  ten <- MBdelayed \%>\% tensurv(txval = "Experimental")
  head(ten)
  
  # MaxCombo with logrank, FH(0,1), FH(1,1)
  ten \%>\% tenFHcorr(rg=tibble(rho=c(0, 0, 1), gamma=c(0, 1, 1))) \%>\%
          pMaxCombo()
  
  # Magirr-Burman modestly down-weighted rank test with 6 month delay
  # First, add weights
  ten <- ten \%>\% wMB(6)
  head(ten)
  
  # Now compute test based on these weights
  ten \%>\% summarise(S = sum(OminusE*wMB),
                    V = sum(Var*wMB^2),
                    Z = S/sqrt(V)) \%>\%
          mutate(p=pnorm(Z))
  
  # Create 0 weights for first 6 months
  ten <- ten \%>\% mutate(w6 = 1 * (tte >= 6))
  ten \%>\% summarise(S = sum(OminusE*w6),
                    V = sum(Var*w6^2),
                    Z = S/sqrt(V)) \%>\% 
          mutate(p=pnorm(Z))
          
  # Generate another dataset
  ds <- simPWSurv(n = 200,
                  enrollRates = tibble(rate = 200 / 12, duration = 12),
                  failRates = tribble(
                     ~Stratum, ~Period, ~Treatment,     ~duration, ~rate,
                     "All",        1,   "Control",      42,        log(2) / 15,
                     "All",        1,   "Experimental", 6,         log(2) / 15,
                     "All",        2,   "Experimental", 36,        log(2) / 15 * 0.6),
                  dropoutRates = tribble(
                     ~Stratum, ~Period, ~Treatment,     ~duration, ~rate,
                     "All",        1,   "Control",      42,        0,
                     "All",        1,   "Experimental", 42,        0)
         )
  # Cut data at 24 months after final enrollment
  MBdelayed2 <- ds \%>\% cutData(max(ds$enrollTime) + 24)
  
  }
  \references{
  Magirr, Dominic, and CarlâFredrik Burman. 
  "Modestly weighted logrank tests." 
  \emph{Statistics in Medicine} 38.20 (2019): 3782-3790.
  }
  \keyword{datasets}

Package: simtrial
File: man/pMaxCombo.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/pMaxCombo.R
  \name{pMaxCombo}
  \alias{pMaxCombo}
  \title{MaxCombo p-value}
  \usage{
  pMaxCombo(Z, dummyvar, algorithm = GenzBretz(maxpts = 50000, abseps = 1e-05))
  }
  \arguments{
  \item{Z}{a dataset output from \code{tenFHcorr()}; see examples.}
  
  \item{dummyvar}{a dummy input that allows \code{group_map()} to be used to
  compute p-values for multiple simulations.}
  
  \item{algorithm}{This is passed directly to the \code{algorithm} argument in the \code{mvtnorm::pmvnorm()}}
  }
  \value{
  A numeric p-value
  }
  \description{
  \code{pMaxCombo()} computes p-values for the MaxCombo test
  based on output from \code{simtrial::tenFHcorr()}.
  This is still in an experimental stage and is intended for use with
  the \code{simtrial::simfix()} trial simulation routine.
  However, it can also be used to analyze clinical trial data such as that provided in the
  ADaM ADTTE format.
  }
  \examples{
  library(tidyr)
  x <- simfix(nsim=1,timingType=5,rg=tibble::tibble(rho=c(0,0,1),gamma=c(0,1,1)))
  head(x)
  pMaxCombo(x)
  # Only use cuts for events, events + min follow-up
  xx <- simfix(nsim=100,timingType=5,rg=tibble::tibble(rho=c(0,0,1),gamma=c(0,1,1)))
  head(xx)
  # MaxCombo power estimate for cutoff at max of targeted events, minimum follow-up
  p <- unlist(xx \%>\%  dplyr::group_by(Sim) \%>\% dplyr::group_map(pMaxCombo))
  mean(p<.025)
  }

Package: simtrial
File: man/pwexpfit.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/pwexpfit.R
  \name{pwexpfit}
  \alias{pwexpfit}
  \title{Piecewise exponential survival estimation}
  \usage{
  pwexpfit(
    Srv = Surv(time = Ex1delayedEffect$month, event = Ex1delayedEffect$evntd),
    intervals = array(3, 3)
  )
  }
  \arguments{
  \item{Srv}{input survival object (see \code{Surv}); note that only 0=censored, 1=event for \code{Surv}}
  
  \item{intervals}{Vector containing positive values indicating interval lengths where the 
  exponential rates are assumed. 
  Note that a final infinite interval is added if any events occur after the final interval 
  specified.}
  }
  \value{
  A matrix with rows containing interval length, estimated rate, -2*log-likelihood for each interval.
  }
  \description{
  Computes survival function, density function, -2*log-likelihood based
  on input dataset and intervals for piecewise constant failure rates.
  Initial version assumes observations are right censored or events only.
  }
  \examples{
  # use default arguments for delayed effect example dataset (Ex1delayedEffect)
  library(survival)
  rateall <- pwexpfit()
  rateall
  # Estimate by treatment effect
  rate1 <- with(subset(Ex1delayedEffect,trt==1), pwexpfit(Surv(month,evntd)))
  rate0 <- with(subset(Ex1delayedEffect,trt==0), pwexpfit(Surv(month,evntd)))
  rate1
  rate0
  rate1$rate/rate0$rate
  # chi-square test for (any) treatment effect (8 - 4 parameters = 4 df)
  pchisq(sum(rateall$m2ll)-sum(rate1$m2ll+rate0$m2ll), df = 4, lower.tail=FALSE)
  # compare with logrank
  survdiff(formula = Surv(month, evntd) ~ trt, data = Ex1delayedEffect)
  # simple model with 3 rates same for each for 3 months, 
  # different for each treatment after months
  rate1a <- with(subset(Ex1delayedEffect,trt==1), pwexpfit(Surv(month,evntd),3))
  rate0a <- with(subset(Ex1delayedEffect,trt==0), pwexpfit(Surv(month,evntd),3))
  rate1a$rate/rate0a$rate
  m2ll0 <- rateall$m2ll[1]+rate1a$m2ll[2]+rate0a$m2ll[2]
  m2ll1 <- sum(rate0$m2ll)+sum(rate1$m2ll)
  # as a measure of strength, chi-square examines improvement in likelihood
  pchisq(m2ll0-m2ll1, 5, lower.tail=FALSE)
  
  }

Package: simtrial
File: man/rpwenroll.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/rpwenroll.R
  \name{rpwenroll}
  \alias{rpwenroll}
  \title{Generate Piecewise Exponential Enrollment}
  \usage{
  rpwenroll(
    n = NULL,
    enrollRates = tibble::tibble(duration = c(1, 2), rate = c(2, 5))
  )
  }
  \arguments{
  \item{n}{Number of observations.
  Default of \code{NULL} yields random enrollment size.}
  
  \item{enrollRates}{A tibble containing period duration (\code{duration}) and enrollment rate (\code{rate})
  for specified enrollment periods.
  If necessary, last period will be extended to ensure enrollment of specified \code{n}.}
  }
  \value{
  A vector of random enrollment times.
  }
  \description{
  With piecewise exponential enrollment rate generation any enrollment rate distribution can be easily approximated.
  \code{rpwenroll()} is to support simulation of both the Lachin and Foulkes (1986) sample size
  method for (fixed trial duration) as well as the Kim and Tsiatis(1990) method
  (fixed enrollment rates and either fixed enrollment duration or fixed minimum follow-up);
  see \code{\link[gsDesign:nSurv]{gsDesign}}.
  }
  \examples{
  # piecewise uniform (piecewise exponential inter-arrival times) for 10k patients enrollment
  # enrollment rates of 5 for time 0-100, 15 for 100-300, and 30 thereafter
  x <- rpwenroll(n=10000, enrollRates=tibble::tibble(rate = c(5, 15, 30), duration = c(100,200,100)))
  plot(x,1:10000,
       main="Piecewise uniform enrollment simulation",xlab="Time",
       ylab="Enrollment")
  # exponential enrollment
  x <- rpwenroll(10000, enrollRates=tibble::tibble(rate = .03, duration = 1))
  plot(x,1:10000,main="Simulated exponential inter-arrival times",
       xlab="Time",ylab="Enrollment")
  
  }

Package: simtrial
File: man/rpwexp.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/rpwexp.R
  \name{rpwexp}
  \alias{rpwexp}
  \title{The Piecewise Exponential Distribution}
  \usage{
  rpwexp(n = 100, failRates = tibble(duration = c(1, 1), rate = c(10, 20)))
  }
  \arguments{
  \item{n}{Number of observations to be generated.}
  
  \item{failRates}{A tibble containing \code{duration} and \code{rate} variables.
  \code{rate} specifies failure rates during the corresponding interval duration
  specified in \code{duration}. The final interval is extended to be infinite
  to ensure all observations are generated.}
  }
  \description{
  The piecewise exponential distribution allows a simple method to specify a distribtuion
  where the hazard rate changes over time. It is likely to be useful for conditions where
  failure rates change, but also for simulations where there may be a delayed treatment
  effect or a treatment effect that that is otherwise changing (e.g., decreasing) over time.
  \code{rpwexp()} is to support simulation of both the Lachin and Foulkes (1986) sample size
  method for (fixed trial duration) as well as the Kim and Tsiatis(1990) method
  (fixed enrollment rates and either fixed enrollment duration or fixed minimum follow-up);
  see \code{\link[gsDesign:nSurv]{gsDesign}}.
  }
  \details{
  Using the \code{cumulative=TRUE} option, enrollment times that piecewise constant over
  time can be generated.
  }
  \examples{
  # get 10k piecewise exponential failure times
  # failure rates are 1 for time 0-.5, 3 for time .5 - 1 and 10 for >1.
  # intervals specifies duration of each failure rate interval
  # with the final interval running to infinity
  x <- rpwexp(10000, failRates=tibble::tibble(rate = c(1, 3, 10), duration = c(.5,.5,1)))
  plot(sort(x),(10000:1)/10001,log="y", main="PW Exponential simulated survival curve",
  xlab="Time",ylab="P{Survival}")
  # exponential failure times
  x <- rpwexp(10000, failRates=tibble::tibble(rate = 5, duration=1))
  
  plot(sort(x),(10000:1)/10001,log="y", main="Exponential simulated survival curve",
       xlab="Time",ylab="P{Survival}")
  
  }

Package: simtrial
File: man/simfix.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/simfix.R
  \name{simfix}
  \alias{simfix}
  \title{Simulation of fixed sample size design for time-to-event endpoint}
  \usage{
  simfix(
    nsim = 1000,
    sampleSize = 500,
    targetEvents = 350,
    strata = tibble::tibble(Stratum = "All", p = 1),
    enrollRates = tibble::tibble(duration = c(2, 2, 10), rate = c(3, 6, 9)),
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2)),
    totalDuration = 30,
    block = rep(c("Experimental", "Control"), 2),
    timingType = 1:5,
    rg = tibble::tibble(rho = 0, gamma = 0)
  )
  }
  \arguments{
  \item{nsim}{Number of simulations to perform.}
  
  \item{sampleSize}{Total sample size per simulation.}
  
  \item{targetEvents}{Targeted event count for analysis.}
  
  \item{strata}{A tibble with strata specified in `Stratum`, probability (incidence) of each stratum in `p`.}
  
  \item{enrollRates}{Piecewise constant enrollment rates by time period.
  Note that these are overall population enrollment rates and the `strata` argument controls the
  random distribution between strata.}
  
  \item{failRates}{Piecewise constant control group failure rates, hazard ratio for experimental vs control,
  and dropout rates by stratum and time period.}
  
  \item{totalDuration}{Total follow-up from start of enrollment to data cutoff.}
  
  \item{block}{As in `simtrial::simPWSurv()`. Vector of treatments to be included in each block.}
  
  \item{timingType}{A numeric vector determining data cutoffs used; see details.
  Default is to include all available cutoff methods.}
  
  \item{rg}{As in `simtrial::tenFHCorr()`.
  A \code{tibble} with variables \code{rho} and \code{gamma}, both greater than equal
  to zero, to specify one Fleming-Harrington weighted logrank test per row.}
  }
  \value{
  A \code{tibble} including columns \code{Events} (event count), \code{lnhr} (log-hazard ratio),
  \code{Z} (normal test statistic; < 0 favors experimental) cut (text describing cutoff used),
  \code{Duration} (duration of trial at cutoff for analysis) and \code{sim} (sequential simulation id).
  One row per simulated dataset per cutoff specified in \code{timingType}, per test statistic specified.
  If multiple Fleming-Harrington tests are specified in \code{rg}, then columns {rho,gamma}
  are also included.
  }
  \description{
  `simfix()` provide simulations of a single endpoint two-arm trial
  where the enrollment, hazard ratio, and failure and dropout rates change over time.
  }
  \details{
  \code{timingType} has up to 5 elements indicating different options for data cutoff.
  1 uses the planned study duration, 2 the time the targeted event count is achieved,
  3 the planned minimum follow-up after enrollment is complete,
  4 the maximum of planned study duration and targeted event count cuts (1 and 2),
  5 the maximum of targeted event count and minimum follow-up cuts (2 and 3).
  }
  \examples{
  library(tidyr)
  library(dplyr)
  # Show output structure
  simfix(nsim=3)
  # Example with 2 tests: logrank and FH(0,1)
  simfix(nsim=1,rg=tibble::tibble(rho=0,gamma=c(0,1)))
  # Power by test
  # Only use cuts for events, events + min follow-up
  xx <- simfix(nsim=100,timingType=c(2,5),rg=tibble::tibble(rho=0,gamma=c(0,1)))
  # Get power approximation for FH, data cutoff combination
  xx \%>\% group_by(cut,rho,gamma) \%>\% summarise(mean(Z<=qnorm(.025)))
  # MaxCombo power estimate for cutoff at max of targeted events, minimum follow-up
  p <- xx \%>\%  filter(cut != "Targeted events") \%>\% group_by(Sim) \%>\% group_map(pMaxCombo)
  p <- unlist(p)
  mean(p<.025)
  # MaxCombo estimate for targeted events cutoff
  p <- unlist(xx \%>\%  filter(cut == "Targeted events") \%>\% group_by(Sim) \%>\% group_map(pMaxCombo))
  mean(p<.025)
  }

Package: simtrial
File: man/simfix2simPWSurv.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/simfix2simPWSurv.R
  \name{simfix2simPWSurv}
  \alias{simfix2simPWSurv}
  \title{Conversion of enrollment and failure rates from simfix() to simPWSurv() format}
  \usage{
  simfix2simPWSurv(
    failRates = tibble::tibble(Stratum = "All", duration = c(3, 100), failRate =
      log(2)/c(9, 18), hr = c(0.9, 0.6), dropoutRate = rep(0.001, 2))
  )
  }
  \arguments{
  \item{failRates}{Piecewise constant control group failure rates, hazard ratio for experimental vs control,
  and dropout rates by stratum and time period.}
  }
  \value{
  A \code{list} of two `tibble` components formatted for `simtrial::simPWSurv()`:
  `failRates` and `dropoutRates`.
  }
  \description{
  `simfix2simPWSurv()` converts failure rates and dropout rates entered in the simpler
  format for `simfix()` to that used for `simtrial::simPWSurv()`.
  The `failRates` argument for `simfix()` requires enrollment rates, failure rates
  hazard ratios and dropout rates by strata for a 2-arm trial, `simtrial::simPWSurv()`
  is in a more flexible but less obvious but more flexible format.
  Since `simfix()` automatically analyzes data and `simtrial::simPWSurv()` just produces
  a simulation dataset, the latter provides additional options to analyze or otherwise evaluate
  individual simulations in ways that `simfix()` does not.
  }
  \examples{
  library(tidyr)
  library(dplyr)
  # Convert standard input
  simfix2simPWSurv()
  # Stratified example
  failRates <- tibble::tibble(Stratum=c(rep("Low",3),rep("High",3)),
                              duration=rep(c(4,10,100),2),
                              failRate=c(.04,.1,.06,
                                         .08,.16,.12),
                              hr=c(1.5,.5,2/3,
                                   2, 10/16, 10/12),
                              dropoutRate=.01
  )
  x <- simfix2simPWSurv(failRates)
  # Do a single simulation with the above rates
  # Enroll 300 patients in ~12 months at constant rate
  sim <-
      simPWSurv(n=300,
            strata=tibble::tibble(Stratum=c("Low","High"),p=c(.6,.4)),
            enrollRates=tibble::tibble(duration=12,rate=300/12),
            failRates=x$failRates,
            dropoutRates=x$dropoutRates)
  # Cut after 200 events and do a stratified logrank test
  dat <- sim \%>\%
         cutDataAtCount(200) \%>\%            # cut data
         tensurv(txval="Experimental") \%>\%  # convert format for tenFH
         tenFH(rg=tibble(rho=0,gamma=0))    # stratified logrank
  }

Package: simtrial
File: man/simPWSurv.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/simPWSurv.R
  \name{simPWSurv}
  \alias{simPWSurv}
  \title{Simulate a stratified time-to-event outcome randomized trial}
  \usage{
  simPWSurv(
    n = 100,
    strata = tibble::tibble(Stratum = "All", p = 1),
    block = c(rep("Control", 2), rep("Experimental", 2)),
    enrollRates = tibble::tibble(rate = 9, duration = 1),
    failRates = tibble::tibble(Stratum = rep("All", 4), period = rep(1:2, 2), Treatment =
      c(rep("Control", 2), rep("Experimental", 2)), duration = rep(c(3, 1), 2), rate =
      log(2)/c(9, 9, 9, 18)),
    dropoutRates = tibble::tibble(Stratum = rep("All", 2), period = rep(1, 2), Treatment
      = c("Control", "Experimental"), duration = rep(100, 2), rate = rep(0.001, 2))
  )
  }
  \arguments{
  \item{n}{Number of observations.
  If length(n) > 1, the length is taken to be the number required.}
  
  \item{strata}{A tibble with strata specified in `Stratum`, probability (incidence) of each stratum
  in `p`}
  
  \item{block}{Vector of treatments to be included in each  block}
  
  \item{enrollRates}{Enrollment rates; see details and examples}
  
  \item{failRates}{Failure rates; see details and examples; note that treatments need
  to be the same as input in block}
  
  \item{dropoutRates}{Dropout rates; see details and examples; note that treatments need
  to be the same as input in block}
  }
  \value{
  a \code{tibble} with the following variables for each observation
  \code{Stratum},
  \code{enrollTime} (enrollment time for the observation),
  \code{Treatment} (treatment group; this will be one of the values in the input \code{block}),
  \code{failTime} (failure time generated using \code{rpwexp()}),
  \code{dropoutTime} (dropout time generated using \code{rpwexp()}),
  \code{cte} (calendar time of enrollment plot the minimum of failure time and dropout time),
  \code{fail} (indicator that \code{cte} was set using failure time; i.e., 1 is a failure, 0 is a dropout).
  }
  \description{
  \code{simPWSurv} enables simulation of a clinical trial with essentially arbitrary
  patterns of enrollment, failure rates and censoring.
  The piecewise exponential distribution allows a simple method to specify a distribtuion
  and enrollment pattern
  where the enrollment, failure and dropout rate changes over time.
  While the main purpose may be to generate a trial that can be analyzed at a single point in time or
  using group sequential methods, the routine can also be used to simulate an adaptive trial design.
  Enrollment, failure and dropout rates are specified by treatment group, stratum and time period.
  Fixed block randomization is used; blocks must include treatments provided in failure and dropout
  specification.
  Default arguments are set up to allow very simple implementation of a non-proportional hazards assumption
  for an unstratified design.
  }
  \examples{
  library(dplyr)
  # Tests
   simPWSurv(n=20)
   # 3:1 randomization
   simPWSurv(n=20,block=c(rep("Experimental",3),"Control"))
  
  # Simulate 2 strata; will use defaults for blocking and enrollRates
  simPWSurv(n=20,
            # 2 strata,30\% and 70\% prevalence
            strata=tibble::tibble(Stratum=c("Low","High"),p=c(.3,.7)),
            failRates=tibble::tibble(Stratum=c(rep("Low",4),rep("High",4)),
                                     period=rep(1:2,4),
                                     Treatment=rep(c(rep("Control",2),rep("Experimental",2)),2),
                                     duration=rep(c(3,1),4),
                                     rate=c(.03,.05,.03,.03,.05,.08,.07,.04)),
            dropoutRates=tibble::tibble(Stratum=c(rep("Low",2),rep("High",2)),
                                        period=rep(1,4),
                                        Treatment=rep(c("Control","Experimental"),2),
                                        duration=rep(1,4),
                                        rate=rep(.001,4))
  )
  
  # If you want a more rectangular entry for a tibble
  failRates <- bind_rows(
     tibble(Stratum="Low" ,period=1,Treatment="Control"     ,duration=3,rate=.03),
     tibble(Stratum="Low" ,period=1,Treatment="Experimental",duration=3,rate=.03),
     tibble(Stratum="Low" ,period=2,Treatment="Experimental",duration=3,rate=.02),
     tibble(Stratum="High",period=1,Treatment="Control"     ,duration=3,rate=.05),
     tibble(Stratum="High",period=1,Treatment="Experimental",duration=3,rate=.06),
     tibble(Stratum="High",period=2,Treatment="Experimental",duration=3,rate=.03)
  )
  dropoutRates <- bind_rows(
     tibble(Stratum="Low" ,period=1,Treatment="Control"     ,duration=3,rate=.001),
     tibble(Stratum="Low" ,period=1,Treatment="Experimental",duration=3,rate=.001),
     tibble(Stratum="High",period=1,Treatment="Control"     ,duration=3,rate=.001),
     tibble(Stratum="High",period=1,Treatment="Experimental",duration=3,rate=.001)
  )
  simPWSurv(n=12,strata=tibble(Stratum=c("Low","High"),p=c(.3,.7)),
           failRates=failRates,dropoutRates=dropoutRates)
  }

Package: simtrial
File: man/simtrial-package.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/simtrial-package.R
  \docType{package}
  \name{simtrial-package}
  \alias{simtrial}
  \alias{simtrial-package}
  \title{simtrial: Clinical Trial Simulation}
  \description{
  simtrial provides some basic routines for simulating a clinical trial. 
      The primary intent is to provide some tools to generate trial simulations for trials with time to event outcomes. 
      Piecewise exponential failure rates and piecewise constant enrollment rates are the underlying mechanism used 
      to simulate a broad range of scenarios.
      However, the basic generation of data is done using pipes to allow maximum flexibility for users to meet different needs.
  }
  \seealso{
  Useful links:
  \itemize{
    \item \url{https://merck.github.io/simtrial/}
    \item \url{https://github.com/Merck/simtrial}
    \item Report bugs at \url{https://github.com/Merck/simtrial/issues}
  }
  
  }
  \author{
  \strong{Maintainer}: Yilong Zhang \email{yilong.zhang@merck.com}
  
  Authors:
  \itemize{
    \item Keaven Anderson \email{keaven_anderson@merck.com}
  }
  
  Other contributors:
  \itemize{
    \item Amin Shirazi \email{ashirazist@gmail.com} [contributor]
    \item Ruixue Wang \email{ruixue.wang@merck.com} [contributor]
    \item Yi Cui \email{yi.cui@merck.com} [contributor]
    \item Ping Yang \email{ping.yang1@merck.com} [contributor]
    \item Yalin Zhu \email{yalin.zhu@merck.com} [contributor]
    \item Heng Zhou \email{heng.zhou@merck.com} [contributor]
    \item Merck Sharp & Dohme Corp [copyright holder]
  }
  
  }
  \keyword{internal}

Package: simtrial
File: man/tenFH.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/tenFH.R
  \name{tenFH}
  \alias{tenFH}
  \title{Fleming-Harrington Weighted Logrank Tests}
  \usage{
  tenFH(
    x = simPWSurv(n = 200) \%>\% cutDataAtCount(150) \%>\% tensurv(txval =
      "Experimental"),
    rg = tibble(rho = c(0, 0, 1, 1), gamma = c(0, 1, 0, 1)),
    returnVariance = FALSE
  )
  }
  \arguments{
  \item{x}{a \code{tensurv}-class \code{tibble} with a counting process dataset}
  
  \item{rg}{a \code{tibble} with variables \code{rho} and \code{gamma}, both greater than equal
  to zero, to specify one Fleming-Harrington weighted logrank test per row;
  Default: tibble(rho = c(0, 0, 1, 1), gamma = c(0, 1, 0, 1))}
  
  \item{returnVariance}{a logical flag that, if true, adds columns
  estimated variance for weighted sum of observed minus expected; see details; Default: FALSE}
  }
  \value{
  a `tibble` with \code{rg} as input and the FH test statistic
  for the data in \code{x}
  (\code{Z}, a directional square root of the usual weighted logrank test);
  if variance calculations are specified (e.g., to be used for covariances in a combination test),
  the this will be returned in the column \code{Var}
  }
  \description{
  With output from the function \code{tensurv}
  }
  \details{
  The input value \code{x} produced by \code{tensurv()} produces a counting process dataset 
  grouped by strata and sorted within strata by increasing times where events occur.
  \itemize{
  \item \eqn{Z} - standardized normal Fleming-Harrington weighted logrank test
  \item \eqn{i}  - stratum index
  \item \eqn{d_i} - number of distinct times at which events occurred in stratum \eqn{i} 
  \item \eqn{t_{ij}} - ordered times at which events in stratum \eqn{i}, \eqn{j=1,2,\ldots d_i} were observed; 
  for each observation, \eqn{t_{ij}} represents the time post study entry
  \item {O_{ij.}} - total number of events in stratum \eqn{i} that occurred at time \eqn{t_{ij}}
  \item {O_{ije}} - total number of events in stratum \eqn{i} in the experimental treatment group that occurred
  at time \eqn{t_{ij}}
  \item {N_{ij.}} - total number of study subjects in stratum \eqn{i} who were followed for at least duration
  \item {E_{ije}} - expected observations in experimental treatment group given random selection of \eqn{O_{ij.}} 
  from those in stratum \eqn{i} at risk at time \eqn{t_{ij}}
  \item {V_{ije}} - hypergeometric variance for \eqn{E_{ije}} as produced in \code{Var} 
  from the \code{tensurv()} routine  
  \item {N_{ije}} - total number of study subjects in stratum \eqn{i} in the experimental treatment group
  who were followed for at least duration \eqn{t_{ij}}
  \item {E_{ije}} - expected observations in experimental group in stratum \eqn{i} at time \eqn{t_{ij}} 
  conditioning on the overall number of events and at risk populations at that time and sampling at risk 
  observations without replacement:
  \deqn{E_{ije} = O_{ij.} N_{ije}/N_{ij.}}
  \item \eqn{S_{ij}} - Kaplan-Meier estimate of survival in combined treatment groups immediately prior 
  to time \eqn{t_{ij}}
  \item \eqn{\rho, \gamma} - real parameters for Fleming-Harrington test
  \item \eqn{X_i} - Numerator for signed logrank test in stratum \eqn{i}
  \deqn{X_i = \sum_{j=1}^d_i S_{ij}^\rho(1-S_{ij}^\gamma)(O_{ije}-E_{ije})}
  \item {V_{ij}} - variance used in denominator for Fleming-Harrington weighted logrank tests
  \deqn{V_i = \sum_{j=1}^d_i (S_{ij}^\rho(1-S_{ij}^\gamma))^2V_{ij})}
  
  The stratified Fleming-Harrington weighted logrank test is then computed as:
  \deqn{Z = \sum_i X_i/\sqrt{\sum_i V_i}}
  }
  }
  \examples{
  library(tidyr)
  # Use default enrollment and event rates at cut at 100 events
  x <- simPWSurv(n=200) \%>\% cutDataAtCount(100) \%>\% tensurv(txval="Experimental")
  # compute logrank (FH(0,0)) and FH(0,1)
  tenFH(x,rg=tibble(rho=c(0,0),gamma=c(0,1)))
  }

Package: simtrial
File: man/tenFHcorr.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/tenFHCorr.R
  \name{tenFHcorr}
  \alias{tenFHcorr}
  \title{Fleming-Harrington Weighted Logrank Tests plus Correlations}
  \usage{
  tenFHcorr(
    x = simPWSurv(n = 200) \%>\% cutDataAtCount(100) \%>\% tensurv(txval =
      "Experimental"),
    rg = tibble(rho = c(0, 0, 1, 1), gamma = c(0, 1, 0, 1)),
    corr = TRUE
  )
  }
  \arguments{
  \item{x}{a \code{tensurv}-class \code{tibble} with a counting process dataset}
  
  \item{rg}{a \code{tibble} with variables \code{rho} and \code{gamma}, both greater than equal
  to zero, to specify one Fleming-Harrington weighted logrank test per row}
  
  \item{corr}{a logical; if TRUE (default), return correlation matrix; otherwise, return covariance matrix}
  }
  \value{
  a `tibble` with \code{rg} as input, the FH test statistics specified
  for the data in \code{Z}, and the correlation or covariance matrix for these tests in variables starting
  with \code{V}
  }
  \description{
  Correlations can be used with \code{mvtnorm::pmvnorm} to compute
  p-value for MaxCombo, the maximum of the specifed
  Fleming-Harrington tests
  }
  \examples{
  library(tidyr)
  library(dplyr)
  # Use default enrollment and event rates at cut of 100 events
  x <- simPWSurv(n=200) \%>\% cutDataAtCount(100) \%>\% tensurv(txval="Experimental")
  # compute logrank (FH(0,0)) and FH(0,1)
  x <- tenFHcorr(rg=tibble(rho=c(0,0),gamma=c(0,1)),x=x)
  # compute p-value for MaxCombo
  library(mvtnorm)
  1-pmvnorm(lower=rep(min(x$Z),nrow(x)),corr=data.matrix(select(x,-c(rho,gamma,Z))),
  algorithm=GenzBretz(maxpts=50000,abseps=0.00001))[1]
  # check that covariance is as expected
  x <- simPWSurv(n=200) \%>\%
           cutDataAtCount(100) \%>\%
           tensurv(txval="Experimental")
  x \%>\% tenFHcorr(rg=tibble(rho=c(0,0),gamma=c(0,1)),corr=FALSE)
  # Off-diagonal element should be variance in following
  x \%>\% tenFHcorr(rg=tibble(rho=0,gamma=.5),corr=FALSE)
  # compare off diagonal result with tenFH()
  x \%>\% tenFH(rg=tibble(rho=0,gamma=.5))
  }

Package: simtrial
File: man/tensurv.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/tensurv.R
  \name{tensurv}
  \alias{tensurv}
  \title{Process Survival Data into Counting Process Format}
  \usage{
  tensurv(x, txval)
  }
  \arguments{
  \item{x}{a tibble with no missing values and contain variables
  
      \code{Stratum} (Stratum)
  
      \code{Treatment} (Treatment group)
  
      \code{tte} (Observed time)
  
      \code{event} (Binary event indicator, 1 represents event, 0 represents censoring)}
  
  \item{txval}{value in the input \code{Treatment} column that indicates treatment group value.}
  }
  \value{
  A \code{tibble} grouped by
  \code{Stratum} and sorted within strata by \code{tte}.
  Remain rows with at least one event in the population, at least one subject is at risk in both treatment group and control group.
  Other variables in this represent the following within each stratum at each time at which one or more
  events are observed:
  
  \code{events} (Total number of events)
  
  \code{txevents} (Total number of events at treatment group)
  
  \code{atrisk} (Number of subjects at risk)
  
  \code{txatrisk} (Number of subjects at risk in treatment group)
  
  \code{S} (Left-continuous Kaplan-Meier survival estimate)
  
  \code{OminusE} (In treatment group, observed number of events minus expected number of events.
            The expected number of events is estimated by assuming no treatment effect with hypergeometric distribution with
            parameters total number of events, total number of events at treatment group and number of events at a time.
            (Same assumption of log-rank test under the null hypothesis)
  
  \code{Var} (variance of OminusE under the same assumption).
  }
  \description{
  Produces a tibble that is sorted by stratum and time.
  Included in this is only the times at which one or more event occurs.
  The output dataset contains Stratum, tte (time-to-event), at risk count and count of events at the specified tte
  sorted by Stratum and tte.
  }
  \details{
  The function only considered two group situation.
  
  The tie is handled by the Breslow's Method.
  }
  \examples{
  library(dplyr)
  
  # Example 1
  x=tibble(Stratum = c(rep(1,10),rep(2,6)),
  Treatment = rep(c(1,1,0,0),4),
  tte = 1:16,
  event= rep(c(0,1),8))
  
  tensurv(x, txval=1)
  
  # Example 2
  x <- simPWSurv(n=400)
  y <- cutDataAtCount(x,150) \%>\% tensurv(txval = "Experimental")
  # weighted logrank test (Z-value and 1-sided p-value)
  z <- sum(y$OminusE)/sqrt(sum(y$Var))
  c(z,pnorm(z))
  
  }

Package: simtrial
File: man/wMB.Rd
Format: text
Content:
  % Generated by roxygen2: do not edit by hand
  % Please edit documentation in R/wMB.R
  \name{wMB}
  \alias{wMB}
  \title{Magirr and Burman Modestly Weighted Logrank Tests}
  \usage{
  wMB(x, delay = 4)
  }
  \arguments{
  \item{x}{a \code{tensurv}-class \code{tibble} with a counting process dataset}
  
  \item{delay}{The initial delay period where weights increase; 
  after this, weights are constant at the final weigh in the delay period}
  }
  \value{
  a vector with weights for the Magirr-Burman weighted logrank test
  for the data in \code{x}
  }
  \description{
  Magirr and Burman (2019) have proposed a weighted logrank test to have better power than
  the logrank test when the treatment effect is delayed, but to still maintain good power under 
  a proportional hazards assumption. 
  The weights for some early interval specified by the user are the inverse of the combined treatment group
  empirical survival distribution; see details. 
  After this initial period, weights are constant at the maximum of the previous weights.
  Another advantage of the test is that under strong null hypothesis that the underlying survival in the control group
  is greater than or equal to underlying survival in the experimental group, 
  Type I error is controlled as the specified level.
  }
  \details{
  This function computes Magirr-Burman weights and adds them to a dataset created by the \code{tensurv()} function.
  These weights can then be used to compute a Z-statistic for the modestly weighted logrank test proposed.
  
  
  We define \eqn{t^*} to be the input variable \code{delay}.
  This specifies an initial period during which weights increase.
  To define specific weights, we let \eqn{S(t)} denote the Kaplan-Meier survival estimate at time \eqn{t}
  for the combined data (control plus experimental treatment groups).
  The weight at time \eqn{t} is then defined as
  \deqn{w(t)=S(\min(t,t^*))^{-1}.}
  }
  \examples{
  library(tidyr)
  library(dplyr)
  # Use default enrollment and event rates at cut at 100 events
  x <- simPWSurv(n=200) \%>\% cutDataAtCount(125) \%>\% tensurv(txval="Experimental")
  # compute Magirr-Burman weights with 
  ZMB <- x \%>\% wMB(6) \%>\% 
               summarize(S=sum(OminusE*wMB),V=sum(Var*wMB^2),Z=S/sqrt(V))
  # Compute p-value of modestly weighted logrank of Magirr-Burman
  pnorm(ZMB$Z)
  }
  \references{
  Magirr, Dominic, and CarlâFredrik Burman. 
  "Modestly weighted logrank tests." 
  \emph{Statistics in Medicine} 38.20 (2019): 3782-3790.
  }

Package: simtrial
File: vignettes/arbitraryhazard.Rmd
Format: text
Content:
  ---
  title: "Approximating an arbitrary hazard function"
  output: rmarkdown::html_vignette
  vignette: >
    %\VignetteIndexEntry{Approximating an arbitrary hazard function}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  ```
  
  ```{r setup}
  library(simtrial)
  library(bshazard)
  library(ggplot2)
  library(dplyr)
  library(survival)
  ```
  
  We simulate a log-logistic distribution as an example of how to simulate a trial with an arbitrary distribution.
  We begin by showing hazard rates that can be used to approximate this distribution.
  
  ```{r,fig.height=4,fig.width=7.5}
  dloglogis <- function(x, alpha = 1, beta = 4){
    1 / (1 + (x/alpha)^beta)
  }
  times <- (1:150) / 50
  xx <- tibble(Times=times,Survival=dloglogis(times,alpha=.5,beta=4)) %>%
            mutate(duration = Times-lag(Times,default=0),
                   H = -log(Survival),
                   rate = (H-lag(H,default=0)) / duration / 3
                   ) %>%
            select(duration,rate)
  ggplot(data = xx %>% mutate(Time=lag(cumsum(duration),default=0)), aes(x=Time,y=rate))+geom_line()
  ```
  
  We assume the time scale above is in years and that enrollment occurs over the first half year at an even rate of 500 per year.
  We assume that observations are censored at an exponential rate of about 5% per year.
  
  ```{r,fig.height=4,fig.width=7.5}
  tx <- "Log-logistic"
  enrollRates <- tibble(duration = .5, rate = 500)
  dropoutRates <- tibble(Treatment = tx, duration = 3, rate = .05, period = 1, Stratum = "All")
  block <- rep(tx,2)
  x <- simPWSurv(n=250, # sample size
                 block = block,
                 enrollRates = enrollRates,
                 failRates = xx %>% mutate(Stratum="All", Treatment=tx, period=1:n(), Stratum = "All"),
                 dropoutRates = dropoutRates)
  ```
  
  
  We assume the entire study lasts 3 years
  
  
  ```{r,fig.height=4,fig.width=7.5}
  y <- x %>% cutData(3)
  head(y)
  ```
  
  Now we estimate a Kaplan-Meier curve.
  
  ```{r,fig.height=4,fig.width=7.5}
  fit <- survfit(Surv(tte, event) ~ 1, data = y)
  plot(fit, mark="|")
  ```
  
  Finally, we plot the estimated hazard rate and its confidence interval as a function of time.
  We overlay the actual rates in red.
  
  ```{r,fig.height=4,fig.width=7.5}
  fit <- bshazard(Surv(tte, event) ~ 1, data = y, nk = 120)
  plot(fit,conf.int=TRUE,xlab='Time',xlim=c(0,3),ylim=c(0,2.5),lwd=2)
  lines(x=times,y=(xx %>% mutate(Time=lag(cumsum(duration),default=0)))$rate,col=2)
  ```
  

Package: simtrial
File: vignettes/modestWLRTVignette.Rmd
Format: text
Content:
  ---
  title: Using the Magirr-Burman weights for testing
  output: rmarkdown::html_vignette
  bibliography: simtrial.bib
  vignette: >
    %\VignetteIndexEntry{Magirr-Burman weights}
    %\VignetteEngine{knitr::rmarkdown} 
  ---
  
  ```{r setup, include = FALSE,message=FALSE,warning=FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  
  options( width = 58 )
  
  ```
  
  # Introduction
  
  @MagirrBurman implemented a modestly weighted logrank test with the following claim: ``Tests from this new class can be constructed to have high power under a delayed-onset treatment effect scenario, as well as being almost as efficient as the standard logrank test under proportional hazards.''
  They have implemented this in the package \strong{modestWLRT} available at Github.com.
  Since the implementation is relatively straightforward, we have added this functionality to the \strong{simtrial} package and explain how to use it here with the `wMB()` function.
  
  We consider two examples:
  
  
  - A single stratum example where we compare results with the \strong{modestWLRT} package.
  - A stratified example which was not implemented in \strong{modestWLRT}.
  
  
  Packages used are as follows:
  
  ```{r, message=FALSE, warning=FALSE}
  library(simtrial)
  library(dplyr)
  library(survival)
  ```
  
  
  
  # Single stratum examples
  
  ## Magirr and Burman delayed effect example
  
  First, we specify study duration, sample size and enrollment rates. The enrollment rate is assumed constant during the enrollment period until the targeted sample size is reached.
  For failure rates, we consider the delayed treatment effect example of Magirr and Burman (2019).
  The control group has an exponential failure rate with a median of 15 months. 
  For the initial 6 months, the underlying hazard ratio is one followed by a hazard ratio of 0.7 thereafter.
  This differs from the Magirr and Burman (2019) delayed effect assumptions only in that they assume a hazard ratio of 0.5 after 6 months.
  
  
  ```{r, message=FALSE, warning=FALSE}
  studyDuration = 36
  sampleSize <- 200
  enrollRates <- tibble::tibble(duration = 12, rate = 200/12)
  failRates <- tibble::tribble(
    ~Stratum,  ~duration, ~failRate, ~hr, ~dropoutRate,
    "All",     6,         log(2)/15,  1,  0,
    "All",     36,        log(2)/15,  .7, 0
  )
  ```
  
  Now we generate a single dataset with the above characteristics and cut data for analysis at 36 months post start of enrollment.
  Then we plot Kaplan-Meier curves for the resulting dataset (red curve for experimental treatment, black for control):
  
  ```{r, message=FALSE,warning=FALSE,fig.width=7.5, fig.height=4}
  set.seed(7783)
  xpar <- simfix2simPWSurv(failRates)
  MBdelay <- simPWSurv(n = sampleSize, 
                       strata = tibble::tibble(Stratum = "All", p = 1),
                       block = c(rep("Control", 2), rep("Experimental", 2)),
                       enrollRates = enrollRates,
                       failRates = xpar$failRates, 
                       dropoutRates = xpar$dropoutRates) %>% 
             cutData(studyDuration)
  fit <- survfit(Surv(tte,event)~Treatment,data=MBdelay)
  plot(fit,col=1:2,mark="|", xaxt="n")
  axis(1, xaxp=c(0, 36, 6))
  ```
  
  We perform a logrank and weighted logrank tests as follows:
  
  ```{r, message=FALSE, warning=FALSE}
  xx <- MBdelay %>% 
      tensurv(txval="Experimental") %>%
      tenFHcorr(rg=tibble(rho=c(0,0,1),gamma=c(0,1,1))) %>%
      mutate(p=pnorm(Z))
  xx
  ```
  
  Now for a MaxCombo test with the above compoenent tests, we have  p-value of
  
  ```{r, message=FALSE, warning=FALSE}
  xx %>% pMaxCombo()
  ```
  
  Next, we consider the Magirr and Burman (2019) modestly weighted logrank test with down-weighting specifid for the first 6 months.
  This requires generating weights and then computing the test.
  
  ```{r}
  ZMB <-  MBdelay %>% 
          tensurv(txval="Experimental") %>% 
          wMB(6) %>% 
          summarize(S=sum(OminusE*wMB),V=sum(Var*wMB^2),Z=S/sqrt(V))
  # Compute p-value of modestly weighted logrank of Magirr-Burman
  pnorm(ZMB$Z)
  ```
  
  Finally, we consider weighted logrank tests with less down-weighting.
  Results are quite similar to the results with greater down-weighting.
  
  ```{r}
  xx <- MBdelay %>% 
      tensurv(txval="Experimental") %>%
      tenFHcorr(rg=tibble(rho=c(0,0,.5),gamma=c(0,.5,.5))) %>%
      mutate(p=pnorm(Z))
  xx
  ```
  
  Check vs tenFH().
  
  ```{r}
  xx <- MBdelay %>% 
      tensurv(txval="Experimental") %>%
      tenFH(rg=tibble(rho=c(0,0,.5),gamma=c(0,.5,.5))) 
  xx
  ```
  
  ```{r}
  xx <- MBdelay %>% 
      tensurv(txval="Experimental") %>%
      tenFHcorr(rg=tibble(rho=c(0,0,.5,.5),gamma=c(0,.5,.5,0))) %>%
      mutate(p=pnorm(Z))
  xx
  ```
  
  
  Now for a MaxCombo test with the above compoenent tests, we have  p-value of
  
  ```{r}
  xx %>% pMaxCombo()
  ```
  
  ## Freidlin and Korn strong null hypothesis example
  
  The underlying survival is worse for the experimental group is uniformly worse for the experimental group until the very end of the study.
  This was presented by @FKNPH2019. For this case, we have a hazard ratio of 16 for 1/10 of 1 year (1.2 months), followed by a hazard ratio of 
  
  First, we specify study duration, sample size and enrollment rates. The enrollment rate is assumed constant during the enrollment period until the targeted sample size is reached.
  For failure rates, we consider the delayed treatment effect example of Magirr and Burman (2019).
  The control group has an exponential failure rate with a median of 15 months. 
  For the initial 6 months, the underlying hazard ratio is one followed by a hazard ratio of 0.7 thereafter.
  This differs from the Magirr and Burman (2019) delayed effect assumptions only in that they assume a hazard ratio of 0.5 after 6 months.
  
  ```{r, message=FALSE, warning=FALSE}
  studyDuration = 5
  sampleSize <- 2000
  enrollDuration <- .0001
  enrollRates <- tibble::tibble(duration = enrollDuration, rate = sampleSize/enrollDuration)
  failRates <- tibble::tibble(Stratum="All",
                   failRate=0.25,
                   dropoutRate=0,
                   hr=c(4/.25,.19/.25),
                   duration=c(.1,4.9)
  )
  ```
  
  Now we generate a single dataset with the above characteristics and cut data for analysis at 36 months post start of enrollment.
  Then we plot Kaplan-Meier curves for the resulting dataset (red curve for experimental treatment, black for control):
  
  ```{r, message=FALSE,warning=FALSE,fig.width=7.5, fig.height=4}
  set.seed(7783)
  xpar <- simfix2simPWSurv(failRates)
  FHwn <- simPWSurv(n = sampleSize, 
                       strata = tibble::tibble(Stratum = "All", p = 1),
                       block = c(rep("Control", 2), rep("Experimental", 2)),
                       enrollRates = enrollRates,
                       failRates = xpar$failRates, 
                       dropoutRates = xpar$dropoutRates) %>% 
             cutData(studyDuration)
  fit <- survfit(Surv(tte,event)~Treatment,data=FHwn)
  plot(fit,col=1:2,mark="|", xaxt="n")
  axis(1, xaxp=c(0, 36, 6))
  ```
  
  We perform a logrank and weighted logrank tests as follows:
  
  ```{r}
  xx <- FHwn %>% 
      tensurv(txval="Experimental") %>%
      tenFHcorr(rg=tibble(rho=c(0,0,1),gamma=c(0,1,1))) %>%
      mutate(p=pnorm(Z))
  xx
  ```
  
  Now for a MaxCombo test with the above compoenent tests, we have  p-value of
  
  ```{r}
  xx %>% pMaxCombo()
  ```
  
  Next, we consider the Magirr and Burman (2019) modestly weighted logrank test with down-weighting specifid for the first 6 months.
  This requires generating weights and then computing the test.
  
  ```{r}
  ZMB <-  FHwn %>% 
          tensurv(txval="Experimental") %>% 
          wMB(6) %>% 
          summarize(S=sum(OminusE*wMB),V=sum(Var*wMB^2),Z=S/sqrt(V))
  # Compute p-value of modestly weighted logrank of Magirr-Burman
  pnorm(ZMB$Z)
  ```
  
  Finally, we consider weighted logrank tests with less down-weighting.
  Results are quite similar to the results with greater down-weighting.
  
  ```{r}
  xx <- FHwn %>% 
      tensurv(txval="Experimental") %>%
      tenFHcorr(rg=tibble(rho=c(0,0,.5),gamma=c(0,.5,.5))) %>%
      mutate(p=pnorm(Z))
  xx
  ```
  
  Now for a MaxCombo test with the above compoenent tests, we have  p-value of
  
  ```{r}
  xx %>% pMaxCombo()
  ```
  
  
  
  # References

Package: simtrial
File: vignettes/pMaxComboVignette.Rmd
Format: text
Content:
  ---
  title: Computing p-values for Fleming-Harring weighted logrank tests and the MaxCombo test
  date: "`r Sys.Date()`"
  output: rmarkdown::html_vignette
  bibliography: simtrial.bib
  vignette: >
    %\VignetteIndexEntry{MaxCombo p-value computation}
    %\VignetteEngine{knitr::rmarkdown} 
  ---
  
  ```{r setup, include = FALSE,message=FALSE,warning=FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  
  ```
  
  # Introduction
  
  This vignette demonstrates use of a simple routine to do simulations and testing using Fleming-Harrington weighted logrank tests and the MaxCombo test.
  In addition, we demonstrate how to perform these tests with a dataset not generated by simulation routines within the package.
  Note that all p-values computed here are one-sided with small values indicating that the experimental treatment is favored. 
  
  # Defining the test
  
  The MaxCombo test has been posed as the maximum of multiple Fleming-Harrington weighted logrank tests (@FH1982, @FH2011).
  Combination tests looking at a maximum of selected tests in this class have also been proposed; see @Lee2007, @NPHWGDesign, and @NPHWGSimulation.
  The Fleming-Harrington class is indexed by the parameters $\rho\ge 0$ and $\gamma\ge 0$.
  We will denote these as FH($\rho,\gamma$). 
  This class includes the logrank test as FH(0,0).
  Other tests of interest here include:
  
  - FH(0,1): a test that down-weights early events
  - FH(1,0): a test that down-weights late events
  - FH(1,1): a test that down-weights events increasingly as their quantiles differ from the median
  
  
  # Executing for a single dataset
  
  ## Generating test statistics with `simfix()`
  
  We begin with a single trial simulation generated by the routine `simfix()` using default arguments for that routine.
  `simfix()` produces one record per test and data cutoff method per simulation.
  Here we choose 3 tests (logrank=FH(0,0), FH(0,1) and FH(1,1)).
  When more than one test is chosen the correlation between tests is computed as shown by @Karrison2016, in this case in the columns `V1, V2, V3`. The columns `rho, gamma` indicate $\rho$ and $\gamma$ used to compute the test. `Z` is the FH($\rho,\gamma$) normal test statistic with variance 1 with a negative value favoring experimental treatment. The variable `cut` indicates how the data were cut for analysis, in this case at the maximum of the targeted minimum follow-up after last enrollment and the date at which the targeted event count was reached. `Sim` is a sequential index of the simulations performed.  
  
  
  ```{r,message=FALSE,warning=FALSE}
  library(simtrial)
  library(knitr)
  library(dplyr)
  x <- simfix(nsim=1,timingType=5,rg=tibble::tibble(rho=c(0,0,1),gamma=c(0,1,1)))
  x %>% kable(digits=2)
  ```
  
  Once you have this format, the MaxCombo p-value per @Karrison2016, @NPHWGDesign can be computed as follows (note that you will need to have the package **mvtnorm** installed):
  
  ```{r,warning=FALSE,message=FALSE}
  pMaxCombo(x)
  ```
  
  ## Generating data with `simtrial::simPWSurv()`
  
  We begin with another simulation generated by `simtrial::simPWSurv()`.
  Again, we use defaults for that routine.
  
  ```{r,message=FALSE,warning=FALSE,cache=FALSE}
  s <- simPWSurv(n=100)
  head(s) %>% kable(digits=2)
  ```
  
  Once generated, we need to cut the data for analysis. Here we cut after 75 events.
  
  ```{r,warning=FALSE,message=FALSE}
  x <- s %>% cutDataAtCount(75)
  head(x) %>% kable(digits=2)
  ```
  
  Now we can analyze this data. We begin with `s` to show how this can be done in a single line.
  In this case, we use the 4 test combination suggested in @NPHWGSimulation, @NPHWGDesign.
  
  ```{r,warning=FALSE,message=FALSE}
  Z <- s %>% cutDataAtCount(75) %>% 
             tensurv(txval="Experimental") %>% 
             tenFHcorr(rg=tibble(rho=c(0,0,1,1),gamma=c(0,1,0,1)))
  Z %>% kable(digits=2)
  ```
  
  Now we compute our p-value as before:
  
  ```{r,warning=FALSE,message=FALSE}
  pMaxCombo(Z)
  ```
  
  Suppose we want the p-value just based on the logrank and FH(0,1) and FH(1,0) as suggested by @Lee2007.
  We remove the rows and columns associated with FH(0,0) and FH(1,1) and then apply `pMaxCombo()`.
  
  ```{r,warning=FALSE,message=FALSE}
  pMaxCombo(Z %>% select(-c(V1,V4)) %>% filter((rho==0 & gamma==1) | (rho==1 & gamma==0)))
  ```
  
  
  
  ## Using survival data in another format
  
  For a trial not generated by `simfix()`, the process is slightly more involved.
  We consider survival data not in the **simtrial** format and show the transformation needed.
  In this case we use the small `aml` dataset from the **survival** package.
  
  ```{r,warning=FALSE,message=FALSE}
  library(survival)
  head(aml) %>% kable()
  ```
  
  We rename variables and create a stratum variable as follows:
  
  ```{r,warning=FALSE,message=FALSE}
  x <- aml %>% transmute(tte=time,event=status,Stratum="All",Treatment=as.character(x))
  head(x) %>% kable()
  ```
  
  Now we analyze the data with a MaxCombo with the logrank and FH(0,1) and compute a p-value.
  
  ```{r,warning=FALSE,message=FALSE}
  x %>% 
    tensurv(txval="Maintained") %>% 
    tenFHcorr(rg=tibble(rho=0,gamma=c(0,1))) %>% 
    pMaxCombo()
  ```
  
  
  
  # Simulation
  
  We now consider the example simulation from the `pMaxCombo()` help file to demonstrate how to simulate power for the MaxCombo test. However, we increase the number of simulations to 100 in this case; a larger number should be used (e.g., 1000) for a better estimate of design properties. Here we will test at the $\alpha=0.001$ level.
  
  ```{r,cache=FALSE,warning=FALSE,message=FALSE}
  # Only use cut events + min follow-up
  xx <- simfix(nsim=100,timingType=5,rg=tibble(rho=c(0,0,1),gamma=c(0,1,1)))
  # MaxCombo power estimate for cutoff at max of targeted events, minimum follow-up
  p <- unlist(xx %>%  group_by(Sim) %>% group_map(pMaxCombo))
  mean(p<.001)
  ```
  
  We note the use of `group_map` in the above produces a list of p-values for each simulation.
  It would be nice to have something that worked more like `dplyr::summarize()` to avoid `unlist()` and to allow evaluating, say, multiple data cutoff methods.
  The latter can be done without having to re-run all simulations as follows, demonstrated with a smaller number of simulations.
  
  ```{r,cache=FALSE,warning=FALSE,message=FALSE}
  # Only use cuts for events and events + min follow-up
  xx <- simfix(nsim=100,timingType=c(2,5),rg=tibble(rho=0,gamma=c(0,1)))
  head(xx) %>% kable(digits=2)
  ```
  
  Now we compute a p-value separately for each cut type, first for targeted event count.
  
  ```{r,warning=FALSE,message=FALSE}
  # subset to targeted events cutoff tests
  p <- unlist(xx %>% filter(cut=="Targeted events") %>% group_by(Sim) %>% group_map(pMaxCombo))
  mean(p<.025)
  ```
  
  Now we use the later of targeted events and minimum follow-up cutoffs.
  
  ```{r,warning=FALSE,message=FALSE}
  # subset to targeted events cutoff tests
  p <- unlist(xx %>% filter(cut!="Targeted events") %>% group_by(Sim) %>% group_map(pMaxCombo))
  mean(p<.025)
  ```
  
  # References

Package: simtrial
File: vignettes/simtrial.bib
Format: text
Content:
  @article{ChenCCS,
    title={Multiplicity for a Group Sequential Trial with Biomarker Subpopulations},
    author={Ting-Yu Chen, Jing Zhao, Linda Sun, Keaven Anderson},
    pubstate = {submitted},
    year={2019}
  }
  @article{ding2016subgroup,
    title={Subgroup mixable inference on treatment efficacy in mixture populations, with an application to time-to-event outcomes},
    author={Ding, Ying and Lin, Hui-Min and Hsu, Jason C},
    journal={Statistics in medicine},
    volume={35},
    number={10},
    pages={1580--1594},
    year={2016},
    publisher={Wiley Online Library}
  }
  @article{Bretz2009,
    author="Frank Bretz and Willi Maurer and Martin Posch",
    title="A graphical approach to sequentially rejective multiple test procedures",
    journal="Statistics in Medicine",
    year="2009",
    volume="28",
    pages="586-604",
    doi="10.1002/sim.3495"
  }
  @ARTICLE{MaurerBretz2013,
  author="Willi Maurer and Frank Bretz",
  title="Multiple testing in group sequential trials using graphical approaches",
  journal="Statistics in Biopharmaceutical Research",
  volume="5",
  year="2013",
  pages="311-320",
  DOI="10.1080/19466315.2013.807748"}
  @Manual{gMCP,
      title = {{gMCP}: Graph Based Multiple Test Procedures},
      author = {Kornelius Rohmeyer and Florian Klinglmueller},
      year = {2018},
      note = {R package version 0.8-14},
      url = {https://cran.r-project.org/package=gMCP},
    }
    @Article{Bretz2011,
      title = {Graphical approaches for multiple comparison procedures
        using weighted Bonferroni, Simes or parametric tests},
      author = {Frank Bretz and Martin Posch and Ekkehard Glimm and
        Florian Klinglmueller and Willi Maurer and Kornelius Rohmeyer},
      journal = {Biometrical Journal},
      year = {2011},
      publisher = {Wiley},
      volume = {53},
      number = {6},
      pages = {894--913},
      url =
        {http://onlinelibrary.wiley.com/doi/10.1002/bimj.201000239/full},
    }
  @ARTICLE{LachinFoulkes,
  	author = "Lachin, John M. and Foulkes, Mary A.",
  	title = "Evaluation of sample size and power for analyses of survival with allowance for nonuniform patient entry, losses to follow-up, noncompliance, and stratification.",
  	journal = "Biometrics",
  	volume = "42",
  	year = "1986",
  	pages = "507-519"}
  @ARTICLE{Karrison2016,
      title = {Versatile tests for comparing survival curves based on weighted log-rank statistics},
      author = {Theodore G Karrison},
      journal = {Stata Journal},
      year = {2016},
      publisher = {Stata Corp},
      volume = {15},
      number = {3},
      pages = {678-690}
  }
  
  @article{Hasegawa2014,
    title={Sample size determination for the weighted log-rank test with the Fleming--Harrington class of weights in cancer vaccine studies},
    author={Hasegawa, Takahiro},
    journal={Pharmaceutical statistics},
    volume={13},
    number={2},
    pages={128--135},
    year={2014},
    publisher={Wiley Online Library}
  }
  
  @article{Xi2017,
    title={A unified framework for weighted parametric multiple test procedures},
    author={Xi, Dong and Glimm, Ekkehard and Maurer, Willi and Bretz, Frank},
    journal={Biometrical Journal},
    year={2017},
    publisher={Wiley Online Library}
  }
  
  @ARTICLE{Tsiatis,
  	author="Anastasios A. Tsiatis",
  	title="Repeated significance testing for a general class of statistics use in censored survival analysis.",
  	journal="Journal of the American Statistical Association",
  	volume="77",
  	pages="855-861",
  	year="1982"}
  
  @ARTICLE{Schemper2009,
    title="The estimation of average hazard ratios by weighted Cox regression",
    author="Schemper, Michael and Wakounig, Samo and Heinze, Georg",
    journal="Statistics in medicine",
    volume="28",
    number="19",
    pages="2473--2489",
    year="2009"
  }
  
  @ARTICLE{Kalbfleisch1981,
    title={Estimation of the average hazard ratio},
    author={Kalbfleisch, John D and Prentice, Ross L},
    journal={Biometrika},
    volume={68},
    number={1},
    pages={105--112},
    year={1981}
  }
  
  @UNPUBLISHED{NPHWGDesign,
    title={Robust design and analysis of clinical trials with non-proportional hazards: a straw man guidance from a cross-pharma working group},
    author={Satrajit Roychoudhury and Keaven M. Anderson and Jiabu Ye and Pralay Mukhopadhyay},
    note={Submitted for publication},
    year={2019}
  }
  
  @UNPUBLISHED{NPHWGSimulation,
    title={Alternative analysis methods for time to event endpoints under non-proportional hazards: a comparative analysis},
    author={Ray S. Lin and Ji Lin and Satrajit Roychoudhury and Keaven M. Anderson and Tianle Hu and Bo Huang and Larry F Leon and Jason J.Z. Liao and Rong Liu and Xiaodong Luo and Pralay Mukhopadhyay and Rui Qin and Kay Tatsuoka and Xuejing Wang and Yang Wang and Jian Zhu and Tai-Tsang Chen and Renee Iacona},
    note={Submitted for publication},
    year={2019}
  }
  
  @article{Karrison2016,
    title={Versatile tests for comparing survival curves based on weighted log-rank statistics},
    author={Karrison, Theodore G},
    journal={The Stata Journal},
    volume={16},
    number={3},
    pages={678--690},
    year={2016},
    publisher={SAGE Publications Sage CA: Los Angeles, CA}
  }
  
  @article{FH1982,
    title={A class of rank test procedures for censored survival data},
    author={Harrington, David P and Fleming, Thomas R},
    journal={Biometrika},
    volume={69},
    number={3},
    pages={553--566},
    year={1982},
    publisher={Oxford University Press}
  }
  
  @book{FH2011,
    title={Counting processes and survival analysis},
    author={Fleming, Thomas R and Harrington, David P},
    volume={169},
    year={2011},
    publisher={John Wiley \& Sons}
  }
  
  @article{Lee2007,
    title={On the versatility of the combination of the weighted log-rank statistics},
    author={Lee, Seung-Hwan},
    journal={Computational Statistics \& Data Analysis},
    volume={51},
    number={12},
    pages={6557--6564},
    year={2007},
    publisher={Elsevier}
  }
  @article{MagirrBurman,
    title={Modestly weighted logrank tests},
    author={Magirr, Dominic and Burman, Carl-Fredrik},
    journal={Statistics in medicine},
    volume={38},
    number={20},
    pages={3782--3790},
    year={2019},
    publisher={Wiley Online Library}
  }
  @article{FKNPH2019,
    title={Methods for Accommodating Nonproportional Hazards in Clinical Trials: Ready for the Primary Analysis?},
    author={Freidlin, Boris and Korn, Edward L},
    journal={Journal of Clinical Oncology},
    volume={37},
    number={35},
    pages={3455--3459},
    year={2019},
    publisher={American Society of Clinical Oncology}
  }

Package: simtrial
File: vignettes/simtrialroutines.Rmd
Format: text
Content:
  ---
  title: "Basic tools for time-to-event trial simulation and testing"
  author: "Keaven Anderson"
  date: "`r Sys.Date()`"
  output: rmarkdown::html_vignette
  bibliography: simtrial.bib
  vignette: >
    %\VignetteIndexEntry{Basic tools for time-to-event trial simulation and testing}
    %\VignetteEngine{knitr::rmarkdown}
  ---
  
  ```{r setup, include = FALSE}
  knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>"
  )
  ```
  
  ## Overview
  
  This vignette demonstrates the lower-level routines in the simtrial package specifically related to trial generation and statistical testing.
  
  The routines are as follows:
  
  * `fixedBlockRand` - fixed block randomization
  * `rpwenroll` - random inter-arrival times with piecewise constant enrollment rates
  * `rpwexp` - piecewise exponential failure rate generation
  * `cutData` - cut data for analysis at a specified calendar time
  * `cutDataAtCount` - cut data for analysis at a specified event count, including ties on the cutoff date
  * `getCutDateForCount` - find date at which an event count is reached
  * `tensurv` - pre-process survival data into a counting process format
  
  Application of the above is demonstrated using higher-level routines `simPWSurv()` and `simfix()` to generate simulations and weighted logrank analysis for a stratified design.
  
  The intent has been to write these routines in the spirit of the tidyverse approach (alternately referred to as data wrangling, tidy data, R for Data Science, or split-apply-combine). 
  The other objectives are to have an easily documentable and validated package that is easy to use and efficient as a broadly-useful tool for simulation of time-to-event clinical trials.
  
  The package could be extended in many ways in the future, including:
  
  * Other analyses not supported in the survival package or other acceptably validated package
      + Weighted logrank and weighted Kaplan-Meier analyses
      + One-step, hazard ratio estimator (first-order approximation of PH)
  * Randomization schemes other than stratified, fixed-block
  * Poisson mixture or other survival distribution generation
  
  ```{r, warning=FALSE, message=FALSE}
  library(simtrial)
  library(knitr)
  library(tibble)
  library(dplyr)
  ```
  
  
  ## Randomization
  
  Fixed block randomization with an arbitrary block contents is performed as demonstrated below. In this case we have a block size of 5 with one string repeated twice in each block and three other strings appearing once each.
  
  ```{r}
  fixedBlockRand(n=10,block=c("A","Dog","Cat","Cat"))
  ```
  More normally, with a default of blocks of size four:
  
  ```{r}
  fixedBlockRand(n=20)
  ```
  
  
  ## Enrollment
  
  Piecewise constant enrollment can be randomly generated as follows. Note that duration is specifies interval durations for constant rates; the final rate is extended as long as needed to generate the specified number of observations.
  
  ```{r}
  rpwenroll(n = 20, enrollRates = tibble(duration = c(1, 2), 
                                         rate = c(2,5)))
  ```
  
  
  ## Time-to-event and time-to-dropout
  
  Time-to-event and time-to-dropout random number generation for observations is generated with piecewise exponential failure times. For a large number of observations, a log-plot of the time-to-failure 
  
  ```{r,fig.width=6}
  x <- rpwexp(10000, 
              failRates=tibble(rate = c(1, 3, 10), 
                               duration = c(.5,.5,1)))
  plot(sort(x),(10000:1)/10001,log="y", 
       main="PW Exponential simulated survival curve",
       xlab="Time", ylab="P{Survival}")
  ```
  
  
  ## Generating a trial
  
  Ideally, this might be done with a routine where generation of randomization, and time-to-event data could be done in a modular fashion plugged into a general trial generation routine.
  For now, stratified randomization, piecewise constant enrollment, fixed block randomization and piecewise exponential failure rates support a flexible set of trial generation options for time-to-event endpoint trials.
  At present, follow this format very carefully as little checking of input has been developed to-date.
  The methods used here have all be demonstrated above, but here they are combined in a single routine to generate a trial. Note that in the generated output dataset, `cte` is the calendar time of an event or dropout, whichever comes first, and `fail` is an indicator that `cte` represents an event time.  
  
  First we set up input variables to make the later call to `simPWSurv()` more straightforward to read.
  
  ```{r}
  strata <- tibble(Stratum=c("Negative","Positive"), p=c(.5,.5))
  
  block <- c(rep("Control",2),rep("Experimental",2))
  
  enrollRates <- tibble(rate=c(3, 6, 9), duration=c(3,2,1))
  
  failRates <- tibble(Stratum=c(rep("Negative",4),rep("Positive",4)),
                      period=rep(1:2,4),
                      Treatment=rep(c(rep("Control",2), rep("Experimental",2)),2),
                      duration=rep(c(3,1),4),
                      rate=log(2)/c(4,9,4.5,10,4,9,8,18))
  dropoutRates <- tibble(Stratum=c(rep("Negative",4),rep("Positive",4)),
                         period=rep(1:2,4),
                         Treatment=rep(c(rep("Control",2), rep("Experimental",2)),2),
                         duration=rep(c(3,1),4),
                         rate=rep(c(.001,.001),4))
  ```
  
  
  ```{r}
  x <- simPWSurv(n=400,
                strata = strata,
                block = block,
                enrollRates = enrollRates,
                failRates=failRates,
                dropoutRates=dropoutRates)
  head(x) %>% kable(digits=2)
  ```
  
  
  ## Cutting data for analysis
  
  There two ways to cut off data in the generated dataset `x` from above. 
  The first uses a calendar cutoff date. The output only includes the time from randomization to event or dropout (`tte`), and indicator that this represents and event (`event`), the stratum in which the observation was generated (`Stratum`) and the treatment group assigned (`Treatment`).
  Observations enrolled after the input `cutDate` are deleted and events and censoring from `x` that are after the `cutDate` are censored at the specified `cutDate`. 
  
  ```{r}
  y <- cutData(x,cutDate=5)
  head(y) %>% kable(digits=2)
  ```
  
  For instance, if we wish to cut the entire dataset when 50 events are observed in the Positive stratum we can use the `getCutDateForCount` function as follows:
  
  ```{r}
  cut50Positive <- getCutDateForCount(filter(x,Stratum=="Positive"),50)
  y50Positive <- cutData(x,cut50Positive)
  with(y50Positive,table(Stratum,event))
  ```
  
  Perhaps the most common way to cut data is with an event count for the overall population, which is done using the `cutDataAtCount` function. Note that if there are tied events at the date the `cte` the count is reached, all are included. Also, if the count is never reached, all event times are included in the cut - with no indication of an error.
  
  ```{r}
  y150 <- cutDataAtCount(x,150)
  table(y150$event,y150$Treatment)
  ```
  
  
  ## Generating a counting process dataset
  
  Once we have cut data for analysis, we can create a dataset that is very simple to use for weighted logrank tests. A slightly more complex version could be developed in the future to enable Kaplan-Meier-based tests. We take the dataset `y150` from above and process it into this format.
  The counting process format is further discussed in the next section where we compute a weighted logrank test.
  
  ```{r}
  ten150 <- tensurv(y150,txval="Experimental")
  head(ten150) %>% kable(digits=2)
  ```
  
  ## Logrank and weighted logrank testing
  
  Now stratified logrank and stratified weighted logrank tests are easily generated based on the counting process format. Each record in the counting process dataset represents a `tte` at which one or more events occurs; the results are stratum-specific. Included in the observation is the number of such events overall (`events`) and in the experimental treatment group (`txevents`), the number at risk overall (`atrisk`) and in the experimental treatment group (`txatrisk`) just before `tte`, the combined treatment group Kaplan-Meier survival estimate (left-continuous) at `tte`, the observed events in experimental group minus the expected at `tte` based on an assumption that all at risk observations are equally likely to have an event at any time, and the variance for this quantity (`Var`).
  
  To generate a stratified logrank test and a corresponding one-sided p-value, we simply do the following:
  
  ```{r}
  z <- with(ten150,sum(OminusE)/sqrt(sum(Var)))
  c(z,pnorm(z))
  ```
  
  A Fleming-Harrington $\rho=1$, $\gamma=2$ is nearly as simple.
  We again compute a z-statistic and its corresponding one-sided p-value.
  
  ```{r}
  xx <- mutate(ten150,w=S*(1-S)^2)
  z <- with(xx,sum(OminusE*w)/sum(sqrt(Var*w^2)))
  c(z,pnorm(z))
  ```
  
  For Fleming-Harrington tests, a routine has been built to do these tests for you:
  
  ```{r}
  tenFH(x=ten150,rg=tibble(rho=c(0,0,1,1),gamma=c(0,1,0,1))) %>% kable(digits=2)
  ```
  
  If we wanted to take the minimum of these for a MaxCombo test, we would first use `tenFHcorr` to compute a correlation matrix for the above Z-statistics as follows.
  Note that the ordering of `rg` and `g` in the argument list is opposite of the above.
  The correlation matrix for the `Z`-values is now in `V1`-`V4`.
  
  ```{r,message=FALSE}
  x <- ten150 %>% tenFHcorr(rg=tibble(rho=c(0,0,1,1),gamma=c(0,1,0,1)))
  x %>% kable(digits=2)
  ```
  
  We can compute a p-value for the MaxCombo as follows using the `pmvnorm` function from the `mvtnorm` package. Note the arguments for `GenzBretz` which are more stringent than the defaults; we have also used these more stringent parameters in the example in the help file.
  
  ```{r,message=FALSE}
  # compute p-value for MaxCombo
  pMaxCombo(x)
  ```
  
  ## Simplification for 2-arm trials
  
  The `simfix()` routine combines much of the above to go straight to generating tests for individual trials so that cutting data and analyzing do not need to be done separately.
  Here the argument structure is meant to be simpler than for `simPWSurv()`.
  
  ```{r}
  strata <- tibble(Stratum = "All", p = 1)
  enrollRates <- tibble(duration = c(2, 2, 10), 
                        rate = c(3, 6, 9)
                       ) 
  failRates <- tibble(Stratum = "All", 
                      duration = c(3, 100),
                      failRate = log(2)/c(9, 18), 
                      hr = c(0.9, 0.6), 
                      dropoutRate = rep(0.001, 2)
                     )
  block <- rep(c("Experimental", "Control"), 2)
  rg <- tibble(rho = 0, gamma = 0)
  ```
  
  Now we simulate a trial 2 times and cut data for analysis based on `timingType = 1:5` which translates to: 
  
  1) the planned study duration, 
  2) targeted event count is achieved, 
  3) planned minimum follow-up after enrollment is complete,
  4) the maximum of 1 and 2, 
  5) the maximum of 2 and 3.
  
  ```{r}
  simfix(nsim = 2,                  # Number of simulations
         sampleSize = 500,          # Trial sample size
         targetEvents = 350,        # Targeted events at analysis
         strata = strata,           # Study strata
         enrollRates = enrollRates, # Enrollment rates
         failRates = failRates,     # Failure rates
         totalDuration = 30,        # Planned trial duration 
         block = block,             # Block for treatment
         timingType = 1:5,          # Use all possible data cutoff methods
         rg = rg                    # FH test(s) to use; in this case, logrank
  ) %>% kable(digits=2)
  ```
  
  If you look carefully, you should be asking why the cutoff with the planned number of events is so different than the other data cutoff methods.
  To explain, we note that generally you will want `sampleSize` above to match the enrollment specified in `enrollRates`:
  
  ```{r}
  enrollRates %>% summarize("Targeted enrollment based on input enrollment rates" = sum(duration * rate))
  ```
  
  
  The targeted enrollment takes, on average, 30 months longer than the sum of the enrollment durations in `enrollRates`  (14 months) at the input enrollment rates. To achieve the input `sampleSize` of 500, the final enrollment rate is assumed to be steady state and extends in each simulation until the targeted enrollment is achieved. The planned duration of the trial is taken as 30 months as specified in `totalDuration`. The targeted minimum follow-up is
  
  ```{r}
  totalDuration <- 30 # from above
  totalDuration - sum(enrollRates$duration)
  ```
  It is thus, implicit that the last subject was enrolled 16 months prior to the duration given for the cutoff with "Minimum follow-up" cutoff in the simulations above.
  The planned duration cutoff is given in the 'totalDuration' argument which results in a much earlier cutoff.
  
  

Package: simtrial
File: data/Ex1delayedEffect.rda
Format: binary
Content:
  425a683931415926535986a1339d000d88ffffffffffdffff7fffffffffffffffffff1e957fd88cd14e9d8ec62d7ff5ddffddfe0071f71c7707d3ad1d7a3e7b0
  f409afa5bb40ca949ea03d20fd49ea006807ea834c9903d23d4687e9201e90ff553313f55031b54ff548d03d469feaa699069a3f54000000001faa00001a068d
  0c9a065514003ffd5541ea06868007ea81ed53f54d0fd53fd527faa869ffaaa0340003d400000000c800000000000680000069a00737a537faaa8629fa930806
  81a6993103464c4641901a34d0c26800c8d1906984d32190d1a64d0d301184c21808c460868c41a68192ffdeaaa988c988609813234c464c9a6004c083430269
  843098980000002608c021a600008d019064c0418234c0da54941a3d4d07ea87ea83407faa80f500000d0000000000000001a000000000000000340000044924
  c8934a6d3ca6a7ea9bd29e48d0c134f5369a9faa34d1936a68610c43d47a9a64f530d10c7a53d46f527a9a7a98d09e89ea06269a03219a9a0f534f26a0c4c6a6
  69313137aa6e8f850fd8c5a90e3e902a5aaaaaaaaaaaaaaaaaaaaaaaaaaa91111111111111111114ae5b4a65fe00000000000000000000000000000000000000
  000000000000000671c205c635ca61ad1b6ad6f6f98c7618618612492492492492492492492492492492492492492492492492492492492f68a8124004ff6448
  940d6c19a16002c424c8b29ab59ed76d950c539748e74ba40af3ef179bd4f5edaf97dbf5ff01819fc14183c26170d87c4627158b8b1a3d1a7473ba4d2e9b4f49
  4b4d21cd42a9f53aaa8cd6af59add76be4ec36353b25eceab69571ab36bb6dbee2b6441cdd99e9e043e6874d206d95043af80aa075e68e2b167716ebaeb94d56
  7698516b84f03da9aed5edbadbd0143aa09674695540cc085f066d390b5f015135500410be01a38f91ad4c0d4ca984cc982b8204073d09088312288845a42910
  10a6064bea45b404a8f8d317675896010905881b9279452856bc7c4a53142784444a6979c019369305c89932655672651a6a581179c4621dc6ece40eb8b935c4
  0cf8bdd840fb01047cf5487d01807f20827706dc2b71fbd0a078b801bb4609f59e2d999add2eb4d2d99a26668208208208268823932003d30661064800473c35
  6778711e60f44bdd39cd7dd5c86a3c761f189296a3456a58c7763042247dd2445dc1ed5243e80818adfb2891cf2c3c5807a8021084210865a2769f53c0b74338
  4b3a49e984efbbfe70b7bb449a411d830e03310eb46b447f8fbddb6d267127d9a81d5136784e0bbe316070f2af51191453e1c1556a37efcd41870be64b747a53
  3ae2e2a05825174c9d17d2e6cd5dfa5d3692d6bc32d6b5cb2d698494a70294a70899b9c4b696db6dbc536dc1981d0332c11fc618c610f8b478e3dcca2e815b39
  5619d6558b4c1c5e2d171e1cc85c5616abad80bac335d664b325f8eaf9ad7ea594dd93659c25917668a7166a69e150ba96468da5705aa9a2cd552ad90bad717d
  a8b8263a0ab131516b5a36c553197e563b498f030301aa2eb5f763ab86e71dd6ef870c5a8ab63ac5977d5d5031fab66c38b534615816c2fed67e42a5ad96afb6
  e7eec2fbadcf706af830430a40ca924800000000000002020202020202020202020202020202022022022022022022020d8360d8360d8360d8360d8360360362
  c8ac8ac8ac8067de8113c18737380337a1bf1d876f71fa86183e0d7cf61feac650db480d64f6ee578aeebb2f45c0d30c994ba8210a28206f99dfeae93d247434
  343434343431fbe181fc091481ddff369ea11809f8bf3523bf363d2af4a5e7e00bd7b60de09514807c59d82f10251084226d0842113c8b9a25d0d34d34d34d5e
  01ec9043003e604adb5f89480c3e033af280e4fd8d266dc4a2d38edbecedd0f3401cc160ce9ccc35bbdf699979aa2f4b710a70f9df56cf9c893c7886ac8873c1
  0e7c8bff089e4ed48214768c5cfccaef51254cce571580d737563c9f8c8b410a0a2db20b7647a8dae0c4b32c671993913952e774196600471ca32e624fb7ade4
  ec1afd7db5facd7f15801f7142007b083e4110e3f7958c8867e54a5cf8865eb253c318ddf4f85b10bac440cc8075107288f00e09f10d908630160481359c3a00
  1997240fa81412d037e230e8831e03cf45d132ed33dc679e7a71e79daaa875d75d75d75dcbbbba752e62682145839671ca471c9111b853adcfe633312247ec03
  443e24126c059510745590aa145f6ce4f88238e38e39a8e7d6065b134d352d24a4836a3c01fe0160d95a2b8a0461622b98950fc902967cd1d58acf7920515081
  423a04ed2d14668c801ce73a539ce739ce74e39d1451451453e41a4c7900657e3d11a60907867a4099e20750b2cb2cb2cc9205493ed07c42f24b1680568a7333
  342cfb55c1647dc97e0580af3646c40af031c41100a94a5294a5294a5294a98661c3870d9999999acccd1c71c71c7d434f03447d3fa27d66ef484013260e3d08
  3c83f933b3650b7c4924924926652462a33f82167796ea2096445fa00c9a3d0015346d5c0a21ff7205166d2b7812d5beac55311fe5d35ecdef4f59e23ebba6c9
  a1032e7ca6a428a4026c61ef7bdef81ef7bdef7da5f0d5e1861868819b04cb03748c438a4486c038ca0f1953dfaa6d2ced4fa34e98beb8566e6e6e6e6e6dea20
  bbd086e4062f0c587611d1e53947725c0ec962c1f536212bff8bb9229c2848435099ce80

Package: simtrial
File: data/Ex2delayedEffect.rda
Format: binary
Content:
  425a6839314159265359371ee1ba000a637fffffffffff7fffffffffffffffffffffffffffcbffefffdfacfaffffdbffffffffe006bf71f7b802829f0ab41f69
  b54a1f0ca9446990cd469a3432310d0c2323691a641a1a0190613d269934f501a640c9840d1a34d01ea641a3260004611ea193027a866a69a68c9a069901ea7a
  86d23464c1a2a50d0034c9b50326801843d41a034ff5536a01fa919ea407937aa8f5346d4ff553134c86989faa1a00034007a81faa0000001a001a007a9a0000
  000018a9ea67ffaaa89a680c8d321a3206264c4d32611a184d034d308191918984c418013041a3268d0190d0d189810c20c0080c9a0c8c0993469a0061308daa
  4a40340340d0d341a0680000d1900640034000000d001880640000d0000000000000340001a681a0020000003400d000000001a00000064d019000001a000000
  0000000000000000000032680489220a6a8f30a6ca4fd265336a9b4d133d2132791a3536a7ea07aa61faa1a31a9a9b4340f54c4cd349e868ca7a9e286f49a013
  1e91e509e29b4118433d43537a8cd53d27a9e43d4d4f4d0269ea68d3323d53f41a9a300e49de4444444444444444444444444444444444444444442448915f1b
  bd4aec25fc00000000000000000000000000000000000000000000000000000524c614d72c741277431fde4ff4642fd0450450451433282198c9abe85694348b
  ebe3492969a9ea266a6a8aaacaeb0b2b4096d7175797d810585898d9195999da1a5a9adb1b5b9bd41c1c5c9cc184740aeaecef86561e52f12b2d8897c4cc4ce2
  b178c9ac6cdce4eb86d8efd5f0191c80c0999999849111110933212485d31d44f292eae5d2cd32476d5984104a64aaac4b0d16174f4047580c00149755000095
  683a0c4f89e0657b4610092ac68102545a56f4f1b187661d90e8b2b9d100a2806916279b04009890d307eff1ee49249124776228209021245eb05ac60369094e
  9a6b7f202afecc3678ec275491a1d12981126b29f6a0c5a1e58a4a17829860a968339cc353ccb22188c60b1ca65ec850b834864044901743a07320c2d339136e
  33c1b9c729cf69872e0d6aca50b9c764174417eab22824c30c11066d8764f8f332429b85841ad135279ff6c21768cc1e44847afcf9ff6482d121334cf5059ff6
  907f236c2e209690195833081fa09f284653e494a531b1d7a4d31294c82529b1c0081060418ee868d4c0c78230d8c8b9c65ed003fa97f9c152245d46f4f0cd94
  f37cc0c3c65f50a0cc8fbe0e1383d6ee1769bc48203154ba87dd3a9c965bcbd3a599320688a59c485ffb7e925b70ef86cbe34ca6f3fcd5080f03b35dff5cc4ac
  e7c424fc219a05745bdb1c48a72ec1cbd94851f8fcbf2295fde3f4c57fae892930d0b7ae3f7ec61710b0df912376c1043121206d4dd49c83b40a2c3708aa0792
  8aa3e2f3a09b2fc2e5c99a03217373a8b9b9b97b709d7b2ef5ebd9454129ab7825294a5294a52ac36b89a7fa8f704d0c021d6102d1a288a54863e2375b3b3596
  cc799f9a54d41b48ab0c5a254ed269b2a43ab2bb6ed7bfc0d74ef829be0234f1f9eaa51f4988a4fd693a161a40974e624af8d97cb2749957abef2c37eaf28ff0
  9ec66f6eee8477a48bd68c93e3bee3e35b66fcf79a087adef3ded12dca69a399f992199380000000000000c8b273804081020408102040810204081020481204
  81204812048120481204812048120481204953f6efccce71df664eb55171f4a8daadf473cf42f8bcdc182b31a92461d86307e28f4de24c96be63080c51ea3b2c
  c6c395fe4f4b2549cccd044390e45685264ee94fd9c9a290a90348b27a579d4c283f45345cb55a987ce3c90fd3bd80d5d82857c9a3ee8d4060cc6f88c89f030e
  40a3868ecd0081cfa94ed566b6b9915294aa8a950f0f000419d90f3cc2f29903fd94c8ad732570ebe8902b866b76d61239d6b134471b51f18b6ca8cabb319d97
  6e3358cb17bb477286c0862d1aca04acb258ec5532c0a08933dd1d1d8f0d24b7ac396f494788f802a942465c618c130d7a6837c97bd018bae81bb129f153cd4d
  34b8353901f7130ad95d1664c96c1d89c3c0641d310d71a0d10981cf95bc05f6fe7bd5d957d1a2bfad2bf6f5933a9368f8ac8840f99c692acbaec4a208c9c7b6
  7b07fcbb028bc0e288c204e004a51ca809920d79d4958450d38f9cb86b4bebbee947d0f132afa86d4ccd47386596596596615965d5a2263e2f01157578f24a21
  e49aede2f58916308d05914093bccd0237c249809f8e2827501f82085b155f90407ab50a7a87aa1e4b2988187035c0260ca5520ad28e40e2a80815283b94019b
  3b6ce9e6023a51e7cc278de2e64377c26a6bcd75c1f8046aba647c61b6374c19b43d1365a52d4c7fdee3b276b6b6bc75aacb2cb2d0914b2cb5a6606a2883fcd8
  0099145f62de4902d617c4b49100852088b280671f8019b3722efd5bcd659a5c62cfe555157d6eb7864fc99fa2e4176a7f4118dbd0d6ecc3c6bd4de64b0b1b92
  fd917284490cf94764e95a227740418846187f61d4430c30c30c5edd365d76ebaebaebd4a952a54a9538bd5600b7abe100c70b40905a577c67e1f4164b4e755a
  70b63acabf3c3261ec5daf0c47b818019ad41639a0aa5bbda6512c98b9a155766bc8314955afdc1d06071bf48cf0e47542309d8f595015b0042108b221149421
  0842108868512275a2af0f6d425bfa03f00ef0567caadc64798281ee2d82ead509bfaa59f5701d96bc5386e73ab9c52581c10b7971eba53879dfa8b25b3c6ce0
  9091b06b57e92bc3c7f94bbccd1fd78211e02416cafed8992ba584905090afedc845f76aa0bbdea8516e1e098f0d64e610cd85f47acabc36a1890bfffe1d8069
  696c692205be30979536a6d83efa42c0352ceeb0c91444cc5177245385090371ee1ba0

Package: simtrial
File: data/Ex3curewithph.rda
Format: binary
Content:
  425a6839314159265359de0ac33b0009f1fffffffffffeffffffffffffffffffffffe3fdefffbff3f3ffefffffffbffffffffbe0069dc7b7c07d0cfbb0e36d4c
  4a5141064d000000000000000000000001a68000000000000007ea80000000000000000000034a4826869a64643403269b5346403d46d4c83d206fd5234d33fd
  544d3d47a43d10ff553d4683401faa61ea80641a6834fd534006800068d000034d0680d001a680d0683293d4cfff555134d3f54d189a069a611a0c9a01a31323
  4c98264c869a62309a3203103206803262686468c9a64c0098002184c984313434d304c468d000c0c2a9268d00000000d00680000006800001900000d069a1a0
  000000000000000000000034000100000000006800d001a68000d0000c8000001a000034000000000000000001901a00000024914354c991a6a36a7a3464d341
  36a64c4d3d4f298d434c9e89a699a4f4ca64da9b5326684da689b51a0f5313d41b507a23269a32306919062034627a4687a219191a3d4f5368021a7a3447b54c
  6800555709b6db6db6db6db6db6db6db6db6db6db489122ce35e92fbc99ee000000000000000000000000000000000000000000000000000015537509db5c771
  53e271f32a16390fa228a28a28a29a8208252082520aa9cb39d4224460292969a9ea256a40d4d5155595d6165696d7175797ce980230b131b232b333b434b535
  b636816e6f0a19c1c5c9ceed778889bc5e6f554bddf2fb7ebfe030382c1e130b86c3e23130f8ab5437ecd95241590db9248db41200595649081654cbc378f59b
  7d23d22a5a46283c81072ae85cb4717a88823380c92013d52b84240a44188185061a5df06c5c8124ae0c900856c603331b30eb7328e61cd0348b40928100c0af
  e3212942f4e1201388481c00213972d09c11d18e66318d500304b1038110d960daa5a1679368c67815815c3ded5dc0b37aeb86868062c24def24b7c5242844ba
  b157e7f8533cf584e8c38ce2e853a583c589f62814dcdba04aa9297ce90af6ca88b20874056c3d45abb5769c423cbe90d051ea8ad4b1a5b80f56eaa47a6a4c0b
  4ad03bde005228207846f84c5f80205f273dcbb1101505abdab279e20e89589b44c3c345e0db45b66d473f474747233927a231ee4094206e3883232eba33b356
  41c542b66e4e695820a1d79ac89f7cbaccbf3c5997b38649914225e8ee0ba0485986fe2c17ddc542e32ac5829abcc7c6640a77506a4db03e32a074f1f0b89ab4
  2313e19991d33fdbaada64c2bb17cf4ecea6e02152bb945a3afe0548ed9e4cd3f417583e6575242c91bcf7ddac04e9c1d3e0ec10ecb22fca1e6417b77d47490e
  445c79f8438d2ff463a04307903e48141303cafce7d4206483e74f67d1540020e287ca31c73bec2c59008f8a049688249291ae41c3883801880a9aec2810d11d
  2afb09bc65d4353fcf286092e2420584828729d80ba71c00011801c858c006e000098609aa4ae3d59062ac13faaaaaaaa9bec8b28189817377960c14cc34fabe
  5aea2714d559652f832f0ef5c96f4b6061b66e3ed04f4a007484e96400c449201b1db0c14112474c293249249249249249249249249148a45229148a45229148
  a45229148a4522912b9a592d12912912912912aae192d1291291291291291291291291252266c292ba44aaf189d6f9cd41fc27c7b6b32c3b5d7c750b67c10983
  73a3e28f238a6d3beb195be22af4fe458f869ea998dd8c6e9a367675f458d1d4c989060e4218d908331b9db4246974ce25d26bbb05932d90dbe6cdb17460df9b
  3bc4aef619b00646d191734c8ef89d2a272027940f2a0f2a93e140f7cd2a72a901ce4941d9094c7252d5294a5ba5e1294d2a6b2a8a0bac10038a8884a4420729
  8223c0bd8630bdfa984843229e11e85875b00dad312700d5cfc18c60a6cd03c2134a312ad30f23c9384a48619aad8e13095c8443944d933c86c459405d6158b9
  08d787008288f9851481344963094ae129c7d832530a18e318985717ef0eb484620635ac1752eae53e9ae78171c1032ee8e060bbb013c306381a4131f3265f01
  e78830bfbe92f26dd7f85dbc46b21f11687d1504c3434ed0066e42cf07a61052af01995215b3ed882c23ad1a5d03d634346ec27dd2025b234b275b99a68fc2d3
  68a9a3a932914bc9af353d36baebaebc24dc74938651f729283887114cdc38c59dea14cdb3aff8352c4e7a7385fc026688b9ff40b0f80e7012a3ae2685c6f6b8
  94021a82a88a7c352a576131cc5e6852f7c1a0bc73f1a650f7138f40d4a3e556ff4fd135678cacb9220ae566531c7da285635cb2ea2646dd221c6a8e0a15f28b
  ab9d5432a0cbd63eac48214e2398b9cac42828280ccccceb86718666cb821f5f06c2874a5439bf71551604439558ab1862d41480d9131180a3855c03f6412328
  ae51ab7e83118902e21943e95a44f3aa5faa0dc29839470ebed0c91844294412aa39482ee4f52615a7e6160141b50861349a356f9639ca8b3f50733b02027565
  968c5968d5a3565965967a58cccccccdf8cccda5dce22749887f303964d3905b13a50216bcc086b83546d6514458a0c99805de156a4116046ab62243afff5674
  ddd59e72cd2b10bf148a3b0fb3c57b21877170524a86f33cb0e88605aa651caec176e5858e82220beac5883777c39cb85111af60a94bc9e93d1d91d0331904ca
  2e017e0a002eca13f00a0a002a50843343ca10842108422d509004f645224a10f51310180f4819553e0328a7e18a9029d6f57912443aa042d6e70f490a417d71
  3ba6f5caac58f8a6c1c1a5479715074d4b70eda401c2592c88f39ab7a38259921023c76416a9a6d4f38d1691c57edf4fc1e4c5227f272c7605301614eb1e8d0d
  b774961a433428c5d2d5457a3f40ad4efc531022d410a7073c9bcaab687134c10ca5d69161c67fe2ee48a70a121bc1586760

Package: simtrial
File: data/Ex4belly.rda
Format: binary
Content:
  425a6839314159265359b1ecd097001cf5ffffffffffffffffffffffffffffffffffffffdfd4fbfbddfefffffe7b76fdffedffe00c3f78f0783cfbe2a476db16
  d5cc03020dddc6cd72ce94aed954a084a44003d4340000001a066a03d461ea8cd3d28f4f511a0f795468ff548cd1faa269fa84d3d21a33f5501a7ea47ea801a0
  001faa000003401ea0000340001a52a353d434680c8d30fffd55520fd4f4a7fa291feaa7ed4a69fe553f7eaaa7ea87ea9feaa7ea9faa0ffd55068d07faa807fa
  a7aa0000006800000000000340000001a000d000001268c80d3fffd555401a000003400068069a00000340000000006868000068000006800000d00683403400
  126ff5546fff555268c80d1a304d1a686469a3264d32641a64c8d01840c08c2304c86098131190d34c4680c800000c200189a608c26464c09a6134c10a94a204
  7a134ffd549881e9334d234d30081a0f50d3d4da4c0803464c000988da9826351b53d10c8d0c230047a8698991a68d00d3118693464c13469a64c268045240a2
  69a9faa7929f9aa9fea93ca3f4a3f4a6f53d44f29bd28f681349e937aa34f48fd28f536a6991b53d369406d47a9fa51e5191b51e9a653d21ea190cd43d2368d4
  f50c8d84d4f265347947a9ea69ea69e9869468f51e53d269ea7a9e994fa6a66d181dd0e89fc0dc138051144a5294a5294a5294a5294a5294a488888888888888
  888888888bf8831228d680000000000000000000000000000000000000000000000000000a4a4cf8a7553e0e2e7319de54a655b8000000000000000000000000
  00000000000000000000000000000000424be8b30a9cc6c332b74cef934c5cd73536d3c26c66fbd9c72673d7ca7bf95d0e5bf700000000000000000000000000
  0000000000000000000000000000000000000000000000000000000000000000000000000000000000000c34c9992497e3161a5878acd8ba338ab615cd603104
  08a5315738d751cabbc5e32f4f23d69091c6e3b01258fc864724f4b84364f292795cb1e26532f9895967d2f1b308999acccde69e2f393b3d9b9fce493fa0cee7
  b3f43a036868a8d3a2d1e92534ba6d3d26a18a598d4ea99d5eb35baed2fc4d27fb0e1c84b86393aaa392410534d2a808698e673b9899d74da9981c7de7442454
  81cfbe325ebfa38da4111493de4841042d820d70642435604b29b4b3980001b7851410b3807bdb4cacab415a094550e32a00a90201da0098de1a0cc00629594e
  2117184602185c2e06649b0d8855049c028037e14aa952140d254c03211400892e43601c598ccb8481c306202431614c0185670a5404c520a6b12eb7e8714b99
  3ece3469984c310093c387d3c89c13dda1b5997bf09c52a09bc0434e3381b55d4a945d5f5f5f5f56bebebec587294d398beb67032004c812f1a5d96a253a5b21
  f0d977eb1802fb3eec73213a7390eed31532e4036eb250e67d13dce93907d76108435f842108432d66cd9b366cd9b39419a24992f8128993366d325124f1a1a7
  c086e1d88b84d2c2fe02447529a730a9965965965965959a195804c8817e7da0fcc93a01a2423fd908b68047569a1926c18306166f39b3e0ce60c198dbf060c0
  eeeeeeeeeeeeeeef6ad5ab56b12d5ab51b34ac9483a9491cfb01a3521fba421db923a4491e3639d03520605fe004065965965965965ac86e71e9c5bbf094e682
  7e32180c01aa4250d19d521d7660d59203d9e870ed6bfd9300a4924924924c74990924924924ad5ab56ad5ab56c956893726d899f58d6d026892ceb251247c15
  2323bf5249a31050e7085d290be83a30e21cc13b4f122947e1a721d789c3ab9e57430f707e31c6950e690c206e0868341a0d05dddddddd9f2463d6847083f586
  3c7763fc04a3963c13484c796063a5f588fe9f27e4ee7a5030e9a131e42e0fb16765536880710022b4a41900042f80004f6008b672001057683106e202cd032b
  5d1966c3686596ecb2ccd22150842108421089641906319d98d7534618c6318c655555555555589c1e0a1278001b4100ec0824dd6a1af98196ad653032f24095
  404e6727f3d679d369135f8496a94eb32944b52bad9e9126e5a34cca138936a04c19a712c53468427a4b9aa6a68e685340cf37f1109a75978604a617534cd3f1
  168b30620096024863da816c84c07425f791aa7c509375492b4497509e6c77296545293629249153d8472c09ea0658d4edb4f3f4893f4acf32cf364c39b1fe06
  b7130db21b2a32493e0cbc94b2e9679bea06af2f69524f68cf1a69c7c2e484fec2ac5d2d1df33f71384daa658b2ce5654e74a2869c4654ce51a428df2d252cfe
  0cfd49654cfbca8d214dad455449953a602666822276450129b4a2514099aaaaaaaaaaaaaaaa9a69a69a69a69a69a69a69a69696969696969696969696969696
  969695a5695a5695a5695a5695a5695a5695a5695a17456255a55a55a55a55a55a55a55a4c707a2daf306ce2dfbbeda18d507158d311f6e0a9b703ac198cfede
  bcd839bc1ea70707afc1b278c0474e11ce423605d2c8e298d97218df9b5fc929bba638285f8f01099f848467070b0bd5e16155a0c0ec70baec2f798585857979
  79797979797960f061bf420dc811e8a1c202478f19f83ce8be8c73da8c8fa5f21093ca81a6c2c9c420cb458b162c58b162c596ebd24f237433a5233cb09cf0c8
  364913891809f650c821375f06c60c348c7a11c81020718004814c200410bb88012e213ed83b6294a5294a532694a6b694a5451451c297efdfbf7caf6efdfb76
  eddbb76f5cb71a4bfe6512c8e776c4963565f262c6626124ac78da3ac6a6aa7de49b3e70b0e8b90dfdcc1a14bd5ced860d64cc99eda85470c0a8fbecbabda806
  b866758f3c0cf2f960db2798bc853c8073619aac26e60bdbdbdbdbdbdbdbdb2c79070d842f4812cc083f5685425971080fde2c066eee595625177d751919de19
  186b01ec5a890ea960f5536695d58492d6166b189d2512d6b4fc2bb93bb8ebba85dbb771f76edd8e3d5638e38e3c93368594493af161ea40de04c27d0124e9e0
  ffd1edf518f50487adcfd8750a41eb2436424f72d9b3dd5690339989d5e0e7a1e31095e880a20b4210b10f550661204c18babababab2bababab20f3639424178
  c09948ff02cbaf0dda8427f32738c2fa542579b1b4685d18010447e022411100a20020c58887b405b4bc08a7d091efe04ed340e852c0e443b548fda69e34d5fd
  addf0774d3e5dcbd7b1186c4aebccaebc396314296395765294a58b294a52acb2cb2cb2cb287b8c4210dec644dd2079f5907780203da927b185c0c0a1bd84754
  909be875804221e943883008eccaf5ebd7af5ebd7b35ce1c1de83b83842e74dea1b04392bcdfd23e9a9badb9de8e79cca669e79e79e79e79e79e79e79e78e38e
  38e38e3f6928325f3a4be465fd3325dca6c7dadadadadadadaf0c0d018800857387d9b1ff8621649a263fb328dfd21ad03343af02040dde221e88bb1035e3396
  076b82bc24e1a6c1edfb7aaaaaaaaaaaaab60aaaaaaa28a28a28a28a3128a28a28a33172e5cb972e5cda99b3eccb41ada6f638e1950b55dd9d1e30bf75d5bd66
  cdc6879d5f155ebd7af5ebea3781f880c86480df896878ea1fdce783ae03a60db603d929b8177c25daf3d4ec40ed2fe69a6c8cd34d366269a69a69ac58d4ec58
  b162c48cdd625659283325faa66d8b2a9974899d9bc002dadadadadadadadb8c07a63881a31e295ba8360199cf89f9435c9d8007e04170019c01e680b14310dd
  8618a36256b8c4db0c30739ce739e14e79139ce7b73a70e9d4210842108421085ebd7af5ebd7af5b4ba0e9d92d8fed5132fe59b1532fcd2edd9961a7ccfabb81
  0e78780171717171717171718c43a443da7245e3839d0e98398028ea8ecfbef7e4d30df101b236c1d157809e181b6e3a9f413ec1dd287cf893289344cca24d13
  344937d2cceeeeeeeeeeeeeefaa4924924924927b9c7884218ec26cfb9437ba51ed90ad5a9ad5ab56adcf1f17623bc13642e61ead9721994d994b41a532ef135
  34d34d34d34d34d34dda69a5dddddddddddddddf17172793c864f2793c8e505d8887f550f3d4dc8d0266e00c31f414f294c761a3e2ef75637373737373737373
  f7c4fb8a63f185fa5af71ac1f8a051ed80d08396573892c2cd744c97d58c664a5965972f2cb90965965965965af5ebd7af5ebd7ca26649920c04217a0a68043d
  bacc8dcf88fc642642a4ccaf8c4cadfdbdbdbdbdbdbdbdbd9a1e129cbd2fcc43e68998169e2af216ccd6e59345de255649d00237087941d11a12aa3ff8bb9229
  c284858f6684b8

Package: simtrial
File: data/Ex5widening.rda
Format: binary
Content:
  425a6839314159265359e6692bb60006707ffffffffffffffffffffffffffffff7ffddffdf7fffff6ffefffdbffffffdffffbfd00539bbc012f18a3776865583
  512046934c87a21a1a0d3d4d3262681e84d1a3d4c9810d03269a69a6268686264c8643462069934d3269bd53264191b534d00669ea9a000d0c04c266a64d1919
  3d4699a6906a542610cd0419a4c98d4c268f51e8236a36a7a8fd088d3c821e46a681e90d37a9326086fd519432347a868f500c83234c81a006807a869a69a680
  034311a3434c6a1a34c4c64a7ffaaa87feaa8c9a1804c99184c09a34c134610c9a3130980801930023401841846269934c00004c0130134c1323000469804626
  9903013069eaa2481a32000000003400003201a68d0340321a683401a00680d0000001a034d0c800034001a1a0000001a00922526d2a7a27a4f21a9e9a87a9ea
  6f547a8c8d0c4d31190193d46d13132319353119a43118d27a87ea6846436480f51a64c9a6264308c4627a8646d134068d3203086986809ea7a8d3d4f1986186
  2dec3de00000000000000000000000000000000012363d7d949ecf0c888888888888888888888888b38d1868c30c3473ba4345df1b26e371c82060f228725938
  545944795cb18912a68687cbbce6223329d444c545c666b379c52aa36395e763f3d9fd068744b243470b0d7ebfd87010f81887fa44ad9802caad4000ad29249a
  08666492504a0de9d5058685fe00bc5c9d903912c3032b64c9929565828e9084eb08e21004d0314ec28f4193858c87004134c902024301472f94798626187689
  8b4b44928900c917a65ceb072b24e084a04e875e56c8412c6ac6124011e48384ccc3b3070adf2a0bde242704476557ce39785db11fdd8c95effae24386918fa6
  21226862a4628630829ef36ea12925b20683d7cb91ac2cf3c31d3c6dddf0b0b485bc883cd48c5b3c84b9658425b37f440972c5c2bfa4f9c29b6d5d8b08a93d07
  37503fd3ef231a29579b6c6cab29665fdb567a4fd71b2bd50462718a06091b77b93600cdd2d2d2f697e891daaab4872d0583af1d8cf34f689f347207fd462af2
  8f7c60aa18bab5131cd721f7e471e6438cebcbcc3c9accd4284332f363f90a1e8f83550c0b7433ba781cbb1fabad0664961138caea27f1b7a4da01c7d1be0335
  1851c3288249b04e24df6d3c7bb861a98cc8dacffe5386c73d24964084dcdcda05e2a98d4712902f0345a1348afe39edbf3d105ccdb53fe7e756c3c00468d840
  3b24201995ffabdbd14e429b41dbb5767f3f166ca9a6a46ed19bcd3531a6c134b241412565c0c7618a160a22bb3ec770a62fdbda4257a0c2b864ea92d5c7a12a
  c4baf1099c43a6d552b4b8decf0bc162ba5af723df36b9cb42f52ad7511e7188a73ec71811354bb6212c99f6d3f78cc63cab317a04c68a4a446e2823e6a8600a
  7db6ff748fc45412b00c3f794329c884b9f75aa0d6824a3bd6089d5348f4e3bbe4a15328978b84a58f4ee79fa55dadf3b3eec3b3ae3290000000000000000060
  6060606060606060606060606060606606606606604a0d4f605c7cabaef3b26a82e3102a82cf95f89b631302c8cdd6af88a416c50ee5fc06c8707a2b8aa81e32
  6ab4f559082a8c64b219a75459fd148beb91ec5a08e525674ecca5ae4a0b537954e5d978d1c85b0177c6dba34627b4251466465baf3b8cd2674ce20adfc38d75
  2bb219f865d13c08d28f886eba4289fd38ca0229c7f4344c6b8a321a321231e935f50974b66e5f07b7f944e4b9e30351b151cccb9047c47e239686fc18376dd8
  4433a935f0ac8d73068bacfd9166998167221a181201e910427546000f9e1822094478d5d42b2102c1a76da6827135d3c8199d0fbd9e18c621f8884232a55eb6
  132115cdee5c66f0661a9661d75ba7ef6fad332342124995a2d3218b150634c3b38060b88244f549cf7466fe609f955bb521899fef4e24e15c724555f6c46765
  4ee14c760a80fc87a2d5c9878162b8d9c973ed6e1528867390eaf1ce3a6ceec9c71ca4e3a2de22748f8f90affbb1616f4985594c133c04606a16056fee6e1e75
  c1742347da9ac58322282580b703e081b96d9ccc37b0084354ec44f085abe88e0c6d2ad5b52414acaf44208f0ef2c222a10564f874f7b6b6c1fa99d7d5a74e9d
  3b82817425c64fcdc8480a28c8e95ee40590a323ccb028b2ed050d92f7d29b5634217478f34cfb90a1ab0a47d193b9e9ac18da024e41b9ff0eb3a67a0c7fb8f1
  63a6016e27547488150225c42984a9761b316e16d94814a5294a5505295f5dca694f274e9fadc2fc102b606ea0460e3079b77d8e4ac80384181fd30830e0bd75
  858af3bdeacfd846b6fd50a706d1a9b09aec5712fb0700819b1f5e232edf9d44aac0ed20b8969552dd3dde33e2d6420895744de983353187cc30ddbae43ca484
  207c7165ffeb884d608bb2190a910e1bc8e5383a327e1390c02e78dabf71933ac8193c4490714c1456b66b932fa4e1927436200404008f775827968b88a52f61
  1a2452f53039d3f89a83eb8540e226ce36a375cf926cbe13793fd699948bb2affde8eda600b51a3a924235e61187ba9af3039d59e0d9d99345ff8bb9229c2848
  733495db00

Package: simtrial
File: data/Ex6crossing.rda
Format: binary
Content:
  425a6839314159265359c781350d000acafffffffffffffffffffffffffffffffffffbfffffffffafffff7ffffffffffffffffe0081f71cc0e9a02c5bc41d017
  4514565094a0809a9e9a9851ed14f50f53da9a9e43d50f6a81e53d47a47e927a9a0f53691ea7943d43087a4f53d207a4cd4c9b49ea7e947a984f5369ea7aa69f
  aa7a83d47a8fd4f5403649b413d4d36a3133d24cd469ea699318841ea193ca64f50f10851143d4f500c86d400190d3469b53279269e994c99a3531326ca7b487
  ea8fd5349a7ea9a3f543fd5046d09a000001ea340341a00000001a00000006869ea340068d097a6a37ffaaaa681feaa019034c4d069a32641934c8698268311a
  32646431061300982191a3268699032621a68641880321932680d304686801826464c98434012fff6aaa8683403232000c43269a3232000064d341a320d0c832
  062000000d01a1a308343434d0c9841a00640c2640c80d320193419000200000000000000034000000000000001a000000000000000000000000000000000121
  51085327a3299aa7e8a7e41a09890fd531b409a348f4ca6800f5034f50f53f493d2794fd26a611e9ea8d8a3d47ea9fa53f5207a81e49a0fd53d47a26c906d4f5
  0c9b51a7a80f4f5403d434d8a34d1a68f50f6a9b44369e89b400e95f0249249249249249249249249249249244a952aadd5ceed2a9bef0000000000000000000
  000000000000000000000000000000000142eeb89cd97880bcf02f5d1bdd3df2d108421084210842108421312244820220f07db5e9edd9f5faef00b2060ac8ba
  0d7b0bdc242df21a1d8c444b2bedc62a2e32fd1b1d7f8f66d300d506d20b24648927285956f2d8197c123839887c26152c361f106999a9b9cc4e2a75c349e9f6
  552fe704701c27468489c736d9d424902bb692490aecf67528542d751aaa0176367cc085603c1b6321b4e67535445e030042a1755040095903326d2adcfdc51f
  52ba20002c9a4204ab9a0f696197962cb164759966eb24ab201a16441a42ffca289240bb78668d41550543f6b48124679821b049556b04da9031bcdc6cee5aef
  83d90d1157304546b61d843198daa75e6b0bb7552a43177b47665fa25a7fafc15c2c674c1a30eecd395e80dc95ab73778c1a301239288a861e9e6494459bb51d
  2d9c720ca982890c94ab84eb8509e9e5968a44f7ef88579d8aa0d9830d30642064bc835f90ed6a23eaef890658b387c6e19eba9310f0a581b95107e9c00711d0
  b5ac3bf1a9e3a367e450bd6899842785917498429f74d84540a71460a0d18d691e83345145aa28a2b56bb2d6cfe29a9002c622788671e695307af0796430fbc9
  85b30be40fa65b8208f5087806ea17005f45e0f3842f18764b33f58282c130ed4cd1dd628a45de7c73f64a74fe0157a066790d1bc5d9a36dd56c11502c331380
  cb7b19a039ee7a857877181b0294ad46b180cd63acd2dc1da441736443f3a6957b46ecd9c4586ecb8a017971860bfdddcc58eee0874e347986b77df29a04d05f
  ebf65423687ac86faac56096f4bea02e4fc44464d1c60995fc26263553e0615917d4f256585b80e8337f0ec72ba3eeaa6c9249b0ebd808111af89a72bdfd7ad6
  537ba73421c73da957d48b01e3560de6cb073183a7afd0e0e0dbced1d1d1b78654f26318c6318c6318cd59403edbd007a39482f873ce754ea2a8f38158ac3cf8
  ba1049e243ad694742c3c8e160aadd5e08aee192cd2a6a5738adeef3aae076265dab0d42de32b0c55b96c32e35f8b3cd529701b3ed5567e4c35f25a03659764b
  73cb569c7c9c81d4a854b4e545bc4178d1560af0ebf8b33ef4f8cb5ffdb19815d67ae65b86cf77f29077abcfd23f0e9d7535727b05e4f2cfb6aab161a281f653
  62f298cffba618509ecf71389d44027880a5ff1ee56cdbaa63bc35aa89710aba673aa513e027cab26e3e851b0a1aeab52754b9f1835546fdde6161dd05359d5b
  4ab7a60d83abedef8b0a36cb4b03a813f82db6db0000000000001020408102040810204081020408120481204812048120481204812048120481204812409206
  0738c2720edaf4dfd3901fb6cc1c90a0fdf7730127cdf45db9ae65363bd9f7d974ceb1e493ce1ca53e60cb70c78c26a9e2d78d2c4eb73a51bcd82994d276945d
  e4c9d2f924d6a7d90ee50f525c4aefc97f7f4965cf8a1cb0b62628df287a37c71f29109abc643dcafe18e1506f8ca45c2d44e2a56d20c2bd9c83b63fc2a03527
  7b1593ea43a1d9d9d27840db38babc3ce8a9e2b5def888987e7c878a7f527fb199a7a666c5f190cd1283b46a2a3d914569c264faab85574aec86dc58790a1576
  96961428331dd773170bc1ab8d7b1878e93f445a905ba079e022d13283159cc5131d4a3447ce95dad2191a08ad78917f333e6269854d41f0f6d116d603867cdb
  a9d1ef3ce508c9b39c7b4e769316525e98715ab5fd9ab54f2cbd89f168c9346819ff076833adc222ba7c13f1c0bf39a09ab3f442e1268da8cf0c62f3a8b42077
  e33376d0d8ae10a8a2df4717ff1fd34c2e55422e598903ab77153ccf5c746543c40cd4463f844ce1a1336519e4d4d24161f5db9a284346604c0d6305a41ec467
  2fd05bea3cd99dcbe71f052995c8ebad0ef34a722adcd9ebf4410c7f765efd2462fa4d516674478e6dee831dd58696ebc5c3d30bb24923db7defeb442ea741d4
  63823ec35cef4b6be84d4743aad3f26abf1f368f33937f11fd739891224496ef7d8e9715cdcdc2f928856b225c386ce1c4e20c55a0823f1d7042be00ed40e00f
  c8d48743aa01cdecc545770264766a7019a72139aa54db040a0c44a2b8920a42d459a9c0be1c85946366fa336ce7d14ab3d4eb2f7b6cd5664723fabc31d2e5d9
  9d031e6fa539d298a14ebda67999148ecd5e3e9e035666ce953204889a1dbe8136455bb425932a1dfd55b0f40b2b7748cc99372479a72c7eb30ce58a386d5b7b
  0053401dfb63bef44a42108421084240908424630616700f01d9fcc07005e0150c206bc8250588fc8f44d84e70543960f534d34f7e4717a79659062fab849f49
  307b5bff89b6a92449c609d0a1ade3092f2ed51b3eeac8b0fb53303a7f3f8c3889bfb5522c4e8b68f24d1fa311f04e69362519291ccd277685de604bad4a43b7
  bac31268b8512ddc497de7fc6ea03e95f8e0c941835ece0c1e82b2328a82899d114a5295716ecf8d0a52958957317e7bd88eb6a00436d6a3c36f4906356b774c
  1d825802d33c4824318e7e40fe7e850d4163573646d3a59bb4c3e4b1bfbc3441ba5991afde266cca2963bd1e85f144374f28c5e06d6e4b33f7fd0e6e225f394e
  f195a4dcde1f23429a32d0e7e05ca541b14281fa6160ba1a9842629b9e8cd7014066825cc7581fb1c4c42c2e675548fac48863254b9bc104e1b211fe7882505d
  d7269d9afd2a0f5d8832d4d3d5402924a92492774a41248b216b4925ebed21279425853a64ddded4ef285686d32aa69b70598d4bffc22939c4ffc46d8285e6f9
  2685d7c4d62635272bcf4f9738f8806750787b5e9d1e4963c046ea1c155433399a440923d807a55171ce4acb8a8d75621f2819923b00eb11d261236e7315c576
  50bcd49ad0afa87fd384525b7fa30077a42b4cf88b980254098d6c0b95e71e6553d6ec11800cd0ce116476d6fc40e567cae4535bca177245385090c781350d

Package: simtrial
File: data/MBdelayed.rda
Format: binary
Content:
  425a683931415926535944911fb4000c817ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffeffffffffe0075f39ddc01e7d205cef08f2
  19d7d6d0d0ca9f814d0621ea4d1a794c98264da4c8f53464d0d1a623434683349a310d311a683d4c4d1a36a68d1a68d006269a69b4269827a987a84f53d0344f
  534d31a9ea66a6691a3434d1b53350d30d407910a536994d5188f5326d4c9a0d3462640c869a62034321a0681a03d10d1e9a993d3434689fa93c88d3d4f53326
  934cd351fa51a6469880641a00680d1a001a341a034c83d41ea686803a3348d04c6808c9e827a0d349b4980d34d0023d02613d34d4f409a668d4cd098468c260
  026464d19190600984d1a61180984c086980341326684064c00262aa7e869a4809a60d27a9a6118269832993266a34c46993434c9a3430134c9a34310304c464
  0d0321934d1a3103268d06013041a68c1346c9313464c9a6232181064c1a7aa225136a31a09918680460d264d3260000264600688698234c4c980000008c0098
  43131a8d34c000993434793421a368099a0000000049299048d4613093c9a43d4c9e93ca68d31000f48d0680794193d40193d43407a868d0c4686868f500d1b5
  1a0f51ea34647a4193d40f53d46d4f486c48c9a68621ea0d003268d0d0d3d4c36c109012f9a862144ea08678789244e9142f26d090291ce2665c9122f8c9ec92
  0535370057344240ca727604205d35ecfdcce5fa6a866aba494c6a8413948a7c680822121f2c408484849384ae6094b4d4d14283024f16a1cd18314b9ca6a8aa
  acaeb0b2b2356870f1ebababebec2c2c4c6c9c3874acaceced2d3d16a6b6c6d22449126f2650a38b8b9ba3a3ababb3b152af0f25bacf45c9822ef6952a64d1d1
  f209e22464a39449ca2434dd52954aa5e5f57ac98d6ab58b35f3534ba6e6d7ce6c277633d3ec2818ec8e3a68e9d9b9c9d9e9e9fa0a1a1a385a33c7a92929a988
  22a2a7c27cfa0ababadadaeaf4361628915959796cfcd696b6c8eacb638db9bde1cb3495d3000ae182b323021204e60c8e3f4d60f7cc48ce4a3604811d437317
  593d968b9dd457f155b3f8097ad8b7ac2bcad6424838987554d49515f75a2ccea66454cc249161ae812dad810858320612014aea8bbdc29da998bb8631012308
  0227c39319e48c922532120981249380010cc02130900bc66422f8c8302c02206041aa642058a64b3b87d6ec69264510c982e190029ef2f4d68f2fa350b25032
  12054ac24242cc7efb17515bc6e53f4797727d93408f135ced83f28c92056369096afaa740810854bb9c09da78206c535ab0903a55db9527ad0dbb3fef83fd77
  bcc840bc1dd6489e8ca6d20c436d34b6d1fc3a0d3c7a5cf57463d2a36e05de1dd59d8f111c40da8d4c087716c08979891310022050d792ffecd742d818f0327d
  077aa6968c068fb269479754846743de0e84c9c817c10232f0bf514a4105115caa02990a18708e23179b68ae26dccb5aaa6114d8b47ca6536435c433fe506ad1
  ce19a48b9a45822563fe7befb607be676ab2fe1dd869317f2e93150f1e39caf3c08fbfc5cd223a406ee0fe2b929326b04fdfa842e480b103e214136bcf650061
  18489b0fd5e8a124854bcb713ea633d356c49a0c292e7bf54100b0a22e3b80807702d3c52d95144e49bd425302019f20d84eba1e2cd8aed7477eba3c2dcd7821
  d093288efe6111c47d93490c0d25f7be36093d34f2d9044293849cc6439b88a2879177072ae723f77c48de9cc60fa7fb88fa20876adea8a564829b2c7aaa8b61
  f439a20dd537e37a8460a1917255fe1c8c6448702359c6de100961dd9f7a5d31bed272422e77b14a9fc30c68a847246d9c3ee3f802c0907cfddcb4127adb1dc0
  8f1c325eb8e016347210878cce18120420013039ce048186010302600610cbee139ee4e5039c03c49c8640c0d29f3452bccb9f3e9498c0bc717b6ebd662b2d21
  958e9649573515b92f4793bdc41a63687855ebf7d4073ea8674e08b621b548ba6e8f29e6a4d22a7f4cc9744920df6ec8d55cae39b1b01f51d3f40e2327ce7556
  9c78301b575c0634ab8ba0deba242a3bd9d1d8d4a02fe3ad4cbe7daebd158965e616d2d27341c13b0d30e76970d6586fd038d97fbec23530c06ab6489e6bb952
  912c84e1f3ab900664ed2745b17bf877e49918d03bca3128ac34511629593ff32d5fa19baa6d5ce7b38d877c325bbdccaeca9555b0a7d917f6f544204c7e8fd9
  77b4d44fe32659040100039f62cf8288cde775f96ba21268a5264200800129fa90730c61e13f2397efccbd73de88becd83bb86ea8d1992d25cb6ccf4692c504e
  096a7aff6175b0868a9e36a6eadb3d57f3318b73fd5c0cb11090a4f4ca14398ea6e2558efa98b861e380f2a44b32ee4046de33cf05b979a2888656d554ac810c
  3c91d58090297cb4b31c1c4509ad52646d3e9a94b9c614517038a25f5a14738dc17da8e174da86c911e5b2aa40a3063b69ee80171e080200070c6349242f078e
  8fa7fe313a1faa643d8ede4a0d1d1b95f7dcec20a8335c7af9721d0683cec4cbbc2fd4456eb2921338c8409c840989ec2ca8963ace709a86aaa2aff5bfff39c8
  edfebd9e7fb1eba2b8c27ed804205430e32b32fa4c154dbc7e43b378ede0ccc578e010272206199219930813302049080424303242010c0c819b3684e4801902
  42040c92040cc0c2124032100cc8664cc30c2a99d796b889a108165b6ae665e2c9259ba16c840a03596d1ebf31b961e6c0429e42054f1e2e950815a210264767
  613145af9895a4a0eed081366dfab6c859612b3465219f28508140840aa6c4ab9ee4bfbc0dee3e60fce7719dd5367b35929c8f945828c81081591c2140f36642
  840a1714f26d9dac860ca7cec491dae55c5b031a1e2e7468d71d2b8508020006f5c65661def71fdd74293e05154e7bd3b4f8892bb5341b53ab61fa2a1c15266c
  73eca7c5231854100fc53ad4a283436518add9ae9eab1f4578a823f695132346b64cc54a4766719d82a2ba38bd78045661be084360cd42e18657cf0713182d11
  70b5722e8dd0a7ef5412862a16549b4e34c8b938647dbce3433052f4947025bbb9949219e2bec07b366804bb8ad3932d0048b64543f52305b737af1cfa33b436
  22747f29bbf60c995965a03e7464024c18e4f8b242ca8943a77b0bc06b7ef20f745b53201f88657d9736e584c0c65b2f6f00a731402d3e1e96a2ee373b238549
  88d525edb4242c79881ded1b8fbef2300246a3a224d24c9e4b7a6f66aea99af3f7665832b802b07f42537884028d3023422b421c4621b0d75e1b1f16e49b19c9
  3cca848964de9a50b68960f55051aff745cc05bf1a33be07c65c3a679a7daf8831d12067d38148dcd79b168d0ea739e8046c4c275c7006448720003af1c811a6
  e345200ae6e94aa109bd84380ee5274425bb4e52e1e064307be4ab1feb058ae1c2656708c3c8e1fcc1000c172976dfa39b92dc4ee55b2a8a97f5285537fed18a
  50d0b0ec8f96550db4f0ee1317d197dd33612757fcd6bebf4f24bcf8c05106b91d55023355ecb416a67cc81d01089b593d8bc2bf4db455774b75816e9ccba9dc
  d5051a47039cc22b241b122cb53071d434711f6cedd80cb8e64eaae6e118354e118a9393839c5f66f66eaccd47e924b4bcbc499767a37158f31c8474b964a8ec
  2275296b2c3b2e1597e6be95ad6c02d5f34b220d6b42f9b65cf78d82d022be46a77b038040108000895b58c7cbc4c762b92673ddaf4ab5a0eea6030a2d0645fb
  09dc12ebdbc8b1a725cfb562dbf5df88665a909621ba135cd19e2b516a8e22dac24576da576f9337ef041dc4792107b50c6ce73f30c2201a807289eec2c88609
  cc7777e11780ff8bb9229c284822488fda00

